since zero access is the rows axis 1 is the columns . the question was are the feeds just for the placeholders . the feeds are just used as a dictionary to fill in the values of our placeholders . great all right so we've now defined the loss and we are ready to compute the gradients . so the way this is done in tensorflow is we're first going to create an optimizer object . so there's a general abstract class in tensorflow called optimizer . where each of the subclasses in that class is going to be an optimizer for a particular learning algorithm . so the learning algorithm that we already use in this class is the gradient descent algorithm but there are many other choices that you might want to experiment with in your final project . they have different advantages so that is just the object to create an optimization node in our graph . we're going to call on the method of it it's called minimize and it's gonna take in its argument to the node that we actually want to minimize . so this adds an optimization operation to the top of our computational graph which when we evaluate that node when we evaluate this variable i wrote in the top line called train_step equals the line . when we call session on run on trainstep it is going to actually apply the gradients onto all of the variables in our model . this is because the dot minimize function actually does two things in tensor flow . it first computes the gradient of our argument in this case cross entropy . with respect to all of the things that we defined as variables in our graph in this case the b and w . and then it's actually going to apply the gradient updates to those variables . so i'm sure the question in the back of all your minds now is how do we actually compute the gradients . so the way it works in tensorflow is that every graph node has an attached gradient operation has a prebuilt gradient of the output with respect to the input . when we want to calculate the gradient of our cross entropy with respect to all the parameters it is extremely easy to just backpropagate through the graph using the chain rule . of expressing this machine-learning framework as this computational graph because it is very easy for the application to step backwards to traverse backwards through your graph and at each point multiply the error signal by the predefined gradient of our node . 
and all of this happens automatically and it actually happens behind the programmer's interface . the question was is the gradients are competed with respect to the cross with respect to all of our variables . so the argument into the minimize function is going to be the node that it's computing the gradient of in the numerator with respect to automatically all of the things we defined as variables in our graph . doesn't that you can add as another argument what variables to actually apply gradients to but if you don't it's just going to automatically do it to everything defined as a variable in our graph . which also answers a question earlier about why we wouldn't want to call x a variable because we're not actually we don't actually want to update that . so how does this look like in code . we're just going to add the top line in the previous slide . we're gonna create a python variable called train_step that takes in our gradient descen toptimizer object with learning rate of 0.5 . we're gonna minimize on it over the cross_entropy . so you can kinda see that that all of the important information about doing optimization . it knows what gradient step algorithm and knows what learning rate and knows what node to compute the gradients over and an oath to minimize it of course . so let's actually see how to run this in code the last thing we have to do . the question was how does session know what variables to link it to . the session is going to deploy all of the nodes in your graph everything in the graph is already on it so when you call minimize on this particular node it's already there inside your session to like compute if that answers it . okay so the last thing we need to do now that we have the gradients we have the gradient update . it's just to create an iterative learning schedule . so we're going to iterate over say 1000 iterations the 1000 is arbitrary . we're going to call on our favorite data sets we're gonna take our next batch data is just any abstract data in this arbitrary program . so we're gonna get a batch for our inputs a batch for our labels . we're then going to call sess.run on our training step variable . 
so remember when we call run on that it already applies the gradients onto all the variables in our graph . and it's gonna take a feed dictionary for the two place holders that we've defined so far . the x and the label where x and label are graph nodes . the keys in our dictionary are graph nodes and the items are going to be numpy data . and this is actually a good place to talk about just how well tensorflow interfaces with numpy because tensorflow will automatically convert numpy arrays when we feed it in to our graph in to tensors . so we can insert in to our feed dictionary numpy arrays which are batch_x and batch_label and we are also going to get is an output from sess.run . if i defined some variable like output equals sess.run that would also be a numpy array of what the nodes of what the nodes evaluate to . though train_step would return you the gradient i believe . are there any other questions up to that point before we take a little bit of a turn . some ways to create queues for that might be the answer to your question . i can testify to why this but it certainly is a simple one where you can just work with numpy data which is what python programmers are used to . and that is the insert point into our placeholder . your question was how does a cross so the cross entropy is going to take an i haven't defined what prediction it is fully i just wanted to abstract that part . the prediction is going to be something at the end of your no network where all of those are symbols inside your graph . something before that's going to be all these notes in your graph . i think this might be a better answer to your question when you evaluate some node in the graph like if i were to call session.runonprediction it automatically computes all of the nodes before it in the graph that need to be computed to actually know what the value of prediction is . behind the season tensorflow it's going to transverse backwards in your graph and compute all of those operations and that happens behind you . so the last important concept that i wanna talk about before we move over to the live demo is the concept of variable sharing . so when you wanna build the large model you often need to share large sets of variables and you might want to initialize them all in one place . for example i might want to instantiate my graph multiple times or even more interestingly i want to train over like a cluster of gpus . 
we might not have the benefit to do that in the class because of the research limitations you wanna talk about but especially moving on from this class it's often the case that you wanna train your model on many different devices at one go . if it's instantiating or model on each of these devices but we wanna share the variables . so one naive way you might think of doing this is creating this variable's dictionary at the top of your code that a dictionary of some strings into the variables that they represent . and in this way if i wanna build locks below it that depends on this parameters . i would call variables_dic and i would take the key as these values . but there are many reasons this is not a good idea . and it's mostly because it breaks the encapsulation . so what the code that builds your graph's intensive flow should always have all of the relevant information about the nodes and operations that you are using . you want to be able to in your code document the names of your neurons . the types of your operations and and you kind of lose this information if you just have this massive variables dictionary at the top of your code . so tensorflow inspired solution for this is something called variable scope . a variable scope provides a simple name spacing scheme to avoid clashes and the other relevant function to go along with that is something called get_variable . so get_variable will create a variable for you if a variable with or it will access that variable if it finds it to exist . so let us see some examples about how this works . let me open a new variable scope called foo . and i'm gonna called so this is the first time i'm calling get_variable on v so it's going to create a new variable and you'll find that the name of that variable is foo/v . so kind of calling this variable scope on foo . it's kind of accessing a directory that we're calling foo . let me close that variable score an reopen it with another argument called reuse to be true . now if i call get_variable with the same name v i'm actually going to find the same variable i'm gonna access the variable i'm gonna access the same variable that i created before . 
so you will see that v1 and v are pointing to the same object . if i close this variable scope again and reopen it but i set reuse to be false your program will absolutely crash if i try to run that line again . because you've set it to not reuse any variables so it tries to create this new variable but it has the same name as the variable we defined earlier . the uses of variable scope will become apparent in the next assignment and over the class but it is something useful to keep in the back of your mind . so in summary what have we looked at . we learned how to build a graph in tensorflow that has some sort of feedforward or prediction stage where you are using your model to predict some values . i then showed you how to optimize those values in your neural network how tensorflow computes the gradients and how to build this train_step operation that applies gradient updates to your parameters . i then showed you what it means to initialize a session which deploys your graph onto some hardware environment to run your program . i then showed you how to build some sort of simple iterating schedule to continuously run and train our model . are there any questions up to this stage before we move on in this lecture . it doesn't because in feed_dict you can see that in feed_dict it takes in some node . we're not really understanding feed_dict with what the names of those variables are . so whenever you create a variable or a placeholder there's always an argument that allows you to give the name of that node . so when you create the name of that node not name in my python variable but name as a tensorflow symbol . that's a great question the naming scope changes the name of the actual symbol of that operation . so if i were to scroll back in the slides and look at my list of operations . the names of all those operations will be appended with foo as we created earlier . maybe one more question before we move on if there's anything . yes if you load a graph using the get variable it will call the same variable across devices . this is why it's extremely important to introduce this idea of variable scope to shared devices . 
i believe the answer to that question is correct . but i'm not entirely sure as of this time . okay so we just have a couple of acknowledgements . when we created this presentation we consulted with a few other people who have done tensorflow tutorials . most of these slides are inspired by jon gauthier in a similar presentation he gave . we also talked with bharath and chip . chip is teaching a class cs20si tensorflow for deep learning research . so we are very grateful to all the people we talked with to create these slides . and now we will move on to the research highlights before we move on to the live demo . and let's take a break from tensorflow and talk about something also very interesting . i'm gonna present a paper called visual dialog . basically in recent years we are witnessing rapid development improvement in ai . especially in natural language processing and computer vision . and many people believe that the next generation of intelligent systems . will be able to hold meaningful conversations with humans in natural language based on the visual content . so for example it should be able to help blind people to understand their surroundings by answering their questions . or you can integrate them together with ai assistants such as alexa to understand people's questions better . and before i move on to the paper let's talk about some related work . there have been a lot of efforts trying to combine natural language parsing and computer vision . and the first category is image captioning . 
the first one is a paper called show attend and tell . which is an extension of tell with some attention mechanisms . and the second one is an open-source code written by andrej karpathy . in both models the models are able to give you a description of the image . and for the second case that's a typo right there . basically the model's able to summarize the content of the video . so imagine if you are watching a movie and you don't wanna watch the whole movie . and in this category is visual-semantic alignment . so instead of giving a description for each image this model actually gives a description for each individual component in the image . and as we can see on the right the data collection process it's very tedious . because you actually need to draw a lot of boundary boxes and give a description to every single one . and the next one is more related to our paper which is called visual question and answering . basically given an image and a question the model answers the question based on the visual content . and in this case as you can see the answers are either binary yes or no or very short . and this paper visual dialog actually tries to solve the issue i just mentioned . and it proposes a new ai task called visual dialog . hold meaningful conversation with humans based on the visual content . and also develop a novel data collection protocol . and in my opinion this is the best invention ever . because you make contributions to science make money and socialize with people all at the same time . 
and it also introduces the family of deep learning models for visual dialog . and i'm not gonna go into too many details today because we are gonna cover deep neural networks later in this class . this model encodes the image using a convolutional neural network . and encodes the question and the chat history using two recurrent neural network . and then concatenates three representations together as a vector . it is then followed by a fully connected layer and a decoder which generate the answer based on the representation . and here's some analysis of the dataset . as you can see the dataset is much better than the previous work because there are more unique answers . and also the question and answers tend to be longer and here are some results . they actually show the model in the form of a visual chat bot . basically you can chat with and if you guys are interested please try it [laugh] and that's it . so we're gonna start with linear regression . i'm sure all of you if you have taken cs 221 or cs 229 then you have heard and coded up linear regression before . this is just gonna be a start to get us familiarized with tensorflow even better . so we're gonna start at what does linear regression do again . it takes all your data and tries to find the best linear fit to it . so imagine house prices with time for example or location it's probably a linear fit . and so we generate our data set artificially using y equals 2 x plus epsilon where epsilon is sampled from a normal distribution . i won't really go much into how we obtain the data . because that we assume is normal python processing and not really tensorflow so we will move on . 
and actually start implementing linear regression and the function run . so in this first function linear regression we will be actually implementing the graph itself . as [inaudible] said we will be implementing and defining the flow graph . so first we're gonna create our placeholders because we're gonna see how we can feed in our data . so we have two placeholders here x and y . so let's just start with creating x first and this is gonna be of type float so we are gonna make float32 and it's gonna be of shape . so we're gonna make this likely more general and have it of shape none . and what this means that you can dynamically change the number of batches that you can send to your network . or in this case your linear model . and it's just a row vector here . all right and we're gonna name it x . we're gonna create y which is the label and which will also be of the same type and shape as well . all right and we're gonna name it y . all right so now that we have defined our placeholders we're gonna start creating other variables . so we start with first by defining our scope . so let's say tf.variable_scope and we're gonna name it just a lreg because linear regression . and we're gonna call it scope all right . so now that we are here we're gonna create our matrix which is w . so we're gonna call it tf.variable and since it's just a linear regression it'll just be a single integer or not an integer my bad but just one number . and we're gonna randomly initialize it with np.random.random . 
we're gonna start with the normal distribution rather . now we're gonna actually build the graph now that we have defined our variables and placeholders . we're gonna define y_pred which is just prediction and it's gonna be given by tf.mul(w x) . yeah so as i mentioned earlier none in this case is so that you can dynamically change the number of batches that so imagine like if i'm doing hyper parameter tuning i don't want to go and change shape to be 10 or 32 or 256 later on . almost you can imagine that you're i'm gonna change the number of batches that i'm gonna send to my network . so as we mentioned it'll just go into the variable scope and then define the name as it pleases so yeah . so now that we have our prediction the next logical thing is to actually compute the loss . so this we are gonna do by just there are two norm . so let's just first get the norm itself . and that's gonna be given by square and we just do (y_pred- y) . and since we wanted to be of a particular shape its gonna be over reduce some . okay so now with this we have finished building our graph . and so now we'll return x y y_pred and from our linear regression model . now we're gonna actually start computing what's in the graph . and we first start by generating our dataset . and i'm gonna fill in code here which we'll define the training procedure for it . all right so let's get started on that part . so first we get what we call the model . we make an instance of it and that's just gonna be given by this . all right so once we have that we are gonna create our optimizer . 
and this is gonna be given by as barack mentioned earlier in his slides descentoptimizer . and we are gonna define the learning rate to be 0.1 . and we are gonna minimize over the loss that we just got from our model . okay now we are gonna start a session . and we are first gonna initialize yeah that's one thing i forgot . we are gonna first initialize our variables as someone earlier asked . so this is actually a new function . so it's likely different from what barrack mentioned . and this sort of explains our tens of our base really quickly and since the time barrack made the slide then i need the code . it's already been updated so we're going to change that . and now we are gonna run the init function which is just initialization of variables . now we are gonna create our feed_dict . and so what we are gonna feed in is essentially just x_batch and y_batch which you got from our regenerate dataset function . all right now we're gonna actually just loop over our data set multiple times because it's a pretty small dataset . 30s is just our arbitrary chosen here . we are gonna get our loss value and optimize it . i'll explain the step in a second . so now we're gonna call run and what we want to fetch is the loss and the optimizer and we are gonna feed in our feed dict . does anyone have any questions on this line . all right and we are just gonna print for the loss here . 
and then since this is an array we are just going to want the mean because we have almost 101 examples . all right so now that we're done with that we can actually go and train our model but we'd also like to see how it actually ends up performing . so what we are gonna do is we are gonna actually see what it predicts and how we get that is again calling the session.run on y_pred . and our feed dictionary here will be just this . so the optimizer was defined as a gradientdescentoptimizer here . so you can see we are not returning anything for that which is why i just ended up with a blank there . so over here you see i'm yeah all right so we can actually go and start running our code and see how it performs okay . so let's actually go and run our [inaudible] . so you see the loss decrease and we can actually go ahead and see how it turns out . okay i guess it didn't like my tmux . anyways so you see we fed a linear line over the data . all right so now we are actually gonna go and implement word2vec using skip-gram which is slightly gonna be more complex . this was just to see how we create very simple models in tensorflow . this is refine our understanding of word2vec again . the first cs224 homework was a lot of fun . and if you were suppose to make a dataset out of this sentence here . and we are going have the cs221 for a 224 end [inaudible] first . and so we are just basically decomposing other sentence into a data set . remember that skipgram tries to predict each context word given this target word . and since the number of context words here is just two because our window size is one . 
and so the task now becomes to predict d from cs224n . and fun from off and so on . and so this is our data set . so just clarifying what word2vec actually was . so let's go ahead and start implementing that . i've already made the data processing functions here . so we won't have to deal with that . and this function load data already loads the pre-process training and validation set . training data is a list of batch and we have about 30000 of those . and we are going to see a train as well here . the valuation data is just a list of all validation inputs . and the reverse dictionary is a python dictionary from word index to word . so let's start and go ahead and implement skipgram first . so we are again going to start and so this is going to be batch inputs . and we are going to define a placeholder here . but in this case we can define with int32 . and the shape is going to be batch_size and nothing . because we are not going to call multiple variable scopes . then we go and create our batch labels . this will also be of the same shape as previous . 
and finally we will go and create a constant for our validation set . because that is not going to change anytime soon . and that is going to be defined by a val_data which we previously loaded . and we have to define what type it is . and the type for that is int32 again . so since i'll be applying transposes later . i just wanted to make sure that it's one . it doesn't really make that big of a difference . so in this case i'll be calling transpose on labels . which is why i just wanted to make you wouldn't . it's just i wanna make it absolutely not a column vector . so now we can go and all right . so this is where we'll define our model . and first we are going to go and create an embeddings as we all did in our assignment . and that's going to be a huge variable . and it's going to be initialized randomly with uniform distribution . and this is going to kick vocabulary size which you have previously defined in the top . and it's going to take embedding size . so this is going to be a number of words in your dictionary times the size of your embedding size . we just going to also give the parameters for that . 
all right so we just created our embeddings . now since we want to index with our batch . we are going to create batch embeddings . and you are going to use this function . which is actually going to be pretty handy for our current assignment . and so we do an embedding lookup with the embeddings . and we are going to put in the batch inputs here . finally we go and create our weights . and we are going to call it here .variable here . so we are going to use truncated normal distribution here . which is just normal distribution where it's cut off at two standard deviations instead of going up the internet . this is also going to be of the same size as previously . but this is going to be vocabulary size and embedding size . and this is because i turn tracks with our input directly . since this is truncated normal we need to define what the standard deviation is . and this is going be given by one over the square root of the embedding size itself . finally we go and create our biases which are also going to be variables . and this is going to be initialized with zeros of size vocabularies . now we define our loss function now that we have all our variables . so in this case we used a soft max cross entropy in our assignment are the negative log likelihood . 
in this case you'd be using something similar . and this is where tensorflow really shines . it has a lot of loss functions built in . and say we are going to use this called negative constraint negative concentrate . but it is very similar in the sense that the words that need to come up with a higher probability are emphasized . and the words which should not appear with lower probability are not emphasized . and so we are going to call tf.nn . nn is the neural network library in tensorflow our module . and this is going to take a couple of parameters . which is what you're trying to learn . no w is the weight matrix that is a parameter that you're trying to also learn . effectively you can think of these embeddings as sort of semantic representations of those words right . our embeddings is defined as the vocabulary size . so let's say we have 10000 words in our dictionary . and each row is now the word vector that goes with that word . and since our batch is only a subset of the vocabulary we need to index into that eh matrix . with our batch which is why we used the embedding lookup function okay . all right so we're gonna go and just use this api obviously everyone would need to look up on the tensorflow website itself . but what this would do is now take the weights and the biases and the labels as well . and they also take an input which is batch_inputs . 
and so here's where tensorflow really shines again the num_sampled . so in our data set we only have positive samples or in the sense that we had the context words and the target word . we also need context words and noisy words . we have defined num_samples to be 64 earlier . and what it would essentially do is look up 64 words which are not in our training set and which are noise words . and this would serve as sort of negative examples so that our network learns which words are actually context words and which are not . and finally our num_classes is defined by our vocabulary size again . with that we have defined our loss function . and now we have to take the mean of that because loss needs to be and we get that by reduced mean . so we get the loss is given for that particular batch . and since we have multiple samples in a batch we want to take the average of those . and now we have completely defined our loss function . now we can go ahead and actually if you remember from the assignment we take the norm of these word vectors . so let's go ahead and do that first . so that will be reduce_mean this is just api calling which is very valuable and detailed on the tensorflow website itself . so this is where in this i have added an argument called keep dimensions . and this is where if you sum over a dimension you don't it to disappear but just leave it as 1 . and now we divide the embeddings with the norm to get the normalized_embeddings . and now we return from we get batch inputs we return batch labels because this will be our feed . with this done we can come back to this function later . 
so now we go and define our run function . we actually make a object of our model . and loss from our function which was just called word2 or skipgram rather . okay and now we initialize the session . and over here again i forgot to initialize our variables . we just initialized all of our variables for the default values as barak mentioned again . now we are gonna go and actually loop over our data to see if we can actually go ahead and train our model . and so let's actually do that first step . so for each iteration in this for loop we are gonna obtain a batch which has its input data as well as the labels . okay so we have inputs and labels from our batch . and we can now define our feed_dictionary accordingly where the batch_inputs . and our batch_labels would just be labels . we go ahead and and we do this by calling session.run where we fetch the optimizer again and the loss . and we pass in our feed dictionary which we already defined above . and since we are trying to get the average we're gonna add it first and then divide by the number of examples that we just saw . so we're just gonna put a couple of print statements now just to make sure to see if our model actually goes and trains . since the loss will be zero in the first step we can just [inaudible] . all right so and we reset our average loss again just so that we don't for every iteration loop . we have almost finished our implementation here . so we can define that as the beginning of a run step . 
and we'll take a learning rate of zero and we're gonna minimize the loss . one thing that we're missing here is we haven't really dealt with our value addition set . so although we are training in our training set we would wanna make sure that it actually generalizes to the value addition set . and that's the last part that's missing . and we just gonna do that now . but before we do that there's only one step missing . where we once we have the validation set we still need to see how similar our word vectors are with that . and we do that in our flow graph itself . so let's go back to our skip gram function . anyway here we can implement that okay . so we have our val_embeddings against index into the embeddings matrix to get the embeddings that correspond to the validation words . and we use the embedding look up function here embedding_lookup embedding and we call in train data set or val data set . we'll actually use the normalized_embedding because we are very concerned about the cosine similarity and not necessarily about the magnitude of the similarity . okay and the similarity is essentially just a cosine similarity . so how this works is we matrix multiply the val_embeddings which we just obtained and the normalized_embeddings . and since they won't work you can just may just multiply both of them because of dimensional incompatibility we'll have to transpose_b . all right since we also returned this from our function again this is just a part of the graph . and we need to actually execute the graph in order to obtain values from it . okay and let's do since this is a pretty expensive process computationally expensive let's do this only every 5000 iterations . so the way you're gonna do this is by calling eval on the similarity matrix what this does is since we had this noted it actually goes and evaluates . 
this is equal on to calling session.run on similarity and fetching that okay . so we go on call and we get similarity and for every word in our validation set you gonna find the top_k and we can define k to be 8 here . and we will now get the nearest words . and we'll sort it according to their magnitude . and since the first word that will be closest will be the word itself we'll want the other eight words and this will be given by top_k+1 any questions so far . right so your embedding is on number of words you have in your vocabulary times the size of your word embedding for each word . so it's a huge matrix and since your batch that you're currently working with is only subset of that vocabulary this function embedding to lookup actually indexes into that matrix for you and obtains the word . this is the equivalent to some complicated python splicing that you do with matrices but it's just good syntax should go over it . so we have our nearest key words we'll just go and i have this function in my utils you can check this on the github that we'll post after the class is over . and you can play with it as you wish and in the past a nearest and a reverse_dictrionary gesture actually see the words and not just numbers all right . finally be open our final_embeddings which will be a normalized_embedding at the end of the train and on that again which is equal to calling session.run and passing and fetching this . all right we are done with the coding and we can actually see and visualize how this performs . and python word2vec oops i missed a bracket again . so we'll first load up our data set and then it will iterate over it and we will use our scripting model . you know why please tell me why have to be there okay . so as you can see here we have 30000 batches each with a bad set of 128 . all right so as we see the loss started off as 259 all right ends up at 145 and then decreases i think it goes somewhere to around 6 . here we can also see as a printing the nearest word for this is e leaders orbit this gets better with time and with more training data . we only use around 30000 examples to actually get better . and in the interest of time i'm only limited to around 30 epochs yes . 
so tensorflow comes with tensorboard which i didn't show in the interest of time . essentially you can go up to your local host and then see the entire graph and how it's organized . and so that'll actually be a huge debugging help and you can use that for your final project . all right well thank you for your time . welcome to lecture seven or maybe it's eight . definitely today is the beginning of where we talk about models that really matter in practice . we'll talk today about the simplest recurrent neural network model one can think of . but in general this model family is what most people now use in real production settings . we only have a little bit of math in between and a lot of it is quite applied and should be quite fun . just one organizational item before we get started . i'll have an extra office i'll be again on queuestatus 68 or so . last week we had to end at 8:30 . and there's still a lot of i'll be here after class for probably another two hours or so . all right then let's take a look at the overview for today . so to really appreciate the power of recurrent neural networks it makes sense to get a little bit of background on traditional language models . which will have huge ram requirements and won't be quite feasible in their best kinds of settings where they obtain the highest accuracies . and then we'll motivate recurrent neural networks with language modeling . it's a very important kind of fundamental task in nlp that tries to predict the next word . something that sounds quite simple but is really powerful . and then we'll dive a little bit into the problems that you can actually quite easily understand once you have figured out how to take gradients and you actually understand what backpropagation does . 
and then we can go and see how to extend these models and apply them to real sequence tasks that people really run in practice . all right so let's dive right in . so basically we want to just compute the probability of an entire sequence of words . and you might say well why is that useful . why should we be able to compute how likely a sequence is . and actually comes up for a lot of different kinds of problems . so one for instance in machine translation you might have a bunch of potential translations that a system gives you . and then you might wanna understand which order of words is the best . so "the cat is small" should get a higher probability than "small the is cat" . that you translate from it might not be as obvious . and the other language might have a reversed word order and whatnot . another one is when you do speech recognition for instance . it also comes up in the machine translation a little bit where you might have well this particular example is clearly more a machine translation example . but comes up also in speech recognition where you might wanna understand which word might be the better choice given the rest of the sequence . so "walking home after school" sounds a lot more natural than "walking house after school" . but home and house have the same translation or same word in german which is haus h a u s . and you want to know which one is the better one for that translation . so comes up in a lot of different kinds of areas . now basically it's hard to compute all potential sequences 'cause there are a lot of them . and so what we usually end up doing is we basically condition on just a window we try to predict the next word based on the just the previous n words before the one that we're trying to predict . 
so this is of course an incorrect assumption . the next word that i will utter will depend on many words in the past . but it's something that had to be done to use traditional count based machine learning models . so basically we'll approximate this overall sequence probability here with just a simpler version . in the perfect sense this would basically be the product here of each word given all preceding words from the first one all the way to the one just before the i_th one . but in practice this probability with traditional machine learning models we actually approximate that with some number of n words just before each word . so this is a simple markov assumption just assuming the next action or next word that is uttered just depends on n previous words . and now if we wanted to use traditional methods that are just basically based on the counts of words and not using our fancy word vectors and so on . then the way we would compute and estimate these probabilities is essentially just by counting how often does if you want to get the probability for the second word given the first word . we would just basically count up how often do these two words co-occur in this order divided by how often the first word appears in the whole corpus . let's say we have a very large corpus and we just collect all these counts . and now if we wanted to condition not just on the first and the previous word but on the two previous words then we'd have to compute all these counts . and now you can kind of sense that well if we want to ideally condition on as many n-grams as possible before but we have a large vocabulary of say 100000 words then we'll have a lot of counts . essentially 100000 cubed many numbers we would have to store to estimate all these probabilities . are there any questions for these traditional methods . all right now the problem with that is that the performance usually improves as we have more and more of these counts . but also you now increase your ram requirements . and so one of the best models of this traditional type actually required 140 gigs of ram for just computing all these counts when they wanted to compute them for 126 billion token corpus . so it's very very inefficient in terms of ram . and you would never be able stores all these different n-gram counts . 
you could never store it in a phone or any small machine . and now of course once computer scientists struggle with a problem like that they'll find ways to deal with it and so there are a lot of different ways you can back off . you say well if i don't find the 4-gram or i didn't store it because it was not frequent enough then maybe i'll try the 3-gram . and if i can't find that or i don't have many counts for that then i can back off and estimate my probabilities with fewer and fewer words in the context size . but in general you want to have at least tri or 4-grams that you store and the ram requirements for those are very large . that you'll observe in a lot of comparisons between deep learning models and traditional nlp models that are based on just counting words for specific classes . the more powerful your models are sometimes the ram requirements can get very large very quickly and there are a lot of different ways people tried to combat these issues . now our way will be to use recurrent neural networks . where basically they're similar to the normal neural networks that we've seen already but they will actually tie the weights between different time steps . and as you go over it you keep using linear plus non-linearity layer . and that will at least in theory allow us to actually condition what we're trying to predict on all the previous words . and now here the ram requirements will only scale with the number of words not with the length of the sequence that we might want to condition on . again they're you'll see different kinds of visualizations and i'm introducing you to a couple . i like sort of this unfolded one where we have here a abstract hidden time step t and we basically it's conditioned on h_t-1 and then here you compute h_t+1 . but in general the equations here are quite intuitive . we assume we have a list of word vectors . for now let's assume the word vectors are fixed . later on we can actually loosen that assumption and get rid of it . and now at each time step to compute the hidden state . at that time step will essentially these two linear layers matrix vector products and we sum them up . 
and that's essentially similar to saying we concatenate h_t-1 and the word vector at time step t and we also concatenate these two matrices . and then we apply an element-wise non-linearity . so this is essentially just a standard single layer neural network . and then on top of that we can use this as a feature vector or as our input to our standard to get an output probability for instance over all the words . this out in this formulation is basically the probability that the next word is of this specific at this specific index j conditioned on all the previous words is essentially the j_th element of this large output vector . so here you can have different some people just use u v but here we basically use the superscript just identify which matrix we have . and these are all different matrices so w_(hh) the reason we call it hh is it's the w that computes the hidden layer h given the input h t- 1 . and then you have an h_x here which essentially maps x into the same vector space that we have . our hidden states in and the weights of the softmax classifier . and so let's look at the dimensions here . so why do we concatenate and not add is the question . same notation plus w_(hx) times x then this is actually the same thing . and so this will now basically be a vector and we are feed in linearity but it doesn't really change things so let's just look at this inside part here . now if we concatenated h and x together we're now have and let's say x here has a certain dimensionality which we'll call d . so x is in r_d and our h will define to be in for having the dimensionality r_(dh) . now what would the dimensionality be if we concatenated these two matrices . so we have here the output has to be again a dh matrix . and now this vector here is a what dimensionality does this factor have when we concatenate the two . so this is a d plus dh times one and here we have dh times our matrix . it has to be the same dimensionality so d plus dh and that's why we could essentially concatenate here w_h in this way and w_hx here . 
and now we could basically multiply these . and if you again if this is confusing you can write out all the indices . and you realize that these two are exactly the same . right so as you sum up all the values here it'll essentially just get summed up also it doesn't matter if you do it in one go or not . just a single layer and that worked where you compact in two inputs but it's in many cases for recurrent neutral networks is written this way . so now here are two other ways you'll often see these visualized . this is kind of a not unrolled version of a hidden of a recurrent neural network . i actually find these kinds of unrolled versions the most intuitive . it's essentially the word vector for the word that appears at the t_th time step . as opposed to x_t and intuitively here x_t you could define it in any way . it's really just like as you go through the lectures you'll actually observe different versions but intuitively here x_t is just a vector at xt but here xt is already an input and what it means in practice is you actually have to now go at that t time step find the word identity and pull that word vector from your glove or word to vec vectors and get that in there . so x_t we used in previous lectures as the t_th element for instance in the whole embedding matrix all our word vectors . so this is just to make it very explicit that we look up the identity of the word at the tth time step and then get the word vector for that identity like the vector in all our word vectors . so i'm showing here a single layer neural network at each time step and then the question is whether that is standard or just for simplicity . it is actually the simplest and still somewhat useful . variant of a recurrent neural network though we'll see a lot of extensions even in this class and then in the lecture next week we'll go to even better versions of these kinds of recurrent neural networks . but this is actually a somewhat practical neural network though we can improve it in many ways . now you might be curious when you just start your sequence and this is age 0 here and there isn't any previous words . what you would do and the simplest thing is you just initialize the vector for the first hidden layer at the first or the 0 time step as just a vector of all 0s . right and this is the x[t] definition you had just describe through the column vector of l which is our embedding matrix at index [t] which the time step t . 
all right so it's very important to keep track properly of all our dimensionality . here w(s) to softmax actually goes over the size of our vocabulary v times the hidden state . so the output here is the same as the vector of the length of the number of words that we might wanna to be able to predict . all right any questions for the feed forward definition of a recurrent neural network . all right so how do we train this . well fortunately we can use all the same machinery we've already introduced and carefully derived . so basically here we have probability distribution over the vocabulary and we're going to use the same exact cross entropy loss function that we had before but now the classes are essentially just the next word . so this actually sometimes 'cause now technically this is unsupervised in the sense that you just give it raw text . but this is the same kind of objective function we use when we have supervised training where we have a specific class that we're trying to predict . so the class at each time step is just a word index of the next word . and you're already familiar with that here we're just summing over the entire vocabulary for each of the elements of y . and now in theory you could just . to evaluate how well you can predict the next word over many different words in longer sequences you could in theory just take this negative of the average log probability is over this entire dataset . but for maybe historical reasons and also reasons like information theory and so on that we don't need to get into what's more common is actually to use perplexity . so that's just 2 to the power of this value and hence we want to basically be less perplexed . so the lower our perplexity is the less the model is perplexed or confused about what the next word is . and we essentially ideally we'll assign a higher probability to the word that actually appears in the longer sequence at each time step . any reason why 2 to the j . yes but it's sort of a rat hole we can go down maybe after class . information theory bits and so on it's not necessary . 
now you would think well this is pretty simple we have a single set of w be relatively straightforward . sadly and this is really the main drawback of this and a reason of why we introduce all these other more powerful recurrent neural network models training these kinds of models is actually incredibly hard . and we can now analyze using the tools of back propagation and chain rule and all of that . now we can analyze and understand why that is . so basically we're multiplying here the same matrix at each time step right . so you can kind of think of this matrix multiplication as amplifying certain patterns over and over again at every single time step . and so in a perfect world we would want the inputs from many time steps ago to actually be able to still modify what we're trying to predict at a later much later time step . and so one thing i would like to encourage you to do is to try to take the derivatives with respect to these ws if you just had a two or three word sequence . it's a great exercise great preparation for the midterm . and it'll give you some interesting insights . now as we multiply the same matrix at each time step during foreprop we have to do the same thing during back propagation we have remember our deltas our air signals and sort of the global elements of the gradients . they will essentially at each time step flow through this network backwards . so when we take our cross-entropy loss here we take derivatives we back propagate we compute our deltas . now the first time step here that just happened close to that output would make a very good update and will probably also make a good update to the word vector here if we wanted to update those . but then as you go backwards in time what actually will happen is your signal might get either too weak or too strong . and that is essentially called the vanishing gradient problem . as you go backwards through time and you try to send the air signal at time step t many time steps into the past you'll have the vanishing gradient problem . so what does that mean and how does it happen . let's define here a simpler but similar recurrent neural network that will allow us to give you an intuition and so here we essentially just say all right instead of our original definition where we had some kind of f some kind of non-linearity here we use the sigma function you could use other one . first introduce the rectified linear units and so on instead of applying it here we'll apply it in the definition just right in here . 
and then let's assume for now we don't have the softmax . we just have here a standard a bunch of un-normalized scores . which really doesn't matter for the math but it'll simplify the math . now if you want to compute the total error with respect to an entire sequence with respect to your w then you basically have to sum up all the errors at all the time steps . at each time step we have an error of how incorrect we were about predicting the next word . and that's basically the sum here and now we're going to look at the element at the t timestamp of that sum . so let's just look at a single time step a single error at a single time step . and now even computing that will require us to have a very large chain rule application because essentially this error at time step t will depend on all the previous time steps too . de_t over dy_t so the t the hidden state . but then you have to multiply that with the partial derivative of yt with respect to the hidden state . so that's just that's just this guy right here or this guy for ht . this one here but it also depends on that one and that one and the one before that and so on . and so that's why you have to sum over all the time step from the first one all the way to the current one where you're trying to predict the next word . and now each of these was also computed with a w so you have to multiply partial of that as well . now let's dig into this a little bit more . and you don't have to worry too much if this is a little fast . you won't have to really go through all of this but the math that we've done before . so you can kind of feel comfortable for the most part going over it at this speed . so now remember here our definition of h_t . we basically have all these partials of all the h_t's with respect to the previous time steps the h's of the previous time steps . 
now to compute each of these we'll have to use the chain rule again . and now what this means is essentially a partial derivative of a vector with respect to another vector . something that if we're clever with our backprop definitions before we never actually have to do in practice right . 'cause this is a very large matrix and we're combining the computation with the flow graph and our delta messages before such that we don't actually have to compute explicitly these jacobians . but for the analysis of the math here we'll basically look at all the derivatives . so just because we haven't defined it what's the partial for each of these is essentially called the jacobian where you have all the partial derivatives with respect to each element of the top here ht with respect to the bottom . and so in general if you have a vector valued function output and a vector valued input and you take the partials here you get this large matrix of all the partial derivatives with respect to all outputs . and now we got this beast which is essentially a matrix . and we multiply for each partial here we actually have to multiply all of these right . so this is a large product of a lot of these jacobians . now we can try to simplify this and just say all right . let's say there is an upper bound . and we also the derivative of h with respect to h_j . actually with this simple definition of each h actually can be computed this way . and now we can essentially upper bound the norm of this matrix with the multiplication of basically these equation right here where we have w_t . and if you remember our backprop equations you'll see some common terms here but we'll actually write this out as not just an element wise product . but we can write the same thing as a diagonal where we have instead of the element wise . elements we basically just put them into the diagonal of a larger matrix and with zero path everything that is off diagonal . now we multiply these two norms here . and now we just define beta w and beta h as essentially the upper bounds . 
some number single scalar for each as like how large they could maximally be right . we have w we could compute easily any kind of norm for our w right . it's just a matrix computed matrix norm we get a single number out . and now basically when we write this all we put all this together then we see that an upper bound for this jacobians is essentially for each one of these elements as this product . and if we define each of the elements here in terms of their upper bounds beta then we basically have this product beta here taken to the t- k power . and so as the sequence gets longer and larger it really depends on the value of beta to have this either blow up or get very very small right . if now the norms of this matrix for instance that norm and then you have control over that norm right . you initialize your wait matrix w with some small random values initially before you start training . if you initialize this to a matrix that has a norm that is larger than one then at each back propagation step and the longer the time sequence goes . you basically will get a gradient that is going to explode cuz you take some value that's larger than one to a large power here . say you have 100 or something and your norm is just two then you have two to the 100th as an upper bound for that gradient and vice-versa . if you initialize your matrix w in the beginning to a bunch of small random values such that the norm of your w is actually smaller than one then the final gradient that will be sent from ht to hk could become a very very small number right half to the power of 100th . basically none of the errors will arrive . none of the error signal we got small and further backwards in time . so if the gradient here is exploding does that mean a word that is further away has a bigger impact on a word that's closer . and the answer is when it's exploding like that you'll get to not a number in no time . and that doesn't even become a practical issue because the numbers will literally become not a number cuz it's too large a value to compute . and we'll have to think of ways to come back . it turns out the exploding gradient problem has some really great hacks that make them easier to deal with than the vanishing gradient problem . and we'll get to those in a second . 
all right so now you might say this could be a problem . now why is the vanishing gradient problem an actual common practice . and again it basically prevents us from allowing a word that we're trying to break in terms of the next word . and so here a couple of examples from just language modeling where that is a real problem . so let's say for instance you have jane walked into the room . now you can put an almost probability mass of one that the next word in this blank is john right . but if now each of these words have the word vector you type it in to the hidden state you compute this . and now you want the model to pick up the pattern that if somebody met somebody else and your all this complex stuff . and then they said hi too and the next thing is the name . you wanna put a very high probability on it but you can't get your model to actually send that error signal way back over here to now modify the hidden state in a way that would allow you to give john a high probability . and really this is a large problem in any kind of time sequence that you have . and many people might intuitively say well language is mostly a sequence problem right . you have words that appear from left to right or in some temporal order as we speak . and so this is a huge problem . and now we'll have a little bit of code that we can look into . but before that we'll have the awesome shayne give us a little bit of an intercession intermission . from recurrent neural networks to talk about transition-based dependency parsing which is exactly what you guys saw this time last week in lecture . so just as a recap a transition-based dependency parser is a method of taking a sentence and turning it into dependence parse tree . and you do this over and over again in a greedy fashion until you have a full transition sequence which itself encodes the dependency so i wanna show you how to get from the model that you'll be implementing in your assignment two question two which you're hopefully working on right now to syntaxnet . syntaxnet is a model that google came out with and they claim it's the world's most accurate parser . 
and it's new fast performant tensorflow framework for syntactic parsing is available for over 40 languages . the one in english is called the parse mcparseface . a little bit here but hopefully you can read through it . so basically the baseline we're gonna begin with is the chen and manning model which came out in 2014 . and chen and manning are respectively your head ta and instructor . and the models that produce syntaxnet in just two stages of improvements those directly modified chen and manning's model which is exactly what you guys will be doing in assignment two . and so we're going to focus today on the main bulk of these changes modifications which were introduced in 2015 by weiss et al . so without further ado i'm gonna look at their three main contributions . so the first one is they leverage unlabeled data using something called tri-training . their neural network and made some slight modifications . and the last and probably most important is that they added a final layer on top of the model involving a structured perceptron with beam search . so each of these seeks to solve a problem . so as you know in most supervised models they perform better the more data that they have . and this is especially the case for dependency parsing where as you can imagine there are an infinite number of possible sentences with a ton of complexity and you're never gonna see all of them and you're gonna see even some of them very very rarely . so the more data you have the better . a ton of unlabeled data and two highly performing dependency parsers that were very different from each other . and when they agreed independently agreed on a dependency parse tree for a given sentence then that would be added to the labeled data set . and so now you have ten million new tokens of data that you can use in addition to what you already have . and this by itself improved performance by 1% using the unlabeled attachment score . so the problem here was not having enough data for the task and they improved it using this . 
the second augmentation they made was by taking the existing model which is the one you guys are implementing which has an input layer consisting of the word vectors . the vectors for the part of speech tags and the arc labels with one hidden layer and one soft max layer predicting which transition and they changed it to this . now this is actually pretty much the same thing except for three small changes . there are two hidden layers instead of one hidden layer . the second is that they used a relu nonlinearity function instead of the cube nonlinearity function . and the third and most important is that they added a perceptron layer and notice that the arrows that it takes in the previous layers in the network . so this perceptron layer wants to solve one particular problem and this problem is that greedy algorithms aren't able to really look ahead . they make short term decisions and recover from one incorrect decision . so what they said is let's allow the network then to look ahead and which we can search over and this tree is the tree of all the possible so each edge is a possible transition form the state that you're at . as you can imagine even with three transitions your tree is gonna blossom very very quickly and you can't look that far ahead and explore all of the possible branches . so what you have to do is prune some branches . and for that they use beam search . now beam search is only gonna keep track of the top k partial transition now how do you decide which k . you're going to use a score computed you guys probably have a decent idea at this point of how perceptron works . the exact function they used is shown here and i'm gonna leave up the annotations so you can take a look at it later if you're interested . but basically those are the three things that they did solve the problems with the previous chen & manning model . so in summary chen & manning had an unlabeled attachment score of 92% already phenomenal performance . they boosted it to 94% and then there's only 0.6% which is google's 2016 state of the art model . and if you're curious what the did to get that 0.6% take a look at andrew all's paper which uses global normalization instead of local normalization . so the main takeaway and it's pretty straight forward but i can't stress it enough is when you're trying to improve upon an existing model you need to identify the specific in this case the greedy algorithm and solved those problems specifically . 
in this case they did that using semi-supervised method using unlabeled data . they tune the model better and they use the structured perception with beam search . you can now look at these kinds of pictures and you totally know what's going on . and in like state of the art stuff the world publishes . all right so we'll gonna through a little bit of like a practical python notebook sort of implementation that shows you a simple version of the vanishing gradient problem . where we don't even have a full recurrent real network we just have a simple two layer neural network and even in those kinds of networks you will see that the error that you start at the top and the norm of the gradients as you go down through your network the norm is already getting smaller . and if you remember these were the two equations where i said if you get to the end of those two equations you know all the things that you need to know and you'll actually see these three equations in the code as well . let me get out of the presentation now zoom in . so here we're going to define a super simple problem . this is a code that we started and 231n (with andrej) and we just modified it to make it even simpler . so let's say our data set to keep it also very simple is just this kind of classification data set . where we have basically three classes the blue yellow and red . and they're basically in the spiral clusterform . we're going to define our simple nonlinearities . you can kind of see it as a solution almost to parts of the problem set which is why we're only showing it now . and we'll put this on the website too so no worries . but basically you could define here f element-wise and the gradients for them . so this is f and f prime if f is a sigmoid function . we'll also look at the relu the other nonlinearity that's very popular . and here we just have the maximum between 0 and x and very simple function . 
now this is a relatively straight forward definition and three layer neural network . has this input here our nonlinearity our data x just these points in two it's one of those three classes . we'll have this model here we have our step size for sdg and our regularization value . now these are all our parameters w1 w2 and w3 for all the outputs and variables of the hidden states . is the relu then we have here relu and we just input x multiply it . and in this case your x can be the entirety of the dataset cuz the dataset's so small each mini-batch we can essentially do a batch . again if you have realistic datasets you wouldn't wanna do full batch training but we can get away with it here . we multiply w1 times x plus our bias terms and rectified linear units or relu . then we've computed in layer two same idea . but now it's input instead of x is the previous hidden layer . and then we compute our scores this way . and then here we'll normalize our scores with the softmax . so very similar to the equations that we walk through . and now it's just basically an if statement . either we have used relu as our activations or we use a sigmoid but the math inside is the same . all right now we're going to compute our loss . our good friend the simple average cross entropy loss plus the regularization . so here we have negative log of the probabilities we summed them up overall the elements . and then here we have our regularization as the l2 standard l2 regularization . and we just basically sum up the squares of all the elements in all our parameters and i guess it does cut off a little bit . 
all three have the same of we add that to our final loss . and now every 1000 iterations see what's happening . and this is something you always want to do too . you always wanna visualize see what's going on . and hopefully a lot of this now looks very familiar . maybe if implemented it not quite as efficiently as efficiently in problem set one but maybe you have and then it's very very straightforward . now that was the forward propagation we can compute our error . now we're going to go backwards and we're computing our delta messages first from the scores . and now we have the hidden layer activations transposed times delta messages to compute w . again remember we have always for each w here we have this outer product . and that's the outer product we see right here . and now the softmax was the same regardless of whether we used a value or a sigmoid . we now basically have our delta scores and have here the product . so this is exactly computing delta for the next layer . and that's exactly this equation here and just python code . and then again we'll have our updates dw which is again this outer product right there . so it's a very nice sort of equations code almost a nice one to one mapping between the two . all right now we're going to go through the network from the top down to the first layer . and now we add the derivatives for our regularization . in this case it's very simple just matrices themselves times the regularization . 
and we combine all our gradients in this data structure . and then we update all our parameters with our step_size and sgd . all right then we can evaluate how well we do on the training set so that we can basically print out the training accuracy as we train us . all right now we're going to initialize all the dimensionality . we compute our hidden sizes of the hidden vectors . let's say they're 50 it's pretty large . all right we'll train it with both sigmoids and rectify linear units . and now once we wanna analyze what's going on we can essentially now plot some of the magnitudes of the gradients . so those are essentially the updates as we do back propagation through the snap work . and what we'll see here is the first and the second layer when we use sigmoid non-linearities . and basically here the main takeaway messages that blue is the first layer and green is the second layer . so the second layer is closer to the softmax closer to what we're trying to predict . and hence it's gradient is usually had larger in magnitude than the one that arrives at the first layer . and now imagine you do this 100 times . and you have intuitively your vanishing gradient problem in recurrent neural networks . they're already almost half in size over the iterations when you just had two layers . and the problem is a little less strong when you use rectified linear units . but even there you're going to have some decrease as you continue to train . all right any questions around this code snippet and vanishing creating problems . the question is why are the gradings flatlining . 
and it's essentially because the dataset is so perfectly fitted your training data . and then there's not much else to do you're basically in a local optimum and then not much else is happening . so yeah so these are the outputs where if you visualize the decision boundaries here at the relue and the relue you see a little bit more sort of edges because you have sort of linear parts of your decision boundary and the sigmoid is a little smoother little rounder . all right so now you can implement a very quick versions to get an intuition for the vanishing gradient problem . now the exploding gradient problem is in theory just as bad . but in practice it turns out we can actually have a hack that was first introduced by unmathematical in some ways 'cause say all you have is a large gradient of 100 . that's it you just define the threshold and you say whenever the value is larger than a certain value just cut it . totally not the right mathematical direction anymore . but turns out to work very well in practice yep . so vanishing creating problems how would you cap it . but then it's like it might overshoot . and you don't want to have the hundredth word unless it really matters . you can't just make all the hundred words or thousand words of the past all matter the same amount . that doesn't make that much sense either . so this gradient clipping solution is actually really powerful . and then a couple years after it was introduced yoshua bengio and one of his students actually gained it's something i encourage you always to do too . not just in the equations where you can write out recurrent neural network where everything's one dimensional and the math comes out easy and you gain intuition about it . but you can also and this is what they did here implement a very simple recurrent neural network which just had a single hidden unit . not very useful for anything in practice but now with the single unit w . and you know at still the bias term they can actually visualize exactly what the air surface looks like . 
so and oftentimes we call the air surface or the energy landscape or so that the landscape of our objective function . you can see here the size of the z axis here is the error that you have when you trained us on a very simple problem . i forgot what the problem here was but it's something very simple like keep around this unit and remember the value and then just return that value 50 times later . and what they essentially observe is that in this air surface or air landscape you have these high curvature walls . and so as you do an update each little line here you can interpret as what happens at an sg update step . and you say in order to minimize my objective function right now of my one hidden unit and my bias term just like by this amount to go over here go over here . and all of a sudden you hit these large curvature walls . and then your gradient basically blows up and it moves you somewhere way different . and so intuitively what happens here is if you rescale to the thick size with the special method then essentially you're not going to jump to some crazy faraway place but you're just going to stay in this general area that seemed useful before you hit that curvature wall . so the question is intuitively why wouldn't such a trick work for the vanishing grading problem but it does work for the exploding grading problem . why does the reason for the vanishing does not apply to the exploding grading problem . so intuitively this is exactly the issue here . so the exploding you basically jump out of the area where you in this case here for instance we're getting closer and closer to a local optimum but the local optimum was very and without the gradient problem without the clipping trick you would go way far away . right now on the vanishing grading problem it get's smaller and smaller . so in general clipping doesn't make sense let's say so that's the obvious answer . you can't something gets smaller and smaller it doesn't help to have a maximum and then make it you know cut it to that maximum 'cause that's not the problem . that's kind of most obvious intuitive answers . why couldn't you if it gets below a certain threshold blow it up . and now you're 50 time steps away . and really the 51st doesn't actually impact the word you're trying to predict at time step t right . 
so you're 50 times to 54 and it doesn't really modify that word . and now you're artificially going to blow up and make it more important . so that's less intuitive than saying i don't wanna jump into some completely different part of my error surface . the wall just comes from this is what the error surface looks like for a very very simple recurrent node network with a very simple kind of problem that it tries to solve . of the networks that you have you can try to make them have just two parameters and then you can visualize something like this too . in fact it's very intuitive sometimes to do that . when you try different optimizers we'll get to those in a later lecture like adam or sgd or achieve momentum we'll talk about those soon . you can actually always try to visualise that in some simple kind of landscape . this just happens to be the landscape that this particular recurrent neural network problem has with one-hidden unit and just a bias term . so the question is how could we know for sure that this happens with non-linear actions and multiple weight . so you also have some non-linearity here in this . so that intuitively wouldn't prevent us from transferring that knowledge . we can't really visualize a very high dimensional spaces . there is actually now an interesting i think by ian goodfellow where you can actually try to let's say you have your parameter space inside your parameter space you have some kind of cross function . so you say my w matrices are at this value and so on and i have some error when all my values are here and then i start to optimize and i end up somewhere here . now the problem is we can't visualize it because it's usually in realistic settings you have the 100 million . at least a million or so parameters sometimes 100 million . and so something crazy might be going on as you optimize between this . and so because we can't visualize it and we can't even sub-sample it because it's such a high-dimensional space . from where they started with their random initialization before optimization . 
and end the line all the way to the point where you actually finished the optimization . and then you can evaluate along this line at a certain intervals you can evaluate how big your area is . and if that area changes between two such intervals a lot then that means we have very high curvature in that area . so that's one trick of how you might use this idea and gain some intuition of the curvature of the space . but yeah only in two dimensions can we get such nice intuitive visualizations . so the question is why don't and the question of course it's a legit question but ideally we'll let the model figure this out . ideally we're better at optimizing the model and long range dependencies . in fact when you implement these and you can start playing around with this and this is something i encourage you all to do too . as you implement your models you can try to make it a little bit more interactive . have some ipython notebook give it a sequence and look at the probability of the next word . and then give it a different sequence where you change words like ten time steps away and look again at the probabilities . and what you'll often observe is that after seven words or so the words before actually don't matter especially not for these simple recurrent neural networks . but because this is a big problem there are actually a lot of and so the biggest and best one is one we'll introduce next week . but a simpler one is to use rectified linear units and to also initialize both of your w's to ones from hidden to hidden and the ones from the input to the hidden state with the identity matrix . and this is a trick that i introduced a couple years ago and then it was sort of combined with rectified linear units . and applied to recurrent neural networks by quoc le . and so the main idea here is if you move around in your space . let's say you have your h and usually we have here our whh times h plus whx plus x . and let's assume for now that h and x have the same dimensionality . so then all these are essentially square matrices . 
and we have here our different vectors . now in the standard initialization what you would do is you'd just have a bunch of small random values and all the different elements of w . and what that means is as you start optimizing whatever x is you have some random projection into the hidden space . instead the idea here is we actually maybe you can scale it so instead you have a half times the identity and what does that do . intuitively when you combine the hidden state and the word vector . if this is an identity initialized matrix . so it's just 1 1 1 1 1 1 on the diagonal . and you multiply all of these by one half . same as just having a half a half a half and so on . and you multiply this with this vector and you do the same thing here . what essentially that means is that you have a half times that vector plus half times that other vector . the beginning if you don't know anything . let's not do a crazy random projection into the middle of nowhere in our parameter space but just average . and say well as i move through the space my hidden state is just a moving average of the word vectors . and then i start making some updates . and it turns out when you look here and you apply this to the very tight problem of mnist . which we don't really have to go into but its a bunch of small digits . and they're trying to basically predict what digit it is by going over instead of using other kinds of neural networks like convolutional neural networks . and basically we look at the test accuracy . and the test accuracy for these is much much higher . 
when you use this identity initialization instead of random initialization and also using rectified linear units . now more importantly for real language modeling we can compare recurrent neural so we had the question before like do these actually matter or did i just kind of describe single layer recurrent neural networks for the class to describe the concept . and here we actually have these simple recurrent neural networks and we basically compare . this one is called kneser-ney with 5 grams so a lot of counts and some clever back off and smoothing techniques which we won't need to get into for the class . and we compare these on two different corpora and we basically look at the perplexity . so these are all perplexity numbers and we look at the neural network or the neural network that's combined with kneser-ney assuming probability estimates . and of course when you combine the two then you don't really get the advantage of having less ram . so ideally this by itself would do best in general combining the two used to still work better . these are results from five years ago and they failed most very quickly . i think the best results now are pure neural network language models . but basically we can see that compared to kneser-ney even back then the neural network actually works very well . and has much lower perplexity than just the kneser-ney or just account based . now one problem that you'll observe in a lot of cases is that the softmax is really really large . so your word vectors are one set of parameters but your softmax is another set of parameters . and if your hidden state is 1000 and let's say you have 100000 different words . then that's 100000 times 1000 dimensional matrix that you'd have to multiply with the hidden state at every single time step . so that's not very efficient and so one way to improve this is with a class-based word prediction . where we first try to predict some class that we can come up and there are different kinds of things we can do . in many cases you can sort just the words by how frequent they are . and say the thousand most frequent the next thousand most frequent words in the second class and so on . 
and so you first basically classify try to predict the class based on the history . and then you predict the word inside that class based on that class . and so this one is only a thousand dimensional and so you can basically do this . and now the more classes the better the perplexity but also the slower the speed the less you gain from this . and especially at training time which is what we see here so if you have just very few classes you can actually reduce the number here of seconds that each eproc takes . by almost 10x compared to having more classes or even more than 10x if you have the full softmax . and even the test time is faster cuz now you only essentially evaluate the word probabilities for the classes that have a very high probability here . this is maybe obvious to some but it wasn't obvious to others even in the past when people published on this . to do a single backward's pass through the sequence . once you accumulate all the deltas from each error at each time set . so looking at this figure really quick again . here essentially you have one forward pass where you compute all the hidden states and all your errors and then you only have a single backwards in time you keep accumulating all the deltas of each time step . and so originally people said for this time step i'm gonna go all the way back and then i go to the next time step and then i go all the way back and then the next step and all the way back which is really inefficient . and is essentially same as combining all the deltas in one clean back propagation step . and again it's kind of is intuitive . an intuitive sort of implementation trick but people gave that the term back propagation through time . all right now that we have these simple recurrent neural networks we can use them for a lot of fun applications . in fact the name entity recognition that we're gonna use in example with the window . in the window model you could only condition the probability of this being a location a person or an organization based on the words in that window . the recurrent neural network you can in theory take and on a lot larger context sizes . 
and so you can do named entity recognition (ner) you can do entity level sentiment in context so for instance you can say . i liked the acting but the plot was a little thin . and you can say i want to now for acting say positive and predict the positive class for that word . predict the null class and all sentiment for all the other words and then plot should get negative class label . or you can classify opinionated expressions and this is what researchers at cornell where they essentially used rnns for opinion mining and essentially wanted to classify whether each word in a relatively smaller purpose here is either the direct subjective expression or the expressive subjective expression so either direct or expressive . so basically this is direct subjective expressions explicitly mention some private state or speech event whereas the eses just indicate the sentiment or emotion without explicitly stating or conveying them . so here's an example like the committee as usual has refused to make any statements . and so you want to classify as usual as an ese and basically give each of these words here a certain label . and this is something you'll actually observe a lot in sequence tagging paths . again all the same models you have the soft max at every time step . has a set of classes that indicate whether a certain expression begins or ends . and so here you would basically have this bio notation scheme where you have the beginning or the end or a null token . it's not any of the expressions that i care about . so here you would say for instance as usual is an overall ese expression so it begins here and it's in the middle right here . and then these are neither eses or dses . all right now they started with the standard recurrent neural network and i want you to at some point be able to glance over these equations and just say i've seen this before . it doesn't have to be w superscript hh and so on . but whenever you see the summation order of course doesn't matter either . but here they use w v and u but then they defined instead of writing out softmax they write g here . but once you look at these equations i hope that eventually you're just like it's just a recurrent neural network right . 
you have here are your hidden to hidden matrix . you have your input to hidden matrix and here you have your softmax waits you . so same idea but these are the actual equations from this real paper that you can now kind of read and immediately sort of have the intuition of what happens . all right you need directional recurrent neural network where we if we try to make the prediction here of whether this is an ese or whatever name entity recognition any kind of sequence labelling task what's the problem with this kind of model . what do you think as we go from left to right only . what do you think could be a problem for making the most accurate predictions . words that come after the current word can't be helping us to make accurate predictions at that time step right . cuz we only went from left to right . extensions of recurrent neural networks is actually to do bidirectional recurrent neural networks where instead of just going from left to right we also go from right to left . and it's essentially the exact same model . in fact you could implement it by changing your input and just reversing all the words of your input and then it's exactly the same thing . and now here's the reason why they don't have superscripts with whh cuz now they have these arrows that indicate whether you're going from left to right or from right to left . and now they basically have this concatenation here and in order to make a prediction at a certain time step t they essentially concatenate the hidden states from both the left direction and the right direction . and those are now the feature vectors . and this vector ht coming from the left has all the context ordinal again seven plus words depending on how well you train your rnn . from all the words on the left ht from the right has all the contacts from the words on the right and that is now your feature vector to make an accurate prediction at a certain time set . any questions around bidirectional recurrent neural networks . the recent papers you'll be learning in various modifications . they have and we have a special lecture also we will talk a little bit about convolutional neural networks . so you don't necessarily have a cycle right . 
you just go basically as you implement this you go once all the way for your the left and you don't have any interactions with the step that goes from the right . you can compute your feet forward hts here for that direction are only coming from the left . and the ht from the other direction you can compete in fact you could paralyze this if you want to be super efficient and . have one core implement the left direction and one core implement the right direction . so in that sense it doesn't make the vanishing create any problem worse . but of course just like any recurring neural network it does have the vanishing creating problem and the exploding creating problems and it has to be clever about flipping it and so yeah we call them standard feedforward neural networks or window based feedforward neural networks . and now we have recurrent neural networks . and this is really one of the most powerful family and in fact if there's no other question we can go even deeper . and so now you'll observe [laugh] we definitely had to skip that superscript . and we have different characters here for each of our matrices because instead of just going from left to right you can also have a deep neural network at each time step . and so now to compute the ith layer at a given time step you essentially again have only the things coming from the left that modify it but you just don't take in the vector from the left you also take the vector from below . so in the simplest definition that is just your x your input vector right . but as you go deeper you now also have the previous hidden layers input . instead of why are the so the question is why do we feed the hidden layer into another hidden layer instead of the y . in fact you can actually have so called short circuit connections too where each of these h's can and so here in this figure you see that only the top ones go into the y . but you can actually have short circuit connections where y here has as input not just ht from the top layer noted here as capital l but the concatenation of all the h's . and in fact there a lot of modifications in fact shayne has a paper an arxiv right now on a search based odyssey type thing where you have so many different kinds of knobs that you can tune for even more sophisticated recurrent neural networks of the type that we'll introduce next week that it gets a little unwieldy and it turns out a lot of the things don't matter that much but each can kind of give you a little so if you have three layers you have four layers what's the dimensionality of all the layers and the various different kinds of connections and short circuit connections . we'll introduce some of these but in general this like a pretty decent model and will eventually extract away from how we compute that hidden state and that will be a more complex kind of cell type that we'll introduce next tuesday . so now how do we evaluate this . it's very important to evaluate your problems correctly and we actually talked about this before . 
when you have a very imbalanced data set where some of the classes appear very frequently and others are not very frequent you don't wanna use accuracy . in fact in these kinds of sentences you often observe this is an extreme one where you have a lot of eses and dses but in many cases just content . standard sort of non-sentiment context and words and so a lot of these are actually o have no label . and so it's very important to use f1 and we basically had this question also after class but it's important for all of you to know because the f1 metric is really one of the most commonly used metrics . and it's essentially just the harmonic precision is just the true positives divided by true positives plus false positives and recall is just true positives divided by true positives plus false negatives . and then you have here the harmonic mean of these two numbers . so intuitively you can be very accurate by always saying something or have a very high recall for a certain class but if you always miss another class that would hurt you a lot . and now here's an evaluation that you should also be familiar with where basically this is something i would like to see in a lot of your project reports too as you analyze the various hyper parameters that you have . and so one thing they found here is they have two different data set sizes that they train on in many cases if you train with more data you basically do better but then also it's not always the case that more layers . so this is the depth that we had here the number l for all these different layers . it's not always the case that more layers are better . in fact here the highest performance they get is with three layers instead of four or five . recurring neural networks best deep learning model family that you'll learn about in this class . you can gain an intuition of why that might be the case . we'll in the next lecture extend them some much more powerful models the gated recurring units or lstms and those are the models you'll see all over the place in all the state all right . today we'll do a brief recap some organizational stuff and then we'll talk about i love when we call them fancy recurrent neural networks . but those are the most important deep learning models of the day lstms and gru type models . they're very exciting and really form the base model for pretty much every deep learning paper or almost all the deep learning papers you see out there . so after today you'll really have in your hands the kind of tool that is the default tool for a lot of different deep learning final p applications so super exciting . and the best part is you kind of know most of the important math already of it so we can just define the model . 
and everything else will kind of follow through with these basic and sometimes painful building blocks that we went through before . we have a new office hour schedule and places . today we're continuously trying to optimize the whole process based on your feedback . so i'll have office hours every day multiple times . i hope that will allow us to kind of distribute the load a little bit cuz i know sometimes lots of people come to one office hour and then there's a long wait there . also it's important that you register for your gpu teams by the end of today or ideally before . so that we can make sure you all get a gpu . ideally we also encourage people to have pairs for problem set 3 and 4 the project at least pairs cuz we only have 300 or so gpus and almost 700 students in the class . so try to form teams but do make sure that you don't just have your partner or work on and implement all the gpu stuff and you do all the other parts of the problem set . cuz then you really miss out on a very important valuable skill for both research and applied deep learning if you don't know how to use a gpu . and sadly i have to get back to some work event . so i'll have a pretty short office hour today . but then i know we have the deadline for project proposals on thursdays . so on thursday i'm gonna have an unlimited office hour . i'm gonna start after class and will end when queue status is empty . so if you come half an hour late prepare to talk to me three hours later . so [laugh] the project is the coolest part so i don't wanna discourage so it's gonna be great . you should bring food too if by any chance even after midnight people we still have the queue status is still full which i doubt at that point i think i hope then we can push the proposals out for those or you can submit the proposal . and then we'll figure out the mentor situation very soon thereafter . so all right any questions around any class organization . 
all right then lets dive right in . so basically today cutting edge blast from the past because while pedagogically it'll make sense for us to first talk about a model from 2014 from just three years ago . the main model we'll end up with the long-short-term-memories is actually a very old model from 97 and has kind of been dormant for a while . as very powerful model you need a lot of training data for it you need fast machines for it . but now that we have those two things this is a very powerful model for nlp . and if you ask one of the inventors the second model is really just a special case of the lstm . but i think pedagogically it makes sense to sort of first talk about the so-called gated recurrent unit which is slightly simpler version . is one of the sort of most useful tasks you might argue of nlp sort of a real life task . something that actual people outside academia outside research outside linguistics really care about . and by the end you'll actually have the skills to build one of the best machine translation models out there modulo a lot of time and some extra effort . but the biggest parts of 90% of you'll be able to understand at least and probably build also if you have the gpu skills after this class . all right so i'm not gonna go through too many of the details but in just in preparation to mentally make you also think about the midterm that's coming up . but ideally these kinds of equations that i'm throwing up here you're pretty familiar with . at this point you're like yeah i just do some negative sampling here from my word2vec . and i have my inside and my outside vectors in the window . and similarly for glove i have two sets of vectors you optimize this . you have a function here dead limits how important very frequent pairs are in your concurrence matrix . you have scores of good windows from the large training corpus and corrupted windows . so all of these should be familiar . and if not then you really should also start thinking about sort of studying again for the midterm . 
the most basic definition of neural net at the end or some soft max . and really being comfortable with these two final equations that if you understand those all the rest of the models will basically be in many cases sort of fancy versions or adapted versions of these two equations . and then we'll have our standard recurrent neural network that we already went through and we kind of assume you should know for the midterm as well . and our grade of cross entropy error as one of the main loss or objective functions that we optimize . and when we optimize we usually use the mini-batched sgd . we don't go through the entire batch of our trained data but we take 100 or so of examples each time we train . so all those concepts you should feel reasonably comfortable now . and if not then definitely come start sort of studying for the midterm . all right and we'll go over more midterm details in the next lecture . all right now onto the main topic of today machine translation . so you might think for some nlp tasks that you can get away with thinking of all the rules that for a sentence might come up positive or negative right . you say i have a list of all the positive words most of all the negative words . and i can think of the ways you can negate positive words and things like that . and you could maybe conceive of creating a sentiment analysis system of just all your intuitions about linguistics and sentiment . that kind of approach is completely ridiculous for machine translation . there's no way you would ever nobody will ever be able to think of all the different rules and exceptions for translating all possible sentences of one language to another . so basically the baseline that's pretty well established is that all machine translation systems are somewhat statistical in nature . we will always try to take a very large corpus . in fact we'll have so called parallel copra where we have a lot of the sentences or paragraphs in one language . and we know that this paragraph in this language translates to that paragraph in another language . 
one of the popular parallel copra of all of for a long time for the last couple thousand years is the bible for instance . and each paragraph is translated in different languages . that would be one of the first parallel corpora . the very first is actually the rosetta stone . which allowed people to have at least some understanding of ancient egyptian hieroglyphs . and it's pretty exciting if you're into historical linguistics . and it allows basically to translate those to the demotic script and the ancient greek also which we still know . and so we can gain some intuition about what's going on in the other two . now in the next couple of slides i will basically try to bring across to you that traditional statistical machine translation systems are very very complex beasts . and it wouldn't have been impossible for me to say at the end of the lecture all right now you could implement this whole thing yourself after just one lecture going over mt cuz there are a lot of different moving parts . you won't have to actually implement traditional statistical mt system in this class . but i want you to appreciate a little bit the history . and why deep learning is so impactful and amazing for machine translation . cuz it's replacing a lot of different submodules in these very complex models . and sometimes it uses still ideas from this but not very . most of them we don't need any more for neural machine translation systems . all right so let's set the stage . let's call that f such as french . and we have a target language e in our case let's say it's english . so we wanna translate from the source french to the target language of english . 
and we'll usually describe rule where we basically try to find the target sentence usually e here we assume is the whole sentence in the target language . that gives us the largest conditional so this is an abstract formulation . we'll try to fill in how to actually compute these probabilities in traditional and then later in neural machine translation systems . posterior equals its prior times likelihood divided by marginal evidence . marginal evidence here would just be for the source language . so we can drop that argmax would not change from that . so basically we'll try to compute these two factors here . sentence given or the source language given the target times the probability of just the target . and the other one is our language model . remember language modeling where we tried to get the probability of a longer sequence . this is a great use case for it . basically you can think of this as you get some french sentence . your translation model will try to find . maybe this phrase i can translate into that . and this phrase i can translate into this . and then you have a bunch of pieces of english . and then your language model will essentially in the decoder be combined to try to get a single smooth sentence in the target language . so it'll help us to take all these pieces that we have from the translation model . and make it into one sentence that actually sounds reasonable and flows and is grammatical and all that . so the language model helps us to weight grammatical sentences better . 
so for instance i go home will sound better than i go house right . because i go home will have a more more likely english sentence to be uttered . and how would you go about doing this . well if you wanted to translate do this translation model here . then the first thing you'd have to do is you'd find so called alignments . which is basically the goal of the alignment step is to know which word or phrase in the source language would translate to the other word or phrase in the target language . and now again we have these three different systems . and now we're zooming in to the step one of that system . now that one is already hard because alignment is non-trivial . these are actually some cool examples from previous incarnation from chris's class alignment is already hard . and this is for a language pair that is actually quite similar . english and french share a lot and they're more similar . but even if we have these two sentences here like japan shaken by two new quakes . or le japon secoue par deux nouveaux seismes . then we'll basically have here a spurious word . so le was actually not translated to anything . and we would skip it in our alignment . so you see here this alignment matrix . and you'll notice that le just wasn't translated . we don't say the japan or a japan or something like that . 
cuz there are also so called zero fertility words that are not translated at all . so we start in a source and we just drop them . and for some reason the translators or for grammatical reasons and so on they don't actually have any equivalent in the target language . and to make it even more complex we can also have one-to-many alignments . so implemented in english is actually so made into an application the verb implemented here . so then we'll have to try to find . and now as you try to think through alignment for you . you'll have to think so this word could go to either this one word or no word . and you can see how that would create if you tried to go through all the statistics and collect all of these probabilities of which phrase would go to what phrase . it'll get pretty hard to actually combine them all . and language is just incredible and very complex . so you'd have two words in german . and so you have many-to-one alignments making the combinatorial explosion even harder if you try to find good alignments . and lastly you'll also have many-to-many alignments . you have certain phrases like don't have any money . this just goes to sont demunis in french . and so it's a very very complex problem that has combinatorial explosion of all potential all right so now really if you were to take a traditional class you could have several lectures or at least an entire lecture just on the various ways you could implement cleverly an alignment model . and sometimes and other times they actually use parses like the one you're now familiar syntactic parses . and try to find which no not just words but phrases from a parse would map to the other language . and then of course it's not just that . 
and not usually are sentences and you can also have complete reorderings . so german sometimes for sub clauses actually has the verb at the end so you flip a lot of the words and you can't just have this vocality assumption that words rough in this area will translate to roughly a similar area in terms of the sequence of words in the other language . so yeah ja nicht here ja is technically just yes in german also not translated at all . and then actually going over there and going moving also . all right now let's say we have all these potential alignments and now as we start from the source language we say all right . let's say the source here is this german sentence geht ja nicht nach hause . now could be translated into many different words . so german it's technically just the he of he she it as the es in german . but sometimes english as you do your alignment when not unreasonable one is just it or comma he or he will be cuz those were dropped before in the alignment and so on . so you now have lots of candidates for each possible word and for might want to combine now in some principled way to so you have again here a combinatorial explosion of lots of potential ways you could translate each of the words or phrases of various lengths . and so basically what that means is you'll have a very hard search problem that also includes having to have a good language model . so that as you put all these pieces together you essentially try to keep saying or combining phrases that are grammatically plausible or sound reasonable to native speakers . and this often ends up being so-called beam search where you try to keep around a couple of candidates as you go from left to right and you try to put all of these different pieces together . now again this is totally not right we just went in five minutes over what could have been an entire lecture on statistical machine translation or maybe even many multiple lectures . so there are lots of important details we skipped over . but the main gist here is that there's a lot of human feature engineering that's required and involved in all of these different pieces that used to require building and it also meant that there were whole companies that you could form just for machine translation because nobody could go through all that work and really build out a good system . whereas now you have companies that have worked for decades in this and they start using an open-source machine translation system that anybody can download . and now a normal student a phd student can spend a couple months and then he has like one of the best mt systems . which just completely would have been completely impossible in their large groups that all work together in very large systems before in academia . so one of the main problems of this kind of approach is actually that not only is it's also a system of independently trained machine learning models . 
and if there's one thing that i think that i like most when property of deep learning models not just for mt but in all of nlp and maybe in all of ai . is that we're usually in deep learning try to have end to end trainable models where you have your final objective function that you care about and everything is learned jointly in one model . and this mt system is kind of the opposite of that . you have an alignment model you optimize for that and then you have a reordering model maybe and then you have the language model . and they're all separate systems and you couldn't jointly train all of it together . so that's kind of the very quick summary for traditional machine transaction . any high level questions around traditional mt . all right so now deep learning to the rescue maybe probably . so let's go through a sequence of models and see if they would suffice . so the simplest one that we could possibly do is kind of an encoder and decoder model that looks like this . where we literally just have a single recurrent neural network where we have our word vectors so let's say here we translate from german to english echt kiste is awesome sauce in english . and we now have our word vectors here we learned them in german and we have our soft max classifier here . and we just have a single recurrent neural network and once it sees the end of german sentence and there's no input left we'll just try to output the translation . not totally unreasonable it's an end-to-end trainable model . we'll have our standard cross entry pair here that tries to just predict the next word . but the next word actually has to be in a different language . now basically this last vector here if this was our main model this last vector would have to capture the entirety of the phrase . and sadly i've already told you that usually five or six words or so can be captured and after that we don't really we can't memorize the entire context of the sentence before . so this might work for like very short sentenced but maybe not . but let's define what this model would be in its most basic form cuz we'll work on top of this afterwards . 
so we have here our standard recurrent neural network from the last lecture . where we have our next hidden state it's just basically a linear network here followed by and we sum here the matrix vector product with the vector the previous hidden state in our current word vector xt . and that's our encoder and then in our decoder in the simplest form again not the final model in the simplest form we could just drop this cuz the decoder doesn't have an input at that time . this matrix vector product and we just go each time step . it's just basically moving along based on the previous hidden time step . and we'll have our final softmax output here at each time step of the decoder . now i also introduced this phi notation here and basically whenever you have we'll see this only in the next couple of slides . but whenever i write phi of two vectors that means we'll have two separate w matrices for each of these vectors . this is the little shorter notation and then the default here would be well just like i said minimize the cross entropy error for all the target words conditioned on all the source words that we hoped would be captured in that hidden state . all right any questions concerns thoughts about how this model would do . so the comment or question is that neither are the traditional model or this model account for grammar . and in some ways that's not true . so there are actually a lot of traditional models that work on top of syntactic grammatical tree structures . and they do this alignment based prefer potentially the alignment step . but also for the generation and the encoding step and all these different steps . so there are several ways you can infuse grammar and chromatical sort of priors into neuro machine translation systems or so syntactic machine translation systems . it turns out it's questionable if that actually helps . in many cases for machine translation you have such a broad range of sentences . you actually might have un-grammatical sentences sometimes and you still want them to be translated . you have very short complex ambiguous kinds of sentences like headlines and so on . 
so it's tricky the jury was sort of out . and some tactic models were battling it out with non-tactic models until neural machine translation came . and now it's not as important of a question anymore . now for neural systems we would assume and hope that our hidden state actually captures some grammatical structures and some grammatical intuitions that we have . but we don't explicitly give that to the algorithm anymore . which some people who are very good at giving those kinds of features your algorithms might think is sad . but at the same time it's good if we don't have to right . it's less work for us putting more artificial back into artificial intelligence less human intelligence on designing grammars . good question so sometimes the number of input words is different to the numbers of output words and that's very true . so one modification we would have to make to this kind of model for sure is actually say have the last output word here ba stop out putting up words work . like a special token that says i'm done . and one you add that to your softmax classifier sort of the last row . and then you hope that when it predicts that token it just stops . and that is good enough and not uncommon actually for all these neural machine translations . the superscript s is just again to distinguish the different w matrices that we have for hidden connections visible or all right now sadly while neural mt is pretty cool and it is simpler than traditional systems it's not quite that simple . so we'll have to be a little more clever . and so let's go through a series of extensions to this model where in the end we'll have a very big powerful lstm type model . so step one is we'll actually have different recurrent neural network weights for encoding and decoding . so instead of having the same w here we actually should have a different set of parameters a different w for the decoding step . all right so again remember this notation here of fi where every input has its own matrix w associated with it . 
the second modification is that the previous hidden state is kind of the standard that you have as input for during decoding . but instead of just having the previous hidden state we'll actually also add the last hidden vector of the encoding . so we call this c here but it's essentially ht . so at this input here we don't just have the previous hidden state but we always take the last hidden and we have again a separate matrix for that . and then on top of that we will also add and that's actually if you think about it it's a lot of parameters we'll add the previous predicted output word . so as we translate we have three inputs for each hidden state during the decoding step . we'll have the previous hidden state as a standard recurrent neural network . we have the last hidden state of the encoder . and we have the actual output word we predicted just before that . and this will essentially help the model to know that it just output a word and it'll prevent it from outputting that word again . cuz it'll learn to transform the hidden state based on having just upload a specific word before . so whenever you have fi of xyz here it'll just f of w times x + u of y + v of z . so you just i don't wanna define all the matrices . so why do we need to make y t minus one a parameter if we actually had computed yt minus one from ht minus one right . so two answers one it will allow us to have the softmax weights also modify a little bit how that hidden state behaves at test time . and two we actually can choose usually yt and there are different ways you can do this . you could take the actual probability the multinomial distribution from the softmax . but here we'll actually make a hard choice and we chose exactly this one . so instead of having the distribution we'll make a hard choice . and we say this is the one word the highest probability that had the highest probability we predicted that one and that's the one we give us input . 
so it turns out in practice that helps to prevent the model from repeating words many times . and again it incorporates the softmax weights in that computation indirectly . that is not how we define the model . in theory again so i didn't define it but you can also you can do the same thing with the softmax and this is what the picture actually shows . so instead of having a softmax of just w you can also concatenate here your c and that's what the picture said . but i wanted to skip over the details so you caught it well done . so this model usually so the question is do we have kind of a look ahead type thing . and the model basically has to output the words in the right order . and it doesn't not have the ability to do this whole reordering step or look ahead kind of thing . or there's no sort of post processing of reordering at the end so this model isn't able to output the verb at the right time stamp . now of course once it works well everybody will try to see if they can kind of improve it and eventually you can do beam searches too for these kinds of models . but surprisingly in many cases you don't have to get a reasonable mt system . become more and more familiar to be able to read the literature . so the same picture that we had here and the same equations we defined here's another way off looking at this . so with the exception that this one doesn't have the c connection that you caught . it's the same exact model just a different way to look at it and it's kind of good to see . sometimes people explicitly write that you start out with a discreet one of k and coding of the words . it's just like you want one-hot vectors that we defined and then you embed it into you give those as input you compute your recurrent neural network ht steps . and now you give those as input to the decoder . and that each time stamp of decoder you get the one word sample that you actually took as input the previous hidden state and to see vector we defined before . 
so all these three already are the inputs for each node in this recurrent neural network . so just a different picture for the same model we just defined so you learn picture in variances first model semantics . it needs to get more powerful cuz even with those two assumptions here we have a very simple recurrent neural network with just one layer that's not going to cut it . so we'll use some of the extensions we discussed in the last lecture we'll actually have stacked deep recurrent neural networks where we have multiple layers . and then we'll also have in some cases this is not as common but sometimes it's used we have a bidirectional encoder . where you go from left to right and then we give both of last hidden states of both directions as input to every step of the decoder . and then this is kind of almost an xor here . if you don't do this than another way to improve your system slightly is by training the input sequence in reverse order because then you have a simpler optimization problem . so especially for languages that align reasonably well like english and french . you might instead of saying a b c the other word's a the word b or c goes to in the different language the words x and y . you'll say c b a goes to x y because as they align a is more likely to translate to x and b is more like to y . and as you have longer sequences you basically bring the words that are actually being translated closer together . and hence you have less of a vanishing gradient problems and so on because where you want the work to be predicted it's closer to where it came in to the encoder . that's right but yeah it's still an average force . so how does reversing not mess it up . cuz this sentence doesn't make grammatical sense . so we never gave this model an explicit grammar for the source language or the target language right . it's essentially trying in some really deep clever continuous function general function approximation kind of way just correlation basically right . and it doesn't have to know the grammar but as long as you're consistent and you just reverse every sequence the same way . and the model reads it from potentially both sides and so on . 
so it doesn't really matter to these learning models as long as your transformation of the input is consistent across training and testing times and so on . so the question is he understands the argument but it could still change the meaning . and it doesn't change the meaning if you assume the model will always go from one direction to the other . if you start to sometimes do it and sometimes not then it will totally mess up the system . but as long as it's a consistent transformation it is still the same order and so you're good . imagine you had a very long sequence here . and again this is only the case if the languages align well . as in usually the first capital words in one of the source language translated to first capital words in the target language . now if you have a long sequence and you try to translate it to another long sequence and say there are a lot of them here . now what that would mean is that this word here is very far away from that word cuz it has to go through this entire transformation . and likewise these words are also very far away . so everything is far away from everything in terms of the number of non-linear function applications before you get to the actual output . now if you just reverse this one then this word so let's call this a b c d e f . now this is now f e d c b a . so now these two are very very close to one another . and so as you do back propagation and we learn about the vanishing creating problem in the last lecture you have much less of a vanishing creating problem . it'll be much better at translating those . so how does this check work for languages with different morphology . it doesn't actually matter but the sad truth is also that very few mt researchers work on languages with super complex morphology . so like finnish doesn't have very large parallel corpora of tons of other languages . 
and so you don't sadly see as many people work on that . and for german actually a lot of other tricks that we'll get to . and really these tricks are not as important as the one as trick number six . but before that we'll have a research highlight . so i'm gonna talk about building towards a better language modeling . so as we've learned last week language modeling is one of the most canonical task in nlp . and there are three different ways we can make it a little bit better . we can have better regularization or preprocessing . and eventually we can have a better model . have all played with glove and that's a word level representation . from you guys who are down there . so in fact you can code the word at a subword level . you can eventually do character level embedding . what it does is that it drastically reduce the size of your vocabulary make the model prediction much easier . so as you can see tomas mikolov in 2012 and yoon kim in 2015 explored this route and got better results compared to just plain word-based models . so another way to improve your model is that one of the bigger problems for language modelling is over-fitting . and we know that we need to apply regularization techniques when the model is over-fitting . so there are a bunch of them but today i'm gonna focus on preprocessing because it's a little bit newer . what preprocessing does is that we know that we're never gonna have unlimited training data . so in order to have our corpus look more like the true distribution of the english language what we can do is quite similar to computer vision we can do this type of data augmentation technique where we try to replace some words in our corpus with some other words . 
so for example your model during the first pass you can see a word called new york the next pass you can see new zealand the next pass you can see new england . so by doing that you're basically generating this data by yourself and eventually you achieve a smoothed out distribution . the reason this happens is that more frequent word by replacing by dropping them . they appear less often and rarer words by making them appear . so a smooth distribution allow us to learn a better language model and the result is on the i think is on the right hand side of you guys . and the left hand side is what happen when we apply better regularization techniques . so at last we can wait that's it okay awesome thank you guys . in these tables is that the default for all these models is an lstm and that's exactly what we'll end up very soon with . which is basically a better type of recurrent unit . and so we'll start with gated recurrent units that were introduced by cho just three years ago . and the main idea is that we wanna basically keep around memories that capture long distance dependencies and you wanna have the model learn when and how to do that . and with that you also allow your error messages to flow differently at different strengths depending on the input . what is a gru as our step to the lsdm . and sometimes you don't need to go all the way to the lsdm . the gru is a really good model by itself . in many cases already in its simpler . so let's start with our standard recurrent neural network which basically computes our hidden layer at the next time step directly . so we just have again previous hidden state recurring to our vector that's it . gated recurring units or grus is we'll compute to gates first . these gates are also just like ht continuous vectors of the same length as the hidden state and they are computed exactly the same way . 
and here it's important to note that the superscripts that's just basically are lined with the kind of gate that you're computing . so we'll compute a so called update gate and a reset gate . now the inside here is the exact same thing but is important to note that we so we'll have elements of this vector are exactly between zero and one . and we could interpret them as probabilities if we want to . and it's also important to note that the super scripts here are different . uses a different set of weights to the reset gate . now why are they called update and reset gates and how do we use them . we just introduced one new function here just the element wise product . we also call it the hadamard product sometimes . where we just element wise multiply this vector here from the reset gate with this which would be our new memory content . we call it ht this is our intermediate memory content we also know as a [inaudible] . this part here is exactly the same we just have to input our word vector and then transformed with a w . so intuitively right this is just a long vector of numbers between zero and one . now intuitively if this reset gate at a certain unit is around zero then we essentially ignore all the past . we ignore that entire computation of the past and we're just going to define that element where our zero now why would we want to do that . let's take the task of sentiment analysis cuz it's very simple and intuitive . if you were to say you're talking about a plot of a movie review . and you talk about the plot and you know some girl falls in love for some guy who falls in love with her but then they can't meet blah blah blah . that's a long plot and in the end you say but the movie was really boring . then really doesn't matter that you keep around that whole plot . 
you wanna say boring as a really negative strong word for sentiments and you wanna basically be able to allow the model to ignore the previous plot summary . cuz for the task of sentiments analysis it's irrelevant . now this is essentially what the reset gate will let you do but of course not in this global fashion where you update the entire hidden state but in a more subtle way where you learn which of the units you actually will reset and which ones you will keep around . so this will allow some of the units to say well maybe i want to be a plot unit and i will keep around the plot . but other units learn well if i see one of the sentiment words i will definitely set that reset gate to zero and i will now make sure that i don't wash out the content with previous stuff by summing these two right . you're sort of like not quite averaging but you're summing the two . so you wash out the content from this word and instead it will set that to zero and take only the content from that current word . now the final memory it will compute we'll combine this with the update gate . and the update gate now basically allows us to keep around only the past and not the future . so intuitively here when you look at z then what we would do is essentially do ht = ht-1 + 1-1 is 0 so this term just falls away . basically if zt was all ones we could just copy over our previous time step . super powerful if you copied over the previous time step you have no vanishing gradient problem right . your vector just gets a bunch of ones . so that's very powerful and intuitively you can use that same sentiment example . but you say in the beginning man i love this movie so much here's this beautiful love story . and now you go through the love story and really what's important for sentiment is not about the love story but it's about the person saying i love this movie a lot . and you wanna make sure you don't lose that information . and with the standard recurring neural network we update our hidden state every time every word . no matter how unimportant a word is we're gonna sum up those two vectors move further and further along . here we can decide and what's even more amazing you don't have to decide . 
you can say this word is positive so i'm gonna set my reset gate manually . no the model will learn when to reset and when to update . so this is a very simple kind of modification but extremely powerful . now we're gonna go through it and explain it a couple more times . have an attempt here at a clean illustration . honestly personally i feel the equations here are still straight forward and very intuitive that i don't know if these illustrations always help but some people like them more than others . so intuitively here you basically see that only the final memory that you computed is the one that's actually used as input to the next step . so all of these are only modifying through the final state . and now this one gets as input to our reset gate or update gate the intermediate state and the final state of the memory . and so does our x vector the word vector here also gets its input through the reset gate the update gate and our intermediate memory state . and then i tried to use this so the dotted line here as basically gates that modify how these two interact . all right so i've said i think most of these things already but again reset gate here is close to 0 . and that again allows the model in general to drop information that is irrelevant for the future predictions that it wants to make . and if we update the gate z controls how much of the past state should matter at the current time stamp . and again this is a huge improvement for the vanishing gradient problem which allows us to actually train these models on nontrivial long sequences . does it matter if you reset first or update first . well so you can't compute h until you have h tilled . so the order of these two doesn't matter . you can compute that in peril but you first have to compute h tilled with the reset gate before you can compute that one . so the question is does it matter to switch and use an equation like this first and then an equation like that . 
i guess it's just a different model . it's not one that i know of people having tried . it's not super unreasonable i don't see a sort of reason why it would be illogical to ever to that but yeah just not the gru model . you will actually see in [inaudible] she has a paper on a search space odyssey type paper where there are a thousand modifications you can make to the next model the lstm . and people have tried a lot of them and it's not trivial . and a lot of times they seem kind of intuitive but don't actually change performance that much across a bunch of different tasks . but sometimes one modification improves things a tiny bit on one of the tasks . it turns out the final model of gru here and the lstm are actually incredibly stable they give good performance across a lot of different tasks . but it can't ever hurt to if you have some intuition of why you want to have make something different it can't hurt to try . so the question is is it important of how they're computed . i think there are some people who have tried once to have a two layer neural network to compute . these a z and update z and r . in general it matters of course a lot of how they're computed but not in the sense that you have to modify them manually or something . it just the model learns when to update and when not to update . so what do i mean when i say unit . so in general what you'll observe in a slide that's coming up very soon is that we will kind of abstract away from the details of what these equations are . and we're going to write that just ht equals gru of xt and ht minus 1 . and then we'll just say that gru abbreviation means all these other things all these equations and we're going to abstract away from that . and that's something that you'll see even more in subsequent lectures where you just say a whole recurrent network with a five layer gru and combine lots of different ways is just one block . we often see this in computer vision too where cnns are now just like the cnn block and you assume you've got a feature vector out at the end . 
and people will start abstracting but yeah you'll always have to remember that yes there's a lot of complexity inside that unit . here's another attempt at an illustration which i'm even less of a fan of then the one i tried to come up with . basically how you have your z gate that kind of can jump back and forth . but of course it's usually a continuous type thing . it's not a zero one type thing so i'm not a big fan of this kind of illustration . and so in terms of derivatives we couldn't theory asks you to derive all the details of the gru . and the only change here is that we now have the derivative of these element wise multiplications both of which i have parameters or inside . and we all should know what derivative of this is and the rest is again the same kind of chain rule . but again now you're sort of realizing why we wanna modularized this more and more and abstract a way from actually manually taking these instead having error messages and deltas sent around . explain why we have both update and reset . so basically it helps the model to have different mechanisms for when to memorize something and keep it around versus when to update it . you're right in theory you could try to put both of those into one thing right . in theory you'd say well if this was just my previous ht here then this could say well i wanna keep it around or i wanna update it here . but now this update here if you just had an equation like this it would be still be a sum of two things . so that means that xt here does not have complete control over modifying the current it would still be summed up with something else and that happens at every single time stamp . so its only once you have this reset gates are here . these reset gates are here that you would allow h to be completely dominated by the current word vector if the model so chooses . if the reset gates are all okay so if these are all ones then you have here basically a standard recurrent neural network type equation . and then if you just have zs all 0s then you take that exact equation and you're right . then you just have a standard rnn . 
it's also beautiful it's always nice to say my model is a more general form of your model or- you're model's a special case of my model . it was actually a couple years ago that you could by and say that . and likewise the inventor of this model made exactly that statement about the gru . not knowing why anybody had to publish a new paper about this instead of just referring to this and the special cases of the lstm . so if we have one more why tanh and sigmoid . so in theory you could say the tan h here could be a rectified linear unit or other kind of unit . in practice you do want sigmoids here because you have this plus 1 minus that . and so if they're all over the place then everything will kind of be modified and it's less intuitive that you kind of have a hard reset in sort of a hard sort of yeah hard reset or a hard update . and if this wasn't 10h and was rectified linear unit then these two might be all over the place too and it might be kind of easy to potentially have the sum also the not very synthecal . but at the same time it's not unreasonable to try having a rectified learning unit here . and maybe if you combine it with proper regularization and so on you could get away with other kinds of other kinds of linearities . that's unlike probabilistic graphical models for certain things just make no sense . and you can't do them deep learning you can often try some things and sometimes even nonsensical things surprisingly work . and then other people try to analyse why that was the case in the first place . but yeah there's no mathematical reasons why you couldn't at all have a rectified linear unit here . all right now on to a even more complex sort of overall recurrent unit . so now this is the hippest model of the day and it's pretty important to know it well . fortunately it's again very similar to the kinds of basic building blocks . but now we allow each of the different steps to have again we separate them out even more . so how do we separate them out . 
basically this is what's going on at each time step . we will have an input gate forget gate output gate memory cell final memory and a final hidden state . now let's gain a little bit of intuition and there is good intuition of why we want any of them . so the input gate will basically determine how much we will care about the current vector at all . so how much does the current cell or the current input word vector matter . the forget gate is a separate mechanism that just says maybe i should forget maybe i don't . in this case here just kind of counterintuitive sometimes and they're actually different models in the literatures . some have the one minus there and others don't . but in general here if it's 0 then we're forgetting the past . then we have an output gate basically when you have this output gate you will separate out what matters to a certain prediction versus what matters to being kept around over the current recurrent time steps . so you might say at this current time step this particular cell is not important but it will become important later . and so i'm not going to output it to my final softmax for instance but i'm still gonna keep it around . so it's yet another separate mechanism to learn when to do that . and then we have our new memory cell here which is similar to what we had before . so in fact all these four here have the same equation inside and just three sigmoid non linearity and one tan h non linearity . so these are all just four single layer neural nets . now we'll put all of these gates together when we compute the memory cell and the final hidden state . so the final memory cell now basically separated out the input and the forget gate . instead of just c and 1 minus c we have two separate mechanisms that can be trained and learn slightly different things . and actually become also in some ways counter intuitive like you say i don't wanna forget but you do wanna forget but you also input something right now . 
but the model turns out to work very well . so basically here we have final hidden state is just to forget gate how to mark product with the previous hidden states final memory cell ct-1 . how much do you wanna keep this around or how much do we wanna forget from the past . and then the new memory cell here this has a standard recurrent neural net . if i is all 1s then we really keep the input around . and if the input gate says no this one doesn't matter then you just basically ignore the current word back there . so in that sense this equation is quite intuitive right . forget the past or not take the input or not that's basically it yeah . so the secret question once you forget the past does it mean you forget grammar or something else . and the truth is we can think of these forget gates as sort of absolutes . they're all vectors and they will all forget only certain elements of a long hidden unit . and so really i can eventually show you what these hidden states look like . and sometimes they're actually more intuitive than others . but it's rare that you would find this particular unit when it was turned off or on actually had like this perfect interpretation that we as humans find intuitive and think of as grammar . and so it's hard to say any single unit would capture any particular like entirety of a grammar it might only capture certain things . so it's not implausible to think suggest that the next noun should be a plural noun or something like that . but that's the most we could hope for in many cases . all right and then here we can keep these cs around right . and cs will compute our computer from other cs . but we might not want to expose the content of this memory cell hidden state ht minus 1 . 
all right now yeah it's a really powerful model are there any questions around the equations . we're gonna attempt at some illustrations again i think the equations does the lstm and gru completely liviate or just help with an engine came problem . and the truth is they helped with it a lot but they don't completely obviate it . you do multiply here a bunch of numbers that are often smaller than 1 . and over time even if it would have to be a perfect one but that would mean that that unit is really really strongly active . and then it's hard to sort of dies it's like the gradient when you have unit that's really really active and looks something like this . to that unit and it's here then grade in around here it's pretty much 0 . and then the model can't do anything with it anymore . you'll observe some units just sort of die after training after awhile . and you'll just sort of keep around stuff or delete stuff at each time step . but in general most of the units are somewhat small than 1 and so you still have a bit of a vanishing creating problem but much less so . and intuitively you can come up with final p for a lot of good ways to think about this right . maybe you want to predict different things at different time steps . but you wanna keep around knowledge through the memory cells but not expose it at a given prediction . what is the point of the exposure gate when it already had the forget gate . so basically you want to sort of forget gate will tell you whether you keep something around or not . but exposure gate will mean does it matter to this current time step or not . so you might not wanna forget something . but you also might not wanna show it to the current output because it's irrelevant for that output . and it would just confuse the softmax classifier at that output . 
does the exposure gate help you or do you mean the output gate here right . so does the output gate does it help you to what exactly . to not have to forget everything forever . you can basically while it doesn't wanna give as output something for a long time . and hence it's basically it will only be forgotten at that time set but actually be kept around in . i don't wanna use like anthropomorphize the models but like the subconsciousness of this model or whatever right . keeps it around but doesn't expose it . the initialization to all these models matters it matters quite significantly . so if you initialize all your weights for instance such that whatever you do in the beginning all of the weights are super large . then your gradients are zero and you're stuck in the optimization . so you always have to initialize them properly . in most cases as long as they're relatively small you can't go too wrong . eventually it might slow down your eventual convergence but as long as all your parameters w here and your word vectors and so on are initialized to very small numbers . it will usually eventually do it pretty well . yes you could use lots of different strategies for initialization . i like this one from chris olah on his blog from not too long ago . i feel like the equations speak mostly for themselves . i have four different neural network layers and then you combine them in various ways with pointwise operations such as multiplication or addition . and sometimes you know multiplication and then addition and concatenation and copies and so on . but in the end you often observe this kind of thing where we'll just write lstm in this block . 
and has an x and an h and we don't really look into too many details of what's going on there . and here's some i think even less helpful [laugh] illustrations that yeah i think are mostly output gates input gates and so on . but and your memory cells as they try to modify each other . you know you have some inputs your gates you have your forget gates on top of your memory cell and so on . but in general i think the equations are actually quite intuitive right . if you think of your extremes if this is zero one then this input matters more to the output . all right now as i said lstms currently super hip . the en vogue model are for pretty much all sequence labeling tasks and sequence to sequence tasks like machine translation . super powerful in many cases you will actually observe that we'll stack them . so just like the other rnn architectures we'll have a whole lstm block and we put another lstm block with different sets of parameters on top of it . and then the parameters are shared over time but are different as you have a very deep model . and of course with all these parameters here we have essentially many more parameters then the standard recurrent neural network . where we only have two such parameters and we update every time . you wanna have more data especially if you stack you now have 10x the parameters of standard rnn we wanna train this on a lot of data . and in terms of amount of training data available machine translation is actually one of the best tasks for that . and is also the one where these and so in 2015 i think the first time i gave the deep learning for nlp lecture the jury was still a little bit out . the neural network models came up fairly quickly . but some different more traditional machine translation systems were still slightly better like by half a bleu point . you can essentially think of it as an engram overlap . the more your translation overlaps in terms of unigrams and bigrams and trigrams the better it likely is period . 
so you have this reference translation sometimes multiple reference translations . you have your translation you look at engram overlap between the two . and basically the neural network models were often also just use it for rescoring traditional mt model . now just one year later last year really a couple months ago the story was completely different . and you have different universities and different companies and so on submit their systems . and the top three systems were all neural machine translation systems . the jury is now basically not out anymore . it's clear neural machine translation is the most accurate machine translation model in the world . yeah that number two was us yeah . james bradbury was actually a linguistics undergrad while he was doing that but now he's full-time . so yeah basically we haven't talked that much about ensembling and ensembles of different models . but you can also train five of these monsters and then average all the probabilities and you'll usually get a little better . we just as general thing you'll observe for every competition machine learning competition out there . if you go on kaggle other machine learning competitions usually train even the same kind of model five times . you end up in slightly different local optimum average what's cool also though is that while we might not be able to exactly recover grammar or have specific units be explicitly sort of capturing very intuitive things . as we project this down similar to the word vectors we actually do observe some pretty interesting regularities . so this is a paper from sutskever in 2014 they projected different sentences . they were trained basically with a machine translation task and basically observe quite interesting regularities . to john is in love with mary and to john respects mary . now of course we have to be a little carefull here to not over interpret the amazingness . 
it's amazing but we also have a selection vice here right . maybe if we just had john did admire mary or something it might also be close to it right . and it might be closer too but if you just project these six particular sentences into lower dimensional space . then you do see very nicely that whenever john has some positive feelings for mary all those sentences are in here . and all the ones that are on this area of the first two item vectors mary admires john mary admires john mary is in love with john and mary respects john . they're all closer together which is kind of amazing cuz well it's a sequence model so how could it ever capture that the word order changes . so here we have she was given a card by me in the garden versus in the garden i gave her a card . and i gave her a card in the garden and despite the word order being actually flipped right . in the garden is in the beginning here and in the end here . these are still closer together than the different ones where in the garden basically she gave me a card verses i gave her a card . so that shows that the semantics here turn out to be more important than the word order . despite the model just going from left to right or this one was still the trick where we reversed the order of the input sentence . but it choses that its incredibly invariant and variance is a pretty important concept right . we want this model to be changes when the semantics are actually kept the same . it's pretty incredible that it does that . so this is also the power i think of some of these . this is a very deep lstm model where you have five different lstm stacked in the encoder and several in the decoder . and they're all connected in multiple places too . all right any questions around those visualizations and lstms . all right you now have knowledge under you belt that is super powerful and very interesting . 
i expected to maybe have five minutes more of time . so i'm going to talk to you about a recent improvement two recurrent neural networks that i think is also very applicable to machine translation . but nobody has actually yet applied it to machine translation . with all softmax classification that we do in all the models i've so far described to you . and really up until two or three months ago that everybody in nlp had as a major problem . and that is you can only ever predict answers if you saw that exact word at training time . and you have your cross entropy error saying i wanna predict this word . and if you've never predicted that word no matter how obvious it is for the translation system it will not be able to do it right . so we have some kind of translation and let's us say we have a new word like a new name or something that we've never seen at training time . and it is very obvious that this word here should go at this location . this is like mrs. and then maybe the new word is like yelling or something like that it could be any other word . and now let's say at training time we've never seen the word yelling . but now it's like vowel german misses miss in yeah german translation for this . and now it's very obvious to everybody that after this word it should be the next one the name of the of the miss . and so these models would never be able to do that right . and so one way to fix that is to think about character meant translation models where the model's actually surprisingly similar to what we described here . well many times it have to go but instead of having words we just have characters . so that's one way but now we have very long sequences . and at every character you have a lot of matrix multiplications . and these matrix multiplications that we have in here are not 50 dimensional for really powerful mt models they're a 1000 dimensional . 
and now you have several thousand by a thousand matrices here multiplying with thousand dimensional vectors . and you stack them so doing that for every single character actually gets really really expensive . so at the same time it's very intuitive that after we see a new word at test time we wanna be able to predict it . and also in general when we have the softmax even for words that we do see once or twice it's hard for the model to then still predict them . it's this skewed data set distribution problem . but you have very rare very infrequent classes our words are hard to predict for the models . so this is one attempt at fixing that which is essentially a mixture model of using standard softmax and what we call a pointer . it's essentially a mechanism to say well maybe my next word is one of the previous words in the context . you say 100 words in the past and every time step you say maybe i just wanna copy a word and if not then i will use my standard softmax for the rest . so this is kind of this sentinel idea here . this is a paper by stephen merity and some other folks . and basically we now have a mixture model where we combine the probabilities from the standard vocabulary and from this pointer . and now how do we compute this pointer . it's very straightforward we basically have a query . this query is just a modification of the last hidden layer that we have here . and we pipe that through a standard single layer neural network to compute another hidden layer which we'll call q . and then we'll do an inter product between this q and all the previous hidden states of the last 100 timed steps . and that will give us basically the single number for each of these interproducts . and then we'll apply a softmax on top of that . and this gives us essentially a probability for how likely do we wanna point to each of these words . 
or the very last one is we don't point to anything we just take the standard softmax . so we keep one unit around where we do this . and now of course in the context the same word might appear multiple times . and so you just sum up all the probabilities for specific words . if they appear multiple times you just sum them up . with this simple modification we now have the ability to predict unseen words . we can predict based on the pattern of how rare words appear much more similar things . for instance fed chair janet yellen raised rates and so on ms . is very obvious that this is the same ms . and you can base or you combine this in this mixture model . and now over many many years for language modeling . the perplexity that we defined before was sort of stock actually around 80 . and then in 2015 we have a bunch of modifications to lstms that were very powerful . and lower this and now were down to the lowest 70s . and was some modifications will cover another class were actually down on the 60s now . so it really had to told for several years and now perplexity numbers are really dropping in . and this models are getting better and bettered capturing more and more the semantics and the syntax of language . this is a pretty advanced lecture i hope you gained some of the intuition . again most of the math falls out from the same basic building blocks we had before . and next week or no next thursday we'll do midterm review . 
stanford university welcome to the midterm of this session for cs224n . i'm just gonna start with some announcements . so the first one as i hope you're aware of homework 2 is due today . and we had an issue with the submissions script that made submissions unavailable for a couple hours . so we apologize for that but that is now fixed . there's a slight change to the submission instructions to the initial version . and that is we don't want you to include the trained weights for your models because it turns out and we run out of afs and we can't take any more submissions . the other thing due today is the project proposal so these are both due at midnight . and we are saying that if you have not yet found a mentor but you would still like to do a final project other than the default final project you can still submit a project proposal . and say that you've not found a mentor yet and we would do our best to find you a mentor anyway . now for a couple of notes on the midterm . same time as the regular lecture but i want to point out that it? not being held here . there is an alternate exam only if you cannot at all make the february 14th is normal midterm time . and if you want to take the alternative exam or you have to take the alternative exam i'll post about that on piazza . we are allowing a single cheat sheet . and the material on this midterm the most recent one . the midterm is gonna be a mix of multiple choice and true/false questions short answer questions and some longer more involved questions that may involve radiant computation . and for scpd students you have to turn up in person or have an exam monitor pre-registered . the rest of this lecture is going to be just review material to help prepare you for the midterm . and in particular we're going to cover word vector representations neural network basics back propagation and gradient computation . 
and we're gonna do some example problems on the white board for that section rnns and dependency parsing . and with that i will leave it to our first section word vector representations . all right we can start that again . for every word we could come up with a vector that encapsulates semantic information about the word . there could be various ways of interpreting what meaning or semantic representation could be . and which is why it so severely depends on how you actually go about training your word vectors . in this class we first covered word2vec and glove and we'll be reviewing that now . so just to recap word2vec the task for it is learn word vectors to encode the probability of a word given its context . now consider this example our window size is 2 here . if you consider the center word to be word2vec understand the and model now become context words . and you'll see how to probabilistically model this now . so for each work we have two vectors . the input vector and the output vector then for vectors represented by v and the output vector by u . and we'll see how v and u are used in the model now . so we'll be covering two algorithms here skipgram which predicts the probability of the context words given the center word . and the continuous bag-of-words or cbow which predicts the center word from surrounding context . all right so to recap on skipgram consider the sentence again where i understand the word2vec model now . and in this case since the center word is word2vec that'll be the only word that's left and the context words will be omitted . and our job is to actually go about and assign probably the words there . so how do we go about doing this . 
in this case this vector will have the same dimensions as the number of words in a vocabulary with one of the index of the word word2vec and 0 everywhere else . now we need to use this one-hot vector to index into over embedding matrix which is capital v the input vector in embedding matrix . and we looked in a word vector corresponding to that word . we then go and do a doc product with the output word vectors hue and to generate a score for it . so once we get a score we get the probabilities by using softmax . and once we have those we have softmax probabilities for every context word . and we can then find out which word actually would go into these context windows . how do we actually go about training this . we assign over a cost which is either given by softmax cross-entropy or the negative sampling loss . and as you can see the formula takes abstracts or so we take the input vector and we take the word vector for the context words and we apply these cost functions and sum them over okay . and our job is to minimize this loss . i'm using cost and loss interchangeably as we will do that in the midterm in the class as well . now we move onto continuous bag of words now let's take the previous sentence again . in this case our job is to actually predict the context word or the center word my bad given the context words . so in this case our job would be to actually guess the word2vec model here . so in this case as in the previous case as well we generate one-hot vectors for each of the context words . and this would again be the size of your vocabulary with one at the index of the word itself . and then we look into our embedding matrix again the input vector embedding matrix and obtain the word vectors for those context words . we take the average of those word vectors and then compute the score again by multiplying with the output vector matrix . we can get the probabilities by computing the softmax function . 
and now we have for each of the words in the vocabulary . we have the probability assigned that what how likely it is to be the center word . this should be pretty clear since we also had a live coding session on this . the cost function here is similar to the one in skipgram except in this case it takes in the average of the word vectors as one of the arguments and the center word itself . and again our job is to minimize the loss here . we didn't do this as a part of the assignment but glove is a pretty famous set of word vectors as well . like word2vec it also captures semantic information about the words . but unlike word2vec however it also takes into account the co-occurence statistics . and we'll see what we refer to just in a second . from one of the lines in our review notes glove consists of a weighted least squared model that trains on global word co-occurrence accounts . so what do we mean by co-occurence . consider our corpus to be only a set of three sentences . i like nlp and i enjoy flying . now you can see that we can come up with a n by n where n represents the number of words in our corpus . a matrix where each index corresponds to whether word ga belongs in the context window of word i this is a symmetric matrix as well . so you can as you can assume . so let us denote this matrix by x . and so if you index into this using -ing that would be the number of times word j occurs in the context of word i . and let us also denote xi to be the number of times any word appears in the context window of a word i . just like in word2vec we also have two vectors for each word here the input vector and the output vector . 
and the cost function is given by the equation here . there are a couple of interesting things about this cost function which we'll cover right in a second . but a few things to note here we're applying the least squares as we mentioned earlier . we are iterating over every pair of words in the co-occurrence matrix . what's interesting is that since you can imagine words like d or a the articles as the stock words would have a large xij value we need to sort of clip on it . and the other option is to just take the logarithm of it . and so which is why you can see there's a log over the xij over there okay . so any questions about the cost function here . so now after minimizing this loss for the glove word vectors they will be having v and u where v represents the output word vectors and u represents the input . and since both of these capture similar co-occurrence information we can obtain the word vector for a single word just by summing them up . so just u plus v will equal the word vector for that particular word . and we can use that for all sorts of nlp tasks . it's essentially an embedding for that word . so before we move on from word vector presentations any questions on any of the terminology . we got a lot of questions about how does an embedding defer from a word vector . we want to make sure that that definitions ambiguity is cleared up . it is a vector i mean it is a scalar . if xi is a vector shouldn't the laws actually be referring to a scalar . xi is actually a scalar because it's solely the number of times that word appears in the context window of any other word . so xi is defined as the number of times any word key appears in the context so it's not a vector . 
most research papers tend to use embeddings and word vectors pretty interchangeably . basically you can imagine it's a a vector that encapsulates semantic information . so in terms of analogies you can imagine that if you had word vectors for king man queen and woman then if you subtract king minus queen it should be roughly equal into sweet any other questions . so those input vectors and output vectors are solely just differing in the sense that if you have a context word we are going to predict the output vector for it . so in any case you're considering a word solely you're considering only the output vectors . okay all right so let's cover neural networks . this will be roughly quick because we also have rnns and lstms to cover . you have a data set x and y where x is the input y are the labels . and you want to train a 1-hidden you do a forward pass given by xw + b and then you apply non-linearity or a activation function on it . you compute the loss you compute the gradients from that loss using back propagation . we'll go into back propagation with barack . then we update the weight using an optimization algorithm like saccustic gradient descent or sgd . we do hyperparameter tuning of dev set not on the test set . and then evaluate the neural network on the test set itself all right . so this was a very high level of review of how neural networks have changed . let's go where the activation functions are the nonlinearities quite quickly . what it does it takes and input and squashes at between 0 and 1 . if the activation function was very large if the activation was very large is still always end up being 1 . or if the activation is really small it's still end up always being 0 . so you can imagine that a lot of neurons get saturated by this . 
and the output is not centered as 0 . this is particularly bad because if the output for our sigmoid is always a positive number then the gradients are always negative or positive . and we do not want that we want the gradients to be more adaptive okay . another issue this is also that takes the exponent and it's computationally expensive but nothing in here can run with the map there . okay then the tan h function at between negative 1 and 1 . here in this case the output is centered at 0 so which is a nice thing and it sort of resolves however similar to a sigmoid this also kills the gradients at saturation . and which is why tan h is not as good as well . right so if your input if the output of your node . the question was why is it bad that the output of our activation function is is not centered at 0 . so if the output of your activation function is always positive then the gradients are always positive or always negative because they can only go in one of the two directions always . they do break the so we are able to train networks with this function . so it's not like it's the end of the world in terms of training neutral networks . so in this case what would actually be better is that you take your input and you center it at zero either way works . moving on to relu the rectify linear unit which essentially takes some acts on checks if it's greater than 0 . if it's greater than 0 it returns the same value . it does not have the problem of saturation because it's linear . it's confidentially cheap there are no explain on calculations . and it's also empirically known to converge faster right . so which is why i have the second point in the problem says referring to that . yeah so the problem with this is that it's again not centered at 0 . 
the larger problem with this and it's slightly annoying as well is that if the input is less than 0 then the relu gradient is always 0 . and this is a problem because once a neuron dies it always is dead . you can never revive a neuron after the input becomes less than 0 . anyways moving on to stochastic gradient descent or the optimization algorithm . in this case theta represents our weights or the parameters . alpha is our learning rate and j is our loss or the cost function . and going very technically sgd update happens after every training example . however researchers tend to abuse the notation and call minibatch sgd also normal sgd . gradient descent does is it takes a small batch of averages their loss over before performing the gradient update . okay moving on to regularization this is a form dropout is a form of regularization . while this might seem very counterintuitive that it should work it does because it prevents overfitting . it forces the network to learn dependencies . neurons to learn the same task it forces each neuron to capture more important information about your data and learn more important abstractions . so think about dropout as training an ensemble of networks . every time you run your network during forward pass a couple of neurons are dropped . and as a result you are getting a new network almost every time and then you're training an ensemble of networks . during test time make sure that you turn your dropout also that you again take this ensemble of networks and then use the shared power between all of these networks . so the question was do you have to scale your output whenever you run dropout at tester . and just because if during test time you make sure that your activations are already scaled correctly then you don't have to do it during test time . so a few training tips and tricks here . 
these are a couple of loss plots . so the one in the green represents a phenomenon when your loss is very noisy . and what happens you can see the jagged line . and in this case what must be happening is that since your gradient updates are very large you are not converging properly . you are almost iterating over and around the local minima which is why it's advisable to lower your learning rate in that case . if it's blue you see that there is high potential for your network to actually train faster . and hence you should make your learning rate higher . the red line is sort of an ideal one . again this is something i just drew up on paint . so it's not something that's very technical . so this is something you must have faced issues with in the last assignment . where if the gap between your training curve and your dev accuracy is very large that means you're overfitting . and in this case there are a couple of ways you can actually counter that . one is by increasing your regularization constant . this could be by increasing your dropout rate or by increasing the l2 norm rate okay . and again this is very important to stress that do not test your model in the test set before you're done resolving overfitting issue . it almost sort of breaks the scientific pipeline if you test your network on the test set and then tweak your parameters because you're not supposed to look at the test set . so the question was if you're not supposed to test them on the test set then what kind of data are you suppose to test it on . and the answer to that is you can divide your data into three parts . you evaluate regularly on the dev set . 
and then finally at the very end test on the test set okay . with that i'll leave you to barak for backpropagation and gradients . it's another bright and sunny day to do some backpropagation . i think it's also pretty ironic that last time i spoke to you from here i told you about all of the wonderful with automatic differentiation and here we are again computing gradients by hand . but anyway so the main ideas of this part of the review is to go over some of the intuition and the math behind backpropogation as well as how to use it in practice . before i begin i highly urge you to check out kevin's notes on gradient computation as well as some of the principles behind like matrix calculus . i think it's very helpful to understand you the most with the midterm . okay so our itinerary is to first review what backpropagation is then to have a quick chat about matrix calculus . then we're gonna talk about how to compute products of gradients correctly . in other words when do i transpose my darn symbols . and then we're going to solve two midterm problems . okay so the problem statement that we are looking at is given some function f with respect to inputs x and some labels y including some parameters theta we wanna compute the gradient of your loss with respect to all your parameters theta . and what backpropagation is it is an algorithm that allows you to compute the gradient for some compound function as a series of local intermediate gradients . so if you have some compound function backpropogation is the tool that you have that allows you to compute the total gradient through an application of the chain rule to all of the local intermediate gradients in you function . so there are three kind of parts to backpropagation . the first is that you want to identify what your intermediate functions are i.e the intermediate variables . and this is basically done for you in the forward propagation stage . you then want to know what the local gradients are of all of the variables inside your compound function . and then you want to somehow combine those through some application of the chain rule to get your full gradients . so the first thing that we need to talk about is what modularity is and let us look at is at an extremely simple example . 
we have some function of three scalar variables x y z . we're going to add x and y and then multiply that by z . our intermediate variable is q which is equal to x + y . and our final compound function is q x z . so the idea of modularity is to kind of separate out our smaller operations such that each level of our forward propagation we know how to complete the gradient of the output with respect to our inputs of interest . so the key idea of modularity is that it allows you separate out the operations that you're using . so that at each point you're able to calculate the gradient in some sort of palatable way . okay so modularity for a neural network is basically what we've seen in our assignments so far in terms of splitting up our compound function as a series of all of our propagation steps . so in this example of a two-layer neural network we have the loss of some cross-entropy of a sigmoid activation of some linear function again applying to linear function over that and taking the cross-entropy with y . so intermediate variables are going to be all the things you're familiar with . it's going to be our hidden layer it's going to be the activaion it's going to be our scoring which is z2 in this example followed by our loss . so the idea is that at each step of this forward propagation stage we know how to compute the gradient of our output with respect to our inputs . so let us look at how the forward propagation and the backward propagation relate to each other . on the left we have the forward propagation which is where the values from our inputs are propagated down through our network . propagation is basically the reverse side of the mirror . where at each point we take the gradient at that level with respect to the variable above it . so the last thing that we need to do to finish off our backpropagation is to find out how we're going to merge all of these local gradients together to get our total gradient . and this is as good a time as ever so the key intuition behind the chain rule is that slopes multiply . so if i'm trying to take the derivative of some sort of compound function f and g of some input x over that x . it's going to equal that derivative of our total function with respect to the intermediate value . 
times the derivative of the intermediate value over our inputs . and this is a bit of a mathematical wonder you might think . it's so beautiful that if you have two functions . you can just multiply the slope of the intermediate by the slope of the function that happens after that . so this is kind of the key tool that allows back propagation to work . another useful analogy for understanding back propagation is looking at circuit diagrams . so this is going to be the circuit diagram of our initial simple example . we're going to add x and y and that is our q node over there . we're going to multiply that by our z variable . so the green values in this diagram represent the forward propagation values . and the red values represent the backward propagation . we start off with an error the derivative of f with respect to itself is just 1 . what is interesting though is if we look at the error signal that is happening at the q node . it's currently -4 because the derivative of f with be the value of z minus 4 . we know that the derivative of q with respect to x is just 1 . but what we're going to do to get the total gradient at position x . is we're going to multiply that gradient times our error signal that is flowing into q . and this is i guess the key point of what we're showing here . and we'll see how this kind of figure can help us do back propagation in midterm questions soon . okay are there any questions so far about this large overview of back propagation before we start talking about matrix calculus . 
great okay moving to some matrix calculus . so let us first talk about derivatives over vectors . so the derivative over a vector is going to be a matrix of partial derivatives . where the derivative of each row is going to be the derivative of that index of the output with respect to all of the indices of your input x . so for the scalar by vector example you're going to get a single vector . where each column is going to be the partial of y by the partial of x at that particular column index . in the vector by vector case we're going to have the matrix of partial derivatives . of yi on the ith row by all of the indices at position x . the case for derivatives over matrices question . the question was does the presentation of the derivatives change with respect to whether x and y are column vectors . so a potentially complicated answer to your question is that each of these derivatives represents the jacobean of that derivative . that is a matrix that takes as input whatever shape your x is and outputs the shape of y . so if x was a row vector yes . so the question was are we going to be using the convention that x is a row or a column . that depends on the question and it'll be stated very clearly when you're taking derivatives in that question okay . so the case for derivatives over matrices is slightly more complicated . you can interpret y as a function . if i'm trying to compute my partial of y over my partial of a where y is a scalar . y is actually going to be a function of every single element inside your matrix a . okay so the proper derivative of partial y over partial a is actually going to be an element of thanks . 
it is actually going to be an element of rmn . so the true jacobean derivative of partial y over partial a is actually going to be some long form vector ofsize mn . but this is not such a good presentation for us when we want to do derivatives of matrices . so we actually rearrange that sort of derivative to be some matrix of derivatives . where the partial of y at each index is going to correspond to the partial of the case for derivatives of vectors over matrices is even more complicated . because the true form derivative of a derivative over a matrix is actually some sort of three-dimensional array . or it's going to be a tensor that is sort of beyond the scope of what we're trying to do here . so since we're only interested in computing with respect to matrices . we can actually hide out that sort of strange three-dimensional array derivative thing . cuz we're going to be computing it by some error signal from above . and let me show you what that means over here . so if we're interested in computing the derivative of z with respect to a . let us look at what the derivative would be of that z with respect to a single element inside our a matrix . so you can think of aij as representing the sensitivity of the ith index of the output with respect to the jth index of the input . and this is what we're going to be looking at . so the derivative at the ith position of the dz gradient is going to be exactly the value of xj in our input x . since that is what is modifying the a . and this leads to the identity that the partial of j over the partial of aij is going to equal the dot of our delta . since that is the only value that is not 0 in our derivative of z over aij times the value at xj . and this leads to the identity of dj by da . 
so you can somewhat ignore the slide and just focus on the identity for the purposes of the midterm . but this might be a bit of an explanation of what it means to take sort of derivatives over matrices . okay so on this slide i have for your reference . a list of perhaps the most useful identities that you'll need for computing all of the gradients that we use in our midterms . so i highly urge you to check these out or to check them in the notes . so are there any questions before we move onwards . so one thing that you might notice from the previous slides . is that the nice thing of taking derivatives of scalars over vectors or matrices . is that the shape of our output has the same dimensions as the shape of our inputs . both in the scalar-by-vector case and the scalar-by-matrix case . so what we enforce when we do back propagation is this shape rule . where when you take the gradients against a scalar which is what you're essentially always doing . the gradients at each intermediate step have the shape of your denominator . so if x whether it's a row vector column vector or matrix has shape m by n . the error signal of d scalar [inaudible] loss by dx is equal to the same size as our denominator and what this allows us to do is dimension balancing . so this is going to be the general gradients for any sort of matrix multiplication you do on any gradient question where x and w can be either both matrices or x can be a row vector and w a matrix or x can be a matrix and w a column vector so this is a general form . so if z is going to be some m times w matrix and we represent the error at that point as partial loss over partial z as being delta that because of our shape rule we know has shape m times w . how do we find out what the dimension of our d loss by d x . well if we are looking for a gradient of shape m times n multiplying our gradient signal by w using our identities from the previous slide . then we realize that we need to take the transpose of w so that the w is on the left side and the delta is and the w is on the right side of the delta . 
so you can kind of solve these identities by looking at the dimensions of all of your terms making sure they balance out to give you the correct dimensions for the gradients . by matching the gradients of your error and the gradient the dimensions of your gradient and the dimensions of your term . are there any questions about this dimension and balancing concept . delta is going to be the error signal at the point of z . so since we're trying to compute the gradient of the loss with respect to x and the level above that the intermediate level above that is z . to apply the chain rule we're going to have delta represent the derivative up to the point z . and we're going to multiply that as an application of our chain rule with the derivative of z with respect to x . so the idea whenever we're if i'm starting with some z as a matrix multiplication of x and derivative of loss with respect to w is . using the chain rule i know that that's the derivative of the loss with respect to z times the derivative of z with respect to w . so this is going to be our intermediate variable that we're taking our local gradient of . and we know what the derivative of this with respect to this is . but this is going to give us the error signal up to that point . the value of the error is up to the point of this intermediate computation okay . the error signal is kind of what the gradient is up to the point of your equation of the loss down to the last variable that you're looking at which is the output of that expression all right so it kind of feels like dimension balancing is this kinda cheap and dirty approach to doing gradient calculations . but they are indeed kind of the most efficient tool that you have to computing gradients quickly in most practical settings and especially the midterm . but i do encourage you to read the gradient computation notes the gradients work from first principle . the last thing that i wanna talk about is how gradients over activation functions work . so one of the questions that we fielded frequently in office hours is why is it the case that when you're taking gradients over an activation function you're doing this element-wise multiplication or hadamard product . and the answer to that is if you're looking at so the answer to that is the activation function is a function that maps a scalar to another scalar . though we represent z as a vector as a sigmoid of an entire vector h the sigmoid is actually being applied to each individual element in that vector which maps the same index in your output . 
so the way we represent the true gradient the true derivative is that it's going to be a diagonal matrix . where each index on your leading diagonal is going to be the gradient of the ith index of your input . and multiplying some matrix or vector by a diagonal matrix is equivalent to doing the element-wise multiplication of a vector itself . which we represent and the hadamard product . far before we get to some midterm problems . this is the true jacobian form of the partial of some y vector with respect to x you can see that it's a matrix that takes as input a column vector x and maps it to an output y . so if the shape of your jacobian takes as an input something with shape x and maps to an output something with a shape y which is exactly what is happening in the top left box over here . can we talk about that question maybe after the lecture so we can move on . cuz we have a few more sections . okay so let's get through and maybe when we actually do some midterm problems it'll become more apparent how it works . okay so the first question that we're going to look at is to do with siamese networks . and a siamese network basically allows you to compute a similarity metric between two inputs x between an x1 and some x2 inputs . and this allows you to sort of compare the similarity of two word vectors or two sentences . so the first thing that i like to do when we're starting some gradient problem variables are related to each other . so we can see is we start off with some j at the top we have our cost function j . it takes as input an h1 and an h2 as two hidden vectors . these are activations of z1 and z2 . and i am defining these variables myself so i need to specify what they are . and this is going to be in this problem we're treating our inputs as column vectors . and for z2 it's going to equal to x2 plus b . 
so we have two inputs x1 and x2 . and one interesting thing to notice about siamese networks is that the variables sorry the parameters w and b are shared across both of these activations . so if we're interested in taking the derivative of j with respect to w we need to add the derivatives coming from both branches down . so the first thing that i like to do when i'm computing my gradients is basically to express this graph and we'll see how it's useful to us in a second . okay can the camera see me awesome . so if we want to start our gradient computations the first thing we want to do is to take the derivative with respect to j . so let me write out what my j is going to be . it's going to be half of the hadamard of h1 minus h2 squared f . and we're going to have a regularization term . so this is i'm first writing out what my cost function is going to be . and i use this graph to kind of inform me about where i'm taking my derivatives and what my error signals are going to be . so the first thing i wanna find out if i wanna compute what is the derivative of j with respect to w . so the first thing we're going to do is flow down this branch . so we're going to compute the partial of j with respect to the partial of h1 and i'll leave this for what the derivative over the frobenius norm is . but it's basically just going to be h1- h2 cuz the 2 cancels out and you're left with an h1- h2 term . equivalently the gradient with respect to h2 is of that expression okay . so one thing that i like to do is to also record what the dimensions are of all of my gradients . so over here since we're dealing with column vectors this is going to remain an m by 1 vector . and this is going to be an m by 1 vector as well . and the other thing i'm going to do is give a name to these error signals . 
this one is going to be delta 1 and this one is delta 2 . using this graph is what my error signals are flowing down it . so over here i've defined delta 1 to be the partial of j with respect to partial of h1 . and over here we have delta 2 being the partial of j over the partial of h2 okay . so the next thing we are interested in computing is what is the derivative of j with respect to z1 . so we're going to apply our chain rule . we know what the derivative of j with respect to h1 is so we only sorry we want to compute the derivate of j with respect to z1 . we know what the derivative of j with respect to h1 is so we only need to compute the derivative of h1 with respect to z1 . so over here we have a sigmoid activation function so we just need to compute the gradient over that . so my partial of j over equal partial of j by partial of h1 times the partial of h1 by the partial of z1 . and that is equal to my first error signal with the hadamard product of this is getting a bit tight here it's going to equal my error signal hadamard product with h1 times 1- h1 . which as we know from assignment one is the derivative of the sigmoid . similarly the partial of j with respect to z2 is equal to the partial of j by the partial of h2 times the partial of h2 by partial of z2 . and that's also going to equal our current error signal flowing down to h2 so it's delta 2 hadamard product with h2 times 1- h2 . and we're gonna give this a name . this is now going to be delta 4 and this one is going to be delta 3 . okay so over here we have delta 3 and here delta 4 okay . so now the last thing that we wanna do if we wanna compute the gradient of j with respect to w this is going to equal my error signal up to z1 so partial of j by partial of z1 times the partial of z1 by the partial of w . it's also going to and we're going to add to that what's happening in the error signal at the other branch . that's partial of j by partial of z2 by the partial of z2 over the partial of w . 
plus we don't want to forget about our regularization experiment . frobenius norm of w by w okay . and this is going to now we need to do some matrix balancing using what we discussed earlier . since we're taking the derivative over this matrix product over here . so what we need to do is we want this final product of each of these things to have the same shape as w . this is just given to you in the problem . we know that delta 3 and delta 4 are m by 1 . because when we computed our delta 3 and delta 4 we just took the elementwise product of whatever was up here so it's still going to remain m times 1 . finally we know that x1 and x2 are both n by 1 since they are column vectors . so if we want to we know that this is delta 3 and we're going to have to multiply it by the xs in some way . so we see that if we want our final to be m times n we're gonna have to do this expression times the transpose of this . plus delta 2 sorry delta 4 times sorry delta 3 times x1 transpose + delta 4 times x2 transpose . and finally we want to add this expression . and again i will leave this for you to show on your own time that the derivative of this frobenius norm the 2 cancels out with this . and you're just left with lambda w okay . what happens if you run into some sort of ambiguity over here . so the nice thing about when we're since at least for computing the neural network models that we are looking at for the midterm and . the only tractable problems that we're going to be computing gradients over are things like linear transformations and activations over those which are already the key operations that you use in neural networks . apart from things like cross entropies and other losses at the top . so in most of those situations you won't really run into ambiguities . 
and when you do you just go back to deriving things using like traditional matrix calculus so you just dig deeper . like if you dug deeper into how these dimensions work out it should still work out . so you just won't find ambiguities in these types of problems okay . i will quickly run through my second example . okay so the next example that we are looking at . suppose you wanted to build a word vector representation that you and couldn't decide whether you wanted to have a hidden layer that is activated by a sigmoid . or you cannot decide whether you wanted a hidden layer that is activated by a relu . so the only thing better than making a choice is to do both of them . so what we're going to do is we're going to have a network where at the top we're going to have a cross entropy over some prediction y that takes n from z3 . so over here we're going to have some input that flows through both of these variables . so here we have xw1 + b1 . so we're going to apply a linear transformation over our input x and then sigmoid activate it to produce this hidden layer . over here we're going to have a different linear transformation with two other parameters two other matrix and bios term . but here we're going to relu activate it . then we're going to add our h1 and h2 . at z3 we're going to guess our score vector which is z3 and then we apply the cross entropy to that that's your model . representation we did in assignment one except now we have two branches that take the input twice . so what we want to compute at least for this question is what is the partial so the nice thing about this problem is that we already know how to compute the derivative of our j with respect to z3 . since z3 is the input into our cross entropy and we did this in our homework . so the partial of j with respect to z3 is just going to equal to y hat minus y as we already know . 
and this is going to be our first r signal . so we already know what the errors from j flowing down to z3 . and we're calling that delta 1 okay . the partial of z3 with respect to h1 and h2 are actually equivalent . cuz if we know what the partial of z3 is with respect to this sum since the gradients of h1 + h2 and similarly with h2 we can simply write down the partial of j with respect to h1 is equal to the partial of j with respect to sorry partial of h2 . which is equal to the partial of j up to z3 times the partial of z3 over partial of h1 + h2 . and what we need to do here is again some matrix balancing . since we know that h is a [1 x m] row vector . and i forgot to specify but this is actually pretty crucial and this is the next slide . i did convert this problem into the exact same model except using row vectors . just to show you that the way we do dimension balancing in a row vector versus a column vector situation is basically exactly the same . and our w3 is m times k is m by k . our delta 1 is also 1 value k . so what we can see is that if we wanted to multiply our error signal from above by what we know is going to be the derivative with respect to h1 + h2 we're going to need to transpose our w3 and multiply our delta 1 by that transpose . so going up here i'll just write an equal sign . this is i'm continuing this line over here . that is just equal to delta 1 by w3 transpose . and i'm going to call this delta 2 . and then i'm going to fill in my error gradient flowing down here okay . moving over to this side if we wanna move down the branches we now wanna compute the derivative of j with respect to z1 and z2 . 
so the partial of j with respect to z1 is equal to the partial of j up to h1 . there's a partial of h1 up to z1 and that's just going to equal since this is an activation this is a gradient over an activation function we again use our elements y as multiplication so that's just equal to delta 2 . by since on the left branch we're doing a sigmoid we're going to have just h1 times 1 minus h1 . and on the other side our partial of j with respect to partial of z2 . and i'll just skip writing out i won't write out the chain rule for the sake of time . that's going to equal our delta 2 by the gradient of the relu activation function which is 1 . if our z2 is greater than 0 based on the value as we it saw earlier . so the last thing we're gonna do and i'm again running out of space is to compute the gradients with respect to x which is what what we wanna do . so the partial of j with respect to x is going to equal the partial of j up to z1 times the partial of z1 respect to x . plus since we're working with our derivatives x appears in both branches it's going to equal the partial of j with the respect to z2 partial of z2 with respect to x okay . and what we're gonna do is matrix bouncing one more since we're taking derivatives so we wanna make sure our dimensions balance out . so we know from the problem statement that x is a row vector . all right w1 w2 are by n by m and that's just given from the problem formulation . so since x is [1 x n] and we're gonna need to compute we're gonna need to multiply our error signals by this being transposed . and although i didn't write over here you can see you can prove to yourself that delta 3 and delta 4 are 1 by m . so this final expression and going down here . since we need to transpose this it's going to be the error gradient up to this point . so delta 3 times our w1 transpose since w1 appears on the left side plus the error signal delta 4 times w2 transpose . bit long winded but the general approach that we're using is to first of all build the graph of our variables starting from j we compute the derivative going down and at each point we mark what the error signal is up to that point . after that we apply our chain rule where we compute the derivative of our current variable times our new thing going down and signal coming from above . 
and we use matrix balancing to make sure our products work out . i would say yes since it helps you keep track it helps to keep track of derivatives up to a certain point . and it also helps you if you can just take my current error signal specify what the dimensions of that are . it's very easy to make sure that your dimensions balance out when you compute my error signal by what follows afterwards . i mean you don't have to but it certainly is a way . of keeping your calculations clean and organized . for the sake of time i'm going to move on to the next section about back log after the lecture . so my last slide and you can look this up later . your menu for success i recommend doing all of them is to write down your graph compute the derivatives from the top down keep track of your error signals and force that the dimensions balance out . and yes that is it thank you . so our men are probably some of the coolest architectures that you've learned in this class and and i may have the pleasure unrolling them a little bit further for you . we'll also share some material questions from past exams . so here you now have a distributed representation of each patient note . you assume that a patient's past medical history is informative of their current illness . as such you apply a recurrent neural network to predict the current illness based on a patient's current and previous note-vectors . you also explain why a recurrent neural network would be better than a feed-forward network in which your input is summation or average of past and current note-vectors . so you can talk to your neighbors just like take a minute to discuss this problem and figure out why an rnn would actually be better than a feed-forward one . are there any suggestions to why our rnn might be better than feed-forward ones . i'll try to repeat what you just said you can correct me if i'm wrong . so you said that the rnns they can do something specific for a particular patient and while if you sum them up they wouldn't . 
are there any other suggestions as well . mm-hm and so what you just said and again you can correct me if i'm wrong is that the more recent information about this patient might be more relevant . and that is something that an rnn could capture which a feed network might not be able to capture . yeah so what i just said right now is that and there is some time dependency in this data since we have a sequence of a patient notes . and if we were to sum them up together we lose that temporal information . and so these are all very good answers . and i do want to comment just on the first answer a slight misunderstanding . so we are not summing up across different patients . i'm summing across the nodes for the same patient across time stubs . and i think that was probably just like maybe it wasn't phase super clearly . and what the other people said was and also correct . so one thing is that rnns can take in more recent information into account . and in a more general sense they can take in temporal relationships into account which we would ignore by summing up or arching over them . cool so in order for us to see this more yes there's a question . the question was on what advantage would an rnn give us over taking a weighted average . often note-vectors well if you do weighted average we actually had to come up with the weights for on the particular notes which we actually don't really know beforehand . and the idea of like chaining an rnn here is that like the rnn might potentially be able to learn to what extent it should take in . really old notes into account versus perhaps like more recent ones so we don't want to predefine those weights . and we can also talk about that after class as well . so let us take a really quick look at the rnn structure . 
so this is a very simple vanilla rnn . they key points are the weights are shared across timesteps . and the core of an rnn is the hidden state and the hidden state depends both on that previous hidden state so kind of like capture some of that memory from earlier as well as the new input and you should have seen that on the assignment two . and rnns because of their architecture are very suitable for learning representations for sequential data that has sometimes temporal relationships . and the predictions can be made at every timesteps so you can have a y as can be seen in this figure over here . you can make a prediction every timestep or you can make can at the end of the sequence depending on whatever your application might be . so in our key rnn's come to use a special when we do language modelling . so language modelling is the tasks of computing probability distributions over a sequence of words so that can be for example seconds or a document . and language modeling is very important when we are doing things like speech recognition text summariztion and so on . use it as a language model would be passing at every time subsidy inputs xt are our word embeddings . and you could use word embeddings for example that you produce with word and so you pass it at every timestep . and our prediction is just the next word in the sequence . so we can use that as our task in order to train this rnn . and the idea is that the hidden representation at the end . so if you take the hidden representation at the last time stamp on our trained rnn capture some of the semantic meaning of that sentence or sequence of words . so that can be for example used for translation which is something you saw or you saw on tuesday i think which richard presented . so here this is actually a [inaudible] rnn which have an encoding part and a decoding part . so for the encoding part that's your first rnn . you feed in the word embeddings for the german words so [foreign] . and then you pass in that hidden representation which captures the meaning of the sentence into the second rnn . 
so the weights are shared across the first rnn and they're shared within the second rnn . and a second rnn is a decoder which produces the english sentence here . so this is one example of using this type of rnn as a language model for a translation . okay so the problem though with the vanilla rnns is that we can see the problem vanishing gradients . in back propagation when we do back propagation rnns call on the hidden layer . and so that's basically since we define a hidden layer in terms of its p precedent layer . and the magnitude of the gradients off the typical activation functions that we'd use like ton h or sigmoid are between 0 and 1 which causes and if you multiply a number that's smaller than 1 multiple times what happens is that the number will shrink very quickly . and if it shrinks very quickly essentially what happens is that you final gradient will be close to 0 . and if your gradients are close to 0 then essentially means that you're so in order to address this issue this is like why people have worked on many of the different architectures and grus lstms were some of the more popular ones . okay i should also repeat the question . so the question was can we adjust the vanishing gradient problem by using a different activation function something like a volume . yeah we can also discuss this after class too . okay so grus and lstms to the rescue . so i'll talk briefly about grus gated recurrent units . so the addition here is that we are introducing gate . so we have a reset gate and an update gate . and you can intuitively understand these gates as they're controlling the long term and short term dependencies . to what extent we want to memorize things from the past . and to what extent to we just want to take something from the current input . and that might be also something that you might see or like . 
it's related to what we talked about with patient notes . it's kind of this network might be able to learn whether i should only look at the last patient note or whether i should take like the entire history into account . so and this is a more visual representation of a gru games . so you can see that the input x team over here is fed into both of these gates which are sigmoid functions . and it's also fed into this other one which calculates h tilde . so this is kind of a first initial hidden state but in order to decide whether we really want to use it we are using the z factor over here to decide whether we just want to use the previous hidden state or if you want to update it to the h tilde this might also be a picture if you wanted to look at it again . it's on our blog so you can just try to figure out how this information is flowing and there . but in order to just get some more intuitive understanding of how gru works we are going to do some exercises together . so first try to figure out what the dimensions are off the ws and the u matrices in here . and you can write it in terms of the x dimension of x and the h which is the dimension of h . again you can just take a minute and try to figure that out . and you can also discuss again with your neighbor . since we are running a little bit out of time i'm just gonna go ahead and explain how i would approach this problem . so first you know that ht the hidden vector h has dimensioin h just by definition . so i'll write a little bit bigger okay . so this is just by definition and we also know that ht minus 1 was also has mentioned dh which means 1 minus zt so zt itself needs to have to mention the dh as well right . since we are doing a aliments bite operation in the fourth equation over here . so it also shows us that h tilde suggest by looking at the last equation h tilde means half dimension dh as well . and now that we know that h tilde has dimension dh equation over here . so we know that the product of w and xc has to give us something with dh . 
so that tells us that the matrix w needs to have dimension dh and dx . cool i see a couple nods okay . so similarly if you look at the matrix u in there we know that like the result of u ht minus 1 needs to be dh . like [inaudible] so percent should be dh . and since we're multiplying it with a hidden vector the second dimension should be dh as well . and you can apply a similar approach to figure out the dimensionality for the other matrices as well . if the update gate is close to 0 the net does not update its state significantly . yeah this one should be pretty obvious if you look at the fourth equation because in that case you're essentially just setting ht to ht minus 1 . and this also shows that if the network learns that zt is pretty close to 0 that just means that we are essentially using our input from the first time . so essentially this means that your input from a long time ago still matters a lot in the future . okay another related question if the update gate is close to one and a reset gate is close to zero the net remembers the passing very well true or false . yeah this is actually very similar to the first question so again you can just set them on z t to 1 r t to 0 in these equations . and you'll find out that in this case h t would depend very strongly on the input since we're essentially saying that this term over here becomes zero . and we're only using the x t part of it . okay so joee will now talk about lstms . lstms are kind of similar to gius except they are a more common model . so instead of just having two gates now we have multiple gates . one is the input gate which decides how much weight should we give to the current input or the current word that we are looking at . the ft gate or the forget gate will decide how much we want to forget our past how much you want to remember the past or just forget it . the new gate ot which is a little bit different is how much we wanna expose our current cell to future . 
so combine the ot from the current combined with the ft from the future will decide how much all of memory will be use in any other future node . so you can see how adjusting this can decide like whether you just wanna remember the b minus 3 times 3 but not t minus 20 minus 1 . instead of just going linearly backwards i'm having to remember everything from the past . so all these gates are computered using a sigmoid because that's between 0 and 1 . and that makes sure that when you are doing the dot products of sorry the micro dot products in the end you'll have kind of like a probability distribution of not just completely forgetting which would be zero . or completely remembering which would one a fuzzy possibility of remembering a little bit or forgetting a little bit . another difference between lstms and grus is the ct and ht node so now you don't just have one memory you have two memories . like the ct would define exactly what your memory is while your ht defines in your it ft and ot how much or how you want to remember the ht minus one memory . or how much your memory should be remembered by the future . so instead of just having one now we have two . these have a certain number of disadvantages for example there are a lot more parameters now which means you need more space . which means you need more learning much more training data so that your model doesn't start warpening . and empirically grus and lstms have been are very close in results . so in the end it's a trade off on how much accuracy you want compared to how much you are willing to use your resources on training time and training data . the lower row corresponds to your ht flow while the upper row corresponds to your ct flow . and both of them are memories and they interact with each other which means they are not completely independent . but they represent different ways in which your future or your next words will be using your current word . so the question was can you give a little bit of intuition on what is the difference between c and h and so if you see from the formulae the edges are present or the edges decide on what your input gates your fts or ots will be . so they decide how much you want to remember or forget or expose yourself . while your cts are the ones that are actually being used in the future . 
so they are the ones that and so basically it's like saying your hts will decide the fuzzy probability of how much you want to remember and how you want to remember and your ct will kind of decide what you want to remember from the previous states . this is just like illustration it's not exactly like that . but that's like a rough way of how you can think about it . great so the question was will we have the equations . and yes all the equations for rins or lstms or grts will be there so that you don't have to remember them . also you are allowed a cheat sheet so to be honest if you wanted to remember it just write it on your cheat sheet and you can bring it . so we'll just go to the midterm questions really quickly because we are running out of time . if xt is the 0 vector then ht = ht -1 . it is true that ht will depend primarily on ht -1 but it won't be exactly equal because of the non-linearities and the multiplication with the parameters . the second question is if fd is very small or 0 then error will not be back-propagated to earlier time steps . again intuitively it feels like because the fd is 0 they are trying to forget . but that is not true because it and ct still depend on ht minus 1 and hence the error will still back propagate . so one easy way to do it is just look at the mat see what depends on what and see how the rows are actually back-propagating . the next question is if either entries ft it and ot are negative and we see that the sigmoid activation is used which is the range is between zero and one . and as i said that just represents the fuzzy probability . which brings me to the next question can they be viewed as probability distributions . and the problem with viewing them as probability distributions is that they do not sum to one like any probability distribution should . so these are applied independently of the element and of having to sum to one . so this is just like an intuition of probability but does that make sense to everyone . so very quickly we'll go dependency parsing . 
so these are two views of linguistic structures we haven't seen which was constituency structure we'll be seeing it for later . but basically it uses a kind of c f g model where you decide which phrase is broken down into what words . what we are seeing is dependency structure where we decide what word depends on what other word in the sentence and how they depend on each other so what modifiers are used . it's binary which means two words are related to each other no more and no less . it's asymmetric which means one word depends on the other but not the other way around . in this case the dependent depends on the head . which are the terms that we'll generally be using . and the arrows when you're drawing the dependency tree will go from the head to the dependent . they usually form a connected acyclic single-head tree . and to make sure that this happens we also add a fake root node so that one of the words will always and this also makes sure that every word has a head . and none of the words are just standing . and in the rare cases where the root known make sure that all of them are connected into a single tree . so we saw two different types of dependency parsing . one was the greedy deterministic transition based parsing . we've just seen this in the assignment right now so i am sure it's all fresh in your memories . you have a stack a buffer and a dependency list . initially the buffer will be full of the words in your sentence in order . so that will act like a queue structure . your stack will act like a stack structure so that you pop from the top of this chart . and whenever you do this shift transition you take something from the top of the queue and put it onto the stack . 
the left toll will take the top two elements from the stack and make an arrow from the first element to the second element and just remove the second element . and this can be seen fro
m the example here which you can just scroll through the slides and go like decide . now how do we decide which transition to use . this is generally done by some kind of a classifier you could use multiclass as svms or any other kind of machine learning classifiers that you know of . learning involves need so here we'll use the features and you can always add more features . but the typical features that are generally used is the top of the word on the stack the first word in the buffer and maybe a lookahead on what words are gonna come next . and also the dependence of the current word in the stack . and now that we have all the words we also use the parts of speech of all these words . so generally we know that adjectives and nouns already likely to be dependents or connector so that there's something that you would want to use as a feature . yeah those are kind of what dependencies that we have already figured out until now . and using all of these we try to get what kind of transition we want to do next . and define the valuation metric that we use is either uas if you're not typing the dependency from one word to another . or las which is the liberal attachment school which types their attachment from one word to another and these are basically kind of like accuracies . you can think of them as accuracies . so one thing that we saw was projectivity and the non-projectivity comes in . and what projectivity means is that there are no arrows that cross each other when you put them in a horizontal line . and why this would be a problem is if you go back to this kind of a parsing mechanism here the from and who will never be next to each other on a stack . and thus left arc or right arc will never be able to get the dependency between from and who . and so how do we handle something like this . one simple thing would be you just declare defeat . 
you just say that okay these are really really rare cases . and if you don't care about your accuracy as much and you don't want to complicate your model a lot then you can just leave them as it is and let them decrease your accuracy a little bit . if you want to handle them one of them would be to use a post resistor . so you go through the entire parsing mechanism and in the end you use some kind of a post-processor to identify which ones have been parsed strongly and try to dissolve them . this can be done using multiple methods like classifiers or other things . the last one is you use a completely different parsing mechanism or to the ones that already exist . so in the greedy transition based parsing you could add a transition let's say swap where you just swap the elements that are already in the stacks so that you bring the element that is at the bottom to the top and then you can either lift right down to it . there are other more complicated ways of doing it and those can also be done . finally we have the neural dependency parsing . so one problem with really the domestic parsing that we saw was the features were all one-hot vectors where it was either a word or a part of speech . and we have seen a lot of problems with one-hot vectors before . one is that you don't have the semantic representation of the word . so if two words are actually similar in either a grammatical sense or meaning wise you don't understand digest because of the one-hot vector notation . and an easy way to do some to solve something like that and here we'll not only be using embedded matrix for the words but also the parts of speech and the dependency labels . and all of these are then added into one feature stack or a feature drawer and they are put into a black box which would be any unit that you want to use . for example here you are using a hidden layer and a soft max layer . neural network and then finally you use classification to decide what transition you want to do . and i know this was a little quick but we are running out of time so . okay so there are just a couple of acknowledgements and all the best for your exam . 
if we're gonna take our model and export it to somebody else . there's no reason for it to actually the data is arbitrary it's the model parameters that are the foundation of your model . they are what makes your model interesting and computing what it computes . great so what have we covered so far . we first built a graph using variables and placeholders . we then deploy that graph onto a session which is the execution environment . and next we will see how to train the model . so the first question that we might ask in terms of optimization is how do we define the loss . so we're going to use placeholder for labels as data that we feed in only at run time and then we're going to build a loss node using our labels and prediction . the first line in code here is we're going to have this python variable that is the prediction at the end of your neural network . it's going to be the top of some soft max over whatever it is that your neural network is outputting a probability the first sign is where is the end of the feed forward stage of your neural network . it's what your network is trying to predict . we're then going to create a variable called label that is a place holder for the ground truth that our model is trying to train against . now we are ready to create our cross entropy node which is just like in our assignment one . it's going to be the sum of the labels times the tensorflow log of the prediction on our column . so just an interesting point so tensorflow functions but tensorflow will automatically convert addition subtraction and element wise multiplication into tensorflow operations question . the row altogether which is what we want to do since label in the label each row so you wanna multiply that by our prediction . and it's going to multiply it at the point of the target index . and when we sum that it's going to give us the correct result . everything else will be a zero in that row so it's squashing it into a column . 
so lets see how this is run in code . we're going to build a session object and we're going to call run on two arguments fetches and feeds . fetches are the list of graph nodes that return the outputs of the nodes . these are the nodes that we're interested in actually computing the values of . the feeds is going to be a dictionary mapping from graph nodes to actual values that we want to run in our model . so this is where we actually fill in the placeholders that we talked about earlier . so this is the code that we have earlier and we're gonna add some new lines . we're first going to build a session object called tf.session . most likely a cpu but you're able to add in as an argument what device you want to run it on . first of all session.run on initialize all the variables . this is concept intensive flow called lazy evaluation . it means that the evaluation of your graph only ever happens at run time and run time now we can add an interpretation to run time in tensorflow so and so means the session . once we build the session we're the tensa flow run time so it is only then that we actually stick or assign the values that we initialize our bmw on to those notes . after those two lines we're finally in a position to call run on the note we're actually interested in the h and we feed in our second argument of dictionary for x it's our placeholder with the values that we're interested . for now just some random values question . initialize all variables will initialize all the things that are formerly called variables in your graph like b and w in this case . what is the difference between variables and place holders and why we might we might want to use which . so place sorry variables are in most cases will be the parameters that we're interested in you can almost think of them as the direct correspondence x are data is not a parameter we're interested in tuning . in the models we are working with . additionally it's important that our parameters have initializations in our model to begin with . 
in our case it's just b and w what we mean by saying that variables are stateful is that they retain their current value over multiple executions and it's easy to restore saved values to variables . so variables have a number of other useful features . they can be saved to your disk during and after training which is what facilitates the use the niche talked about earlier that allows people from different companies and groups to save store and send over their model parameters to other people . and they also make gradient updates by default . will apply over all of the variables and your graph . the variables are the things that you wanna tune to minimize the loss and we will see how to do that soon . it is really important to remember that variables in the graph like b and w are still operations . by definition if there can be such a thing as a definition on this all of your nodes in the graph are operations . so when you evaluate the operation that is these variables in our run time and we will see what run time means very shortly you will get the value of those variables . the next type of nodes are placeholders . so placeholders are nodes whose value is fed in at execution time . if you have inputs into your network that depend on some sort of external data that you don't want build your graph that depends on any real value . so these are place folders for values that we're going to add into our computation during training . so this is going to be our input . so for placeholders we don't give any initial values . we just assign a data type and we assign a shape of a tensor so the graph still knows what to compute even though it doesn't have any stored values yet . the third type of node are mathematical operations . this is going to be your matrix multiplication all of these are nodes in your tensorflow graphs . and it's very important that we're actually calling on tensorflow mathematical operations as opposed to numpy operations . okay so let us actually see how this works in code . 
we're going to create a placeholder variable for our input x and then we're going to build our flow graph . so how does this look in code . we're gonna import our tensorflow package we're gonna build a python variable b that is a tensorflow variable . taking in initial zeros of size 100 . our w is going to be a tensorflow variable taking uniformly distributed values between -1 and 1 of shapes of 184 by 100 . we're going to create a placeholder for our input data that doesn't take in any initial values it just takes in a data type 32 bit floats as well as a shape . now we're in position to actually build our flow graph . we're going to express h as the tensorflow relu of the tensorflow matrix multiplication of x and w and we add b . so you can actually see that the form of that line when we build our h essentially looks exactly the same as how it would look like a numpy except we're calling on our tensorflow mathematical operations . and that is absolutely essential because up to this point we are not actually manipulating any data we're only building symbols inside our graph . no data is actually moving in through our system yet . you can not print off h and actually see the value it expresses . first and foremost because x is just a place holder it doesn't have any real data in it yet . but even if x wasn't you can not print h until we run a tune . we are just building a backbone for our model . but you might wonder now where is the graph . if you look at the slide earlier i didn't build a separate node for this matrix multiplication node and a different node for add and a different node for relu . we've only defined one line but i claim that we have all of these nodes in our graph . so if you're actually try to analyze what's happening in the graph what we're gonna do and there are not too many reasons for you to do this when you're actually programming a tensorflow operation . but if i'm gonna call on my default graph and then i call get_operations on it i see all of the nodes in my graph and there are a lot of things going on here . 
you can see in the top three lines that we have three separate nodes just to define what is this concept of zeroes . there are no values initially assigned yet to our b but the graph is getting ready to take in those values . we see that we have all of these other nodes just to define what the random uniform distribution is . and on the right column we variable_1 that is probably going to be our w . and then at the bottom four lines we actually see the nodes as they appear in our figure the placeholder the matrix multiplication the addition and the relu . so in fact the figure that we're presenting on the board is simple for what tensorflow graphs look like . there are a lot of things going behind the scenes that you don't really need to interface with as a programmer . but it is extremely important to keep in mind that this is the level of abstraction that tensorflow is working with above the python code . this is what is actually going and it is also interesting to see that if you look at the last node relu it is pointing to the same object in memory as the h variable that we defined above . both of them are operations referring to the same thing . so in the code before what this h actually stands for is the last current node in the graph that we built . so the question was about how we're deciding what the values are and the types . this is purely arbitrary choice we're just showing an example it's not related to it's just part of our example . and the next question is how do we actually run it . so the way you run graphs in tensorflow is you deploy it in something called a session . a session is a binding to a particular execution context like a cpu or a gpu . so we're going to take the graph that we built and we're going to deploy it on to a cpu or a gpu . and you might actually be interested to know that google is developing their own integrated circuit called a tensor processing unit just to make tensor computation extremely quickly . it's in fact orders of magnitude more quick then even a gpu and they did use a tender alpha go match against lissdell . so the session is any like hardware environment that supports the execution of all so that's how you deploy a graph great . 
i'm gonna be introducing some of the main ideas behind tensorflow some of its main features . the big ideas about tensorflow is that numeric computation is expressed as a computational graph . if there was one lesson that you took out of this presentation today at the back of your mind is that the backbone of any tensorflow program is going to be a graph where the graph nodes are going to be operations shorthand as ops in your code . and they have any number of inputs and a single output . and the edges between our nodes are going to be tensors that flow between them . and the best way of thinking about what tensors are in practice is as n-dimensional arrays . the advantage of using flow graphs as the backbone of your deep learning framework is that it allows you to build complex models in terms of small and simple operations . and this is going to make your gradient calculations extremely simple when we get to that . you're going to be very very grateful for the automatic differentiation when you're coding large models in your final project and in the future . another way of thinking about a tensorflow graph is that each operation is a function that can be evaluated at that point . and hopefully we will see why that is the case later in the presentation . so let us look at an example of a neural network with one hidden layer and what its computational graph in tensorflow might look like . so we have some hidden layer that we are trying to compute as the relu activation of some parameter matrix w times some input x plus a bias term . so if you recall from last lecture the relu is an activation function standing for rectified linear unit in the same way that a sigmoid is an activation function . we are applying some nonlinear function over our linear input networks their expressive function . and the relu takes the max of your input and zero . on the right we see what the graph might look like in tensorflow . we have variables for our b and w we have a placeholder we'll get to that soon with the x and nodes for each of the operations in our graph . so let's actually dissect those node types . variables are going to be stateful nodes which output their current value . 
there were also some ideas for projects that been stuck up on the projects page . so encourage people to look at that . now people have also asked us about assignment four . so we've also stuck up a description of assignment four . and so look at that if you're considering whether to do assignment four . so assignment four is gonna be doing question answering over the squad dataset and you can look in more details about that . so then there are two other things i wanted to mention . and we'll also put up messages on piazza etc about this . i mean the first one is that for assignment three we want people to have experience of doing things on a gpu . and we've arranged with microsoft azure to use their gpus for doing that and for people to use for the final project . and so we're trying to get that all organized at the moment . there's a limit to how many gpus we can have . so what we're gonna be doing for assignment three and for the final project is to allow teams of up to three . and really it's in our interest and the resource limit's interest if many people could be teamed up . so we'd like to encourage people to team up for assignment three . and so we've put up a google form for people to enter their teams . and we need people to be doing that in advance because we need to get that set up at least a week in advance so we can get the microsoft people to set up accounts for people so that people can use azure . so please think about groups for assignment three and then fill in the google form for that . and then the final thing is for next week we're gonna make some attempts of reorganizing the office hours and get some rooms for office hours so they can hopefully run more smoothly in the countdown towards the deadline for assignment two than they did for assignment one . so keep an eye out for that and expect that some of the office hour times and locations will be varying a bit compared to what they've been for the first three weeks . i couldn't really believe that these were gonna work and do anything useful . 