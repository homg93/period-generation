the bigger your group the more we expect from the project . and you have to also write out exactly what each person in the project has done . you can actually use any kind of open source library and code that you want . but if you just take kaldi which is a speech recognition system and you say i did speech recognition . and then really all you did was download the package and run it then that's not very impressive . so the more you use the more you also have to be careful and say exactly what parts you actually implemented . and in the code you also have to submit your code so that we understand what you've done and the results are real . so this year we do want some language in there . last year i was a little more open . it could be the language of music and so on now . so we've got to have some natural language in there yeah . but other than that that can be done quite easily so projects you might want to do . and if you have a more theoretically inclined project where you really are just faking out some clever way of doing a sarcastic ready to sent or using different kinds of optimization functions . about leading the class to then as long as you at least applied it in one experiment to a natural language processing data set that would still be a pretty cool project . so you can also apply it to genomics data and to text data if you wanna have a little bit of that flavor . but there is gonna be at least one experiment where you apply it to a text data set . all right so now let's walk through the different kinds of projects that you might wanna consider and what might be entailed in such project to give you an idea . unless there are any other questions around the organization of the projects deadlines and so on . so let's start with the kind of simplest and all the other ones are sort of bonuses on top of that simple kind of project . and this is actually i think generally good advice not just for a class project but in general how to apply a deep learning algorithm to any kind of problem whether in academia or in industry or elsewhere . 
so let's assume you want to apply an existing neural network to an existing task . so you want to be able to take a long document and summarize into a short paragraph . now step one after you define your task is you have to define your dataset . and that is actually sadly in many cases in both industry and in academia an incredibly time intensive problem . to that is you just search for there's some people who've worked in summarization before . the nice thing is if you use an existing data set for instance from the document understanding conference duc here then other people have already applied some algorithms to it you'll have some base lines you know what kind of metric or evaluation is reasonable versus close to random . and so on cuz sometimes that's not always obvious . we don't always us just accuracy for instance . so in that case using an existing academic data set gets rid of a lot of complexity . however it is really fun if you actually come up with your own kind of dataset too . so maybe you're really excited about food and you want to prowl yelp or use a yelp dataset for restaurant review or something like that . so however when you do decide to do that you definitely have to check in with your mentor or with chris and me and others . because i sadly have seen several projects in the last couple of years where people have this amazing idea . and then they spent 80% of the time on their project on a web crawler getting not blocked from ip addresses writing multiple ip addresses having multiple machines and crawling . sometimes it's just the document they were hoping to get and crawl it's just a 404 page . and then they realize html and they filter that . and before you know it it's like they have like three more days left to do any deep learning for nlp . and so it has happened before so don't fall into that trap . if you do decide to do that check with us and try to before the milestone deadline . for sure have the data set ready so you can actually do deep learning for nlp cuz sadly we just can't give you a good grade for a deep learning for nlp class if you spend 95% of your time writing a web crawler and explaining your data set . 
so in this case for instance you might say all right i want to use wikipedia . you can actually download sort of already pre-crawled versions of it . maybe you want to say my intro paragraph is the summary of the whole rest of the article . not completely crazy to make that assumption but really you can be creative in this part . you can try to connect it to your own research or your own job if your a [inaudible] student or just any kind of interest that you have . to time it's really fun nlp combine with language of music with natural language and so on . so you can be creative here and we kind of value a little bit of the creativity this is like a task of data set we had never seen before and you actually gain some interesting linguistic insights or something . that is the cool part of the project right . any questions around defining a data set . all right so then you wanna define your metric . for instance you have maybe let's say you did something simpler like restaurant star rating classification . this is a review and i want to classify if this a four star review or a one star review or a two or three . and now you may have a class distribution where this is one star this is two stars three and four and now the majority are three . maybe that you troll kind of funny and are three star reviews . so this is just like number and maybe 90% of the things you called are in the third class . and then you write your report you're super excited it was a new data set you did well you crawled it quickly . and then all you give us is an accuracy metric so accuracy is total correct divided by total . and now let's say your accuracy is 90% . it's 90% accurate 90% of the cases gives you the ride star rating . you're essentially overfit to your dataset and your evaluation metric was completely bogus . 
it's hard to know whether they basically could have implemented a one line algorithm that's just as accurate as yours which is just no matter what the input return three . so hard to give a good grade on that and it's a very tricky trap to fall into . i see it all the time in industry and for young researchers and so on . so in this case you should've used does anybody know what kind f1 that's right . so and we'll go through some of these as we go through the class but it's very important to define your metric well . now for something as tricky as summarization this isn't where you're really just like this is the class this is the final answer . you have to actually either extract or generate a longer sequence . and there are a lot of different bleu's n-gram overlap or rouge share which is a recall-oriented understudy for gisting evaluation which essentially is just a metric to weigh differently how many n-grams are correctly overlapping between a human generated summary . for instance your wikipedia paragraph number one and whatever output your algorithm gives . so rouge is the official metric for summarization in different sub-communities and nop have their own metrics and it's important that you know what you're optimizing . so the machine translation for instance you might use bleu scores bleu scores are essentially also a type of n-gram overlap metric . if you have a skewed data set you wanna use f1 . and in some cases you can just use accuracy . and this is generally useful even if you're in industry and later in life you always wanna know what metric you're optimizing . it's hard to do well if you don't know the metric that you're optimizing for both in life and deep learning projects . all right so let's say you defined your metric now you need to split your dataset . and it's also very important step and it's also something that you can easily make sort of honest mistakes . again in advantage of taking pre-existing academic dataset is that in many cases it's already pre-split but not always . and you don't wanna look at your 1 week before the deadline . so let's say you have downloaded a lot of different articles and now you basically have 100% of some articles you wanna summarize . 
and normal split would be take 80% for training you take 10% for your validation and your development . the validation split or the development split or dev split or various other terms . and 10% for your final test split . and so the final one you ideally get a sense of how your algorithm would work in real life on data you've never seen before you didn't try to chew on your model like how many layers should i use how wide should each layer be . you'll try a lot of these things we'll describe these in the future . but it's very important to correctly split and why do i make such a fuss about that . well there too you might make mistakes . so let's say you have unused text and let's say you crawled it in such a way there's a lot of mistakes that you can make if you try to predict the soft market for instance don't do that it doesn't work . but in many cases you might say or there some temporal sequence . and now you basically have all your dataset and the perfect thing to do is actually do it like this you take 80% of let's say month january to may or something and then your final test split is from november . that way you know there's no overlap . but maybe you made a mistake and you said well i crawled it this way but so as sample an article from here and one from here and one from here . and then the random sample goes to the 80% of my training data . and now the test data and the development data might actually have some overlap . cuz if you're depending on how you chose your dataset maybe the another article which just like a slight addition like some update to an emerging story . and now the summary is almost exact same but the input document just changed a tiny bit . and you have one article in your training set and another one in your test set . but the test set article is really only one extra paragraph on an emerging story and the rest is exactly the same . so now you have an overlap of your training and your testing data . and so in general if this is your training data and this should be your test data . 
it should be not overlapping at all . and whenever you do really well you run your first experiment and you get 90 f1 . and things look just too good to be true sadly in many cases they are and you made some mistake where maybe your test set had some overlap for instance with your training data . it's very important to be a little paranoid about that when your first couple of experiments turn out just to be too good to be true . that can mean either your training your task is too simple or you made a mistake in splitting and defining your dataset . all right any questions around defining a metric or your dataset yeah . so if we split it temporally wouldn't we learn a different distribution . that is correct we would learn a different distribution these are non-stationary . and that is kinda true for a lot of texts but if you ideally when you built a deep learning system for an lp you want it to built it so that it's robust . it's robust to sum such changes over time . and you wanna make sure that when you run it in a real world setting on something you've never seen before it's doing something it will still work . and this was the most realistic way to capture how well it would work in real life . would it be appropriate to run both experiments as in both where you subsample randomly and then you subsample temporally for your . you could do that and the intuitive thing that is likely going to happen is if you sample randomly from all over the place then you will probably do better than if you have this sort of more strict kind of split . but running an additional experiment will rarely ever get you points subtracted . you can always run more experiments and we're trying really hard to help you get computing infrastructure and cloud compute . so you don't feel restricted with the number of experiments you run . all right now number 5 establish a baseline . so you basically wanna implement the simplest model first . this could just be a very simple logistic regression on unigrams or bigrams . 
then compute your metrics on your train data and your development data so overfitting or underfitting . let's say your loss is very very low on training . you do very well on training but you don't do very well on testing then you're in an over fitting regime . if you do very well on training and well on testing you're done you're happy . but if your training loss can't be lower so you're not even doing well on your training that often means your so it's very important to compute both the metrics on your training and your development split . and then and this is something we value a lot in this class too . and it's something very important for you in both research and industries like you wanna analyze your errors carefully for that baseline . and if the metrics are amazing and there are no errors you're done . probably a problem was too easy and you may wanna restart unless it's really a valuable problem for the world . and then maybe you can just really describe it carefully and you're done too . it is very important to not just go in and add lots of bells and whistles that you'll learn about in the next couple of weeks in this class and create this monster of a model . you want to start with something simple sanity check make sure you didn't make mistakes in splitting your data . you have the right kind of metric . and in many cases it's a good indicator for how successful your final project is if you can get this baseline in the first half of the quarter . cuz that means you figured out a lot of these potential issues here . and you kind of have your right data set . you know what the metric is you know what you're optimizing and everything is good . so try to get to this point as quickly as possible . cuz that is also not as interesting and you can't really use that much knowledge from the class . and now you can implement some existing neural network model that we taught you in class . 
for instance this window-based model if your task is named entity recognition . you can compute your metric again on your train and dev set . hopefully you'll see some interesting patterns such as usually train neural nets is quite easy in a sense that we lower the loss very well . and then we might not generalize as well in the development set . and then you'll play around with regularization techniques . and don't worry if some of the stuff i'm saying now is kind of confusing . if you want to do this we'll walk you through that as we're mentoring you through the project . and that's why each project has to have an assigned mentor that we trust . all right then you analyze your very important be close to your data . you can't give too many examples usually ever . and this is kind of the minimum bar for this class . so if you've done this well and there's an interesting dataset then your project is kind of in a safe haven . once you have a metric and everything looks good we still want you to visualize the kind of data even if it's a known data set . we wanted you to visualize it collect summary statistics . it's always good to know the distribution if you have different kinds of classes . you want to again very important look at the errors that your model is making . cuz that can also give you intuitions of what kinds of patterns can your deep learning algorithm not capture . maybe you need to add a memory component or maybe you need to have longer temporal kind of dependencies and so on . those things you can only figure out if you're close to your data and you look at the errors that your baseline models are making . and then we want you to analyze also different hyperparameters . 
a lot of these models have lots of choices . did we add the sigmoid to that score or is the second layer 100 dimensional or 200 dimensional . should we use 50 dimensional word vectors or 1000 dimensional word vectors . there are a lot of choices that you make . and it's really good in your first couple projects to try more and and sometimes if you're running out of time and only so much so many experiments you can run we can help you and use our intuition to guide you . but it's best if you do that a little bit yourself . and once you've done all of that now you can try different model variants and you'll soon see a lot of these kinds of options . we'll talk through all of them in the class . so now another kind of class project is you actually wanna implement a new fancy model . those are the kinds of things that will put you into potentially writing an academic paper peer review and at a conference and so on . the tricky bit of that is you kinda have to do all the other steps that i just described first . and then on top of that you know the errors that you're making . and now you can gain some intuition of why the existing models are flawed . and you come up with your own new model . if you do that you really wanna be in close contact with your mentor and some researchers unless you're a researcher yourself and you earned your phd . but even then you should chat with us from the class . you want to basically try to set up an infrastructure such that you can iterate quickly . you're like maybe i should add this new layer type to this part of my model . you want to be able to quickly iterate and see if that helps or not . so it's important and actually require a fair amount of software engineering skills to set up efficient experimental frameworks that and again you want to start with simple models and then go to more and more complex ones . 
so for instance in summarization you might start with something super simple like just average all your word vectors in the paragraph . and then do a greedy search of generating one word at a time . or even greedily searching for just snippets from the existing article in wikipedia and you're just copying certain snippets over . and then stretch goal is something more advanced would be lets you actually generate that whole summary . and so here are a couple of project ideas . but again we'll post the whole list of them with potential mentors from the nop group and the vision group and various other groups inside stanford . sentiment is also a fun data set . you can look at this url here for one of the preexisting data sets that a lot of people have worked on . all right so next week we'll look at some fun and fundamental linguistic tasks like syntactic parsing . and then you'll learn tensorflow and have some great tools under your belt . again with cs224n natural language processing with deep learning . so you're in for a respite or a change of pace today . so for today's lecture what we're principally going to look at is syntax grammar and dependency parsing . so my hope today is to teach you in one lecture enough about dependency grammars and parsing that you'll all be able to do the main part of assignment 2 successfully . so quite a bit of the early part of the lecture is giving a bit of background about syntax and dependency grammar . and then it's time to talk about a particular kind of dependency grammar transition-based also dependency parsing transition-based dependency parsing . and then it's probably only in the last kind of 15 minutes or so of the lecture that we'll then get back into specifically neural network content . talking about a dependency parser that danqi and i wrote a couple of years ago . okay so for general reminders i hope you're all really aware that assignment 1 is due today . and i guess by this stage you've either made good progress or you haven't . 
but to give my good housekeeping reminders i mean it seems like every year there are people that sort of blow lots of late days on the first assignment for no really good reason . and that isn't such a clever strategy [laugh] . so hopefully [laugh] you are well along with the assignment and can aim to hand it in before it gets to the weekend . okay then secondly today is also the day that the new assignment comes out . till the start of next week but we've got it up ready to go . and so that'll involve a couple of new things and in some respects probably for much of it you might not want to start it until after next tuesday's lecture . so two big things will be different for that assignment . big thing number one is we're gonna do assignment number two using tensorflow . and that's the reason why quite apart from exhaustion from assignment one why you probably you don't wanna start it on the weekend is because on tuesday tuesday's lecture's gonna be an introduction to tensorflow . so you'll really be more qualified then to start it after that . and then the other big different thing in assignment two is we get into natural language processing content . in particular you guys are going to build neural dependency parsers and the hope is that you can learn about everything that you need to know to do that today . the readings on the website if you don't get quite everything straight from me . we're going to sort of post hopefully tomorrow or on the weekend a kind of an outline of what's in assignment four so you can have sort of a more informed meaningful choice between whether you want to do assignment four or the area of assignment four if you do it is going to be question answering over the squad dataset . but we've got kind of a page and a half description to explain what that means so you can look out for that . but if you are interested in we'll encourage people to come and meet with one of the final project mentors or find some other well qualified person around here to be a final project mentor . so what we're wanting is that sort of everybody has met with their final project mentor before putting in an abstract . and that means it'd be really great for people to get started doing that as soon as possible . i know some of you have already talked to various of us . for me personally i've got final from 1 to 3 pm so i hope some people will come by for those . 
and again sort of as richard mentioned not everybody can possible have richard or me as the final project mentor . and besides there's some really big advantages of having some of the phd student tas as final project mentors . cuz really for things like spending time hacking on tensorflow they get to do it much more than i do . and so danqi kevin ignacio arun that they've had tons of experience doing nlp research using deep learning . and so that they'd also be great mentors and look them up for their final project advice . the final thing i just want to touch on is we clearly had a lot of problems i realize at keeping up and coping with people in office hours and queue status has just i'm sorry that that's been kind of difficult . i mean honestly we are trying to work and work out ways that we can do this better and we're thinking of sort of unveiling a few changes for doing things for the second assignment . if any of you peoples have any better advice as to how things could be organized so that they could work better feel free to send a message on piazza with suggestions of ways of doing it . i guess yesterday i ran down percy liang and said percy percy how do you do it for cs221 . do you have some big secrets to do this better . but unfortunately i seem to come away with no big secrets cuz he sort of said: "we use queue status and we use the huang basement" what else are you meant to do . so i'm still looking for that divine insight [laugh] that will tell me how to get this so if you've got any good ideas feel free to share . but we'll try to get this as much better under control as we can for the following weeks . okay any questions or should i just go into the meat of things . all right so what we're going to want to do today is work out how to put structures over sentences in some human language . all the examples i'm going to show is for english but in principle the same techniques you can apply for any language where these structures are going to sort of reveal how the sentence is made up . so that the idea is that sentences and parts of sentences have some kind of structure and there are sort of regular ways that people put sentences together . so we can sort of start off with very simple things that aren't yet sentences like "the cat" and "a dog" and they seem to kind of have a bit of structure . we have an article or what linguists often call a determiner that's followed by a noun . and then well for those kind of phrases which get called noun you can kind of make them bigger and there are sort of rules for how you can do that . 
so you can put adjectives in between the article and the noun . you can say the large dog or a barking dog or a cuddly dog and things like that . and well you can put things like what i call prepositional phrases after the noun so you can get things like "a large dog in a crate" or something like that . and so traditionally what linguists and natural language processors have wanted to do is describe the structure of human languages . and they're effectively two key tools that people have used to do this and one of these key tools and i think in general the only one you have seen a fraction of is to use what in computer science terms what is most commonly referred to as context free grammars which are often referred to by linguists as phrase structure grammars . and is then referred to as the notion of constituency and so for that what we are doing is writing these context free grammar rules and the least if you are standford undergrad or something like that . i know that way back in 103 you spent a whole lecture learning about context-free grammars and their rules . so i could start writing some rules that might start off saying a noun phrase and go to a determiner or a noun . then i realized that noun phrases would get a bit more complicated . and so i came up with this new rule that says- noun phrase goes to terminal optional adject of noun and then optional prepositional phrase wherefore prepositional phrase that's a preposition followed by another noun phrase . because i can say a crate or a large crate . or a large crate by the door . even further and i could say you know a large barking dog by the door in a crate . so then i noticed wow i can put in multiple adjectives there and i can stick on multiple prepositional phrases so i'm using that star the kinda clingy star that you also see see in regular expressions to say that you can have zero or any number of these . and then i can start making a bigger thing like talk to the cuddly dog . and well now i've got a verb followed by a prepositional phrase . and so i can sort of build up a constituency grammar . so that's one way of organizing the structure of sentences and you know in 20th dragging into 21st century america this has been i mean it's what you see mainly in your intro cs class when you get taught about regular languages and context free languages and context sensitive languages . hierarchy where noam chomsky did not actually invent the chomsky hierarchy to torture cs under grads with formal content to fill the scs 103 class . the original purpose of the chomsky hierarchy was actually to understand the complexity of human languages and to make arguments about their complexity . 
sorry it's also dominated sorta linguistics in america in the last 50 years through the work of noam chomsky . but if you look more broadly than that this isn't actually the dominate form of syntactic description that is being used for understanding of the structure of sentences . so there is this other alternative view of linguistic structure which is referred to as dependency structure and what your doing with dependency structure . is that you're describing the structure of a sentence by taking each word and saying what it's a dependent on . so if it's a word that kind of modifies or is an argument of another word that you're saying it's a dependent of that word . so barking dog barking is a dependent of dog because it's of a modifier of it . large barking dog large is a modifier of dog as well so it's a dependent of it . and dog by the door so the by the door is somehow a dependent of dog . and we're putting a dependency between words and we normally indicate those dependencies with arrows . structures over sentences that say how they're represented as well . and when right in the first class i gave examples of ambiguous sentences . a lot of those ambiguous sentences we can think about in terms of dependencies . so do you remember this one scientists study whales from space . and well why is it an ambiguous headline . well it's ambiguous because there's sort of two possibilities . so in either case there's the main verb study . and it's the scientist that's studying that's an argument of study the subject . and it's the whales that are being studied so that's an argument of study . but the big difference is then what are you doing with the from space . you saying that it's modifying study or are you saying it's modifying whales . 
quickly read the headline it sounds like it's the bottom one right . but [laugh] what the article was meant to be about was really that they were being able to use satellites to track the movements of whales . and so it's the first one where the from space is modifying . and so thinking about ambiguities of sentences can then be thought about many of them in terms of these dependency structures as to what's modifying what . and this is just a really common thing in natural language because these kind of questions of what modifies what really dominate a lot of questions of interpretation . so here's the kind of sentence you find when you're reading the wall street journal every morning . the board approved its acquisition by royal trustco limited of toronto for $27 a share at its monthly meeting . and as i've hopefully indicated by the square brackets if you look at the structure of this sentence it sort of starts off as subject verb object . the board approved its acquisition and then everything after that is a whole sequence of prepositional phrases . by royal trustco ltd of toronto for $27 a share at its monthly meeting . and well so then there's the question of what's everyone modifying . so the acquisition is by by royal trustco ltd is modifying the thing that immediately precedes that . and of toronto is modifying the company royal trustco limited so that's modifying the thing that comes immediately preceeding it . so you might think this is easy everything just modifies the thing that's coming immediately before it . so what's for $27 a share modifying . yeah so that's modifying the acquisition so then we're jumping back a few candidates and saying is modifying acquisition and then actually at it's monthly meeting . that wasn't the toronto the royal trustco ltd or the acquisition that that was when the approval was happening so that jumps all the way back up to the top . so in general the situation is that if you've got some stuff like a verb and getting these prepositional phrases . well the prepositional phrase can be modifying either this noun phrase or the verb . but then when you get to the second prepositional phrase . 
well there was another noun phrase inside this prepositional phrase . it can be modifying this noun phrase that noun phrase or the verb phrase . and then we get to another one . and you don't get sort of a completely free choice cuz you do get a nesting constraint . so once i've had for $27 a share referring back to the acquisition the next prepositional phrase has to in general refer to either the acquisition or approved . i say in general because and i'll actually talk about that later . but most of the time in english it's true . you have to sort of refer to the same one or further back so you get a nesting relationship . but i mean even if you obey that nesting relationship the result is that you get an exponential number of ambiguities in a sentence based on in the number of prepositional phrases you stick on the end of the sentence . and so the series of the exponential series you get of these catalan numbers . and so catalan numbers actually show up in a lot of places in that is somehow sort of similar if you're putting these constraints in you get catalan series . so are any of you doing cs228 . yeah so another place the catalan series turns up is that when you've got a vector graph and you're triangulating it the number of ways that you can triangulate your vector graph is also giving you catalan numbers . okay so human languages get very ambiguous . and we can hope to describe them on the basis of sort of looking at these dependencies . the other important concept i wanted to introduce at this point is this idea of full linguistics having annotated data in the form of treebanks . this is probably a little bit small to see exactly . but what this is is we've got sentences . these are actually sentences that come off yahoo answers . and what's happened is human beings have sat around and drawn in the syntactic structures of these sentences as dependency graphs and those things we refer to as treebanks . 
and so a really interesting thing that's happened starting around 1990 is that people have devoted a lot of resources to building up these kind of annotated treebanks and various other kinds of annotated linguistic resources that we'll talk about later . now in some sense from the viewpoint of sort of modern machine learning in 2017 that's completely unsurprising because all the time what we do is say we want labelled data so we can take our supervised classifier and chug on it and get good results . but in many ways it was kind of a surprising thing that happened which is sort of different to the whole of the rest of history right . cuz for the whole of the rest of the history it was back in this space of well to describe linguistic structure what we should be doing is writing grammar rules that describe what happens in linguistic structure . where here we're no longer even attempting to write grammar rules . we're just saying give us some sentences . and i'm gonna diagram these sentences and show you what their structure is . and tomorrow give me a bunch more and i'll diagram them for you as well . and if you think about it in a way that initially seems kind of a crazy thing to do cuz it seems like just putting structures over sentences one by one seems really really inefficient and slow . whereas if you're writing a grammar you're writing this thing that generalizes right . the whole point of grammar is that you're gonna write this one small finite grammar . and it describes an infinite number of sentences . and so surely that's a big labor saving effort . but slightly surprisingly but maybe it makes sense in terms of what's happened in machine learning that it's just turned out to be kind of super successful this building of explicit annotated treebanks . and it ends up giving us a lot of things . and i sort of mention a few of their advantages here . first it gives you a reusability of labor . but the problem of human beings handwriting grammars is that they tend to in practice be almost unreusable because everybody does it differently and has their idea of the grammar . and people spend years working on one and no one else ever uses it . where effectively these treebanks have been a really reusable tool that lots of people have then built on top of to build all kinds of natural language processing tools of part of speech taggers and parsers and things like that . 
they've also turned out to be a really useful resource actually for linguists because they give a kind of real languages are spoken complete with syntactic analyses that you can do all kinds of quantitative linguistics on top of . it's genuine data that's broad coverage when people just work with their intuitions as to what are the grammar rules of english . and so this is actually a better way to find out all of the things that actually happened . for anything that's sort of probabilistic or machine learning it gives some sort of not only what's possible but how frequent it is and what other things it tends to co-occur with and all that kind of distributional information that's super important . and crucially crucially crucially and we'll use this for assignment two it's also great because it gives you a way to evaluate any system that you built because this gives us what we treat as ground truth gold standard data . and then we can evaluate any tool on how good it is at reproducing those . and what i wanted to do now is sort of go through a bit more carefully for sort of 15 minutes what are dependency grammars and dependency structure . so we've sort of got that straight . i guess i've maybe failed to say yeah . i mentioned there was this sort of constituency context-free grammar viewpoint and the dependency grammar viewpoint . and what we're doing for assignment two is all dependencies . we will get back to some notions of constituency and phrase structure . you'll see those coming back in later classes in a few weeks' time . but this is what we're going to be doing today . and that's not a completely random choice . it's turned out that unlike what's happened in linguistics in most of the last 50 years in the last decade in natural language processing it's essentially been swept by the use of dependency grammars that people have found dependency grammars just a really suitable framework on which to build semantic representations to get out the kind of understanding of language that they'd like to get out easily . they enable the building of very fast efficient parsers as i'll explain later today . and so in the last sort of ten years you've just sort of seen this huge sea change in natural language processing . whereas if you pick up a conference volume around the 1990s it was basically all phrase structure grammars and one or two papers on dependency grammars . and if you pick up a volume now what you'll find out is that of the papers they're using syntactic representations dependency representations . 
phrase structure what's the phrase structure grammar that's exactly the same as the context-free grammar when a linguist is speaking . okay so what does a dependency syntax say . so the idea of dependency syntax is to say that the sort of model of syntax is we have relationships between lexical items words and only between lexical items . they're binary asymmetric relations which means we draw arrows . so the whole there is a dependency analysis of bills on ports and senator brownback republican of kansas . okay so that's a start normally hen we do dependency parsing we do a little bit more than that . so typically we type the dependencies by giving them a name for some grammatical relationship . so i'm calling this the subject and it's actually a passive subject . and then this is an auxiliary modifier republican of kansas is an appositional phrase that's coming off of brownback . and so we use this kind of typed dependency grammars . and interestingly i'm not going to go through it but there's sort of some interesting math that if you just have this although it's notationally very different from context-free grammar to a restricted kind of context-free grammar with one addition . but things become sort of a bit more different once you put in a typing of the dependency labels where i wont go into that in great detail right . so a substantive theory of dependency grammar for a language we're then having to make some decisions . so what we're gonna do is when we between two things and i'll just mention a bit more terminology . so we have an arrow and its got what we called the tail end of the arrow i guess . and the word up here is sort of the head . so bills is an argument of submitted were is an auxiliary modifier of submitted . and so this word here is normally referred to as the head or the governor or the superior or sometimes even the regent . and then the word at the other end of the arrow the pointy bit i'll refer to as the dependent but other words that you can sometimes see are modifier inferior subordinate . some people who do dependency grammar really get into these classist notions of superiors and inferiors but i'll go with heads and dependents . 
okay so the idea is you have a head of a clause and then the arguments of the dependence . and then when you have a phrase like by senator brownback republican of texas . being taken as brownback and then it's got words beneath it . and so one of the main parts of dependency grammars at the end of the day as to which words are heads and which words are then the dependents of the heads of any particular structure . so in these diagrams i'm showing you here the ones i showed you back a few pages here is analysis according to universal dependencies . so universal dependencies is a new tree banking effort which i've actually been very strongly involved in . that sort of started a couple of years ago and there are pointers in both earlier in the slides and on the website if you wanna go off and learn a lot about universal dependencies . i mean it's sort of an ambitious attempt to try and have a common dependency representation that works over a ton of languages . i could prattle on about it for ages and if by some off chance there's time at the end of the class i could . but probably there won't be so i won't actually tell you a lot about that now . but i will just mention one thing that probably you'll notice very quickly . and we're also going to be using this representation in the assignment that's being given out today the analysis of universal dependencies treats prepositions sort of differently to what you might have seen else where . if you've seen any many accounts of english grammar or heard references in some english classroom to have prepositions having objects . in universal dependencies prepositions don't have any dependents . of like they were case markers if you know any language like german or latin or hindi or something that has cases . so that the by is sort of treated as if it were a case marker of brownback . so this sort of a bleak modifier of by senator brownback . brownback here as the head with the preposition as sort of like a case marking dependent of by . and that was sort of done to get more parallelism across different languages of the world . other properties of old dependencies normally dependencies form a tree . 
so there are formal properties that goes along with that . that means that they've got a single-head they're acyclic and they're connected . so there is a sort of graph theoretic properties . yeah i sort of mentioned that really dependencies have dominated most of the world . the famous first linguist was panini who wrote his grammar of sanskrit really most of the work that panini did was kind of on sound systems and make ups of words phonology and morphology when we mentioned linguistic levels in the first class . and he only did a little bit of work on the structure of sentences . but the notation that he used for structure of sentences was essentially a dependency grammar of having word relationships being marked as dependencies . yeah so the question is well compare cfgs and pcfgs and do they dependency grammars look strongly lexicalized they're between words and does that makes it harder to generalize . i honestly feel i just can't do justice to that question right now if i'm gonna get through the rest of the lecture . but i will make two comments so i mean there's certainly the natural way to think of dependency grammars they're strongly lexicalized you're drawing relationships between words . whereas the simplest way of thinking of context-free grammars is you've got these rules in terms of categories like . noun phrase goes to determiner noun optional prepositional phrase . and so that is a big difference . but it kind of goes both ways . so normally when actually natural language processing people wanna work with context-free grammars they frequently lexicalize them so they can do more precise probabilistic prediction and vice versa . if you want to do generalization and dependency grammar you can still use at least notions of parts of speech to give you a level of generalization as more like categories . but nevertheless the kind of natural ways of sort of turning them into probabilities and machine learning models are quite different . though on the other hand there's sort of some results or sort of relationships between them . but i would think i'd better not go on a huge digression . that means to rather than just have categories like noun phrase to have categories like a noun phrase headed by dog and so it's lexicalized . 
let's leave this for the moment though please okay . okay so that's panini and there's a whole big history right . so essentially for latin grammarians what they did for the syntax of latin again not very developed . they mainly did morphology but it was essentially a dependency kind of analysis that was given . there was sort of a flowering of arabic grammarians in the first millennium and they essentially had a dependency grammar . i mean by contrast i mean really kind of context free grammars and constituency grammar only got invented almost in the second half of the 20th century . i mean it wasn't actually chomsky that originally invented them there was a little bit of earlier work in britain but only kind of a decade before . so there was this french linguist lucien tesniere he is often referred to as the father he's got a book from 1959 . dependency grammars have been very popular and more sorta free word order languages cuz notions sort of like context-free languages like english that have very fixed word order but a lot of other languages of the world have much freer word order . and that's often more naturally described with dependency grammars . interestingly one of the very first natural language parsers developed in the us was also a dependency parser . so david hays was one of the first us computational linguists . and one of the founders of the association for computational linguistics which is our main kind of academic association where we publish our conference papers etc . and he actually built in 1962 a dependency parser for english . okay so a lot of history of dependency grammar . so couple of other fine points to note about the notation . people aren't always consistent in which way they draw the arrows . i'm always gonna draw the arrows so they point go from a head to a dependent which is the direction which tesniere drew them . but there are some other people who draw the arrows the other way around . so they point from the dependent to the head . 
and so you just need to look and see what people are doing . the other thing that's very commonly done is you stick this pseudo-word wall or some other name like that and that kind of makes the math and formalism easy because then every sentence starts with root and something is a dependent of root . or turned around the other way if you think of what parsing a dependency grammar means is for every word in the sentence you're going to say what is it a dependent of because if you do that you're done . you've got the dependency structure of the sentence . and what you're gonna want to say is well it's either gonna be a dependent of some other word in the sentence or it's gonna be a dependent of the pseudo-word root which is meaning it's the head of the entire sentence . specifics of dependency parsing but the kind of thing that you should think about is well how could we decide which words are dependent on what . and there are certain various information sources that we can think about . so yeah it's sort of totally natural with the dependency representation to just think about word relationships . and that's great cuz that'll fit super well with what we've done already in distributed word representations . so actually doing things this way just fits well with a couple of tools we already know how to use . we'll want to say well discussion of issues is that a reasonable attachment as lexical dependency . and that's a lot of the information that we'll actually use but information that we'd also like to use . dependency distance so sometimes there are dependency relationships and sentences between words that is 20 words apart when you got some big long sentence and you're referring that back to some previous clause but it's kind of uncommon . most of dependencies are pretty short distance so you want to prefer that . many dependencies don't sort of so if you have the kind of dependencies that occur inside noun phrases like adjective modifier they're not gonna cross over a verb . it's unusual for many kinds of dependencies to cross over a punctuation so it's very rare to have a punctuation between a verb and a subject and things like that . so looking at the intervening material gives you some clues . and the final source of information is sort of thinking about heads and thinking how likely they are to have to dependence in what number and on what sides . so the kind of information there is right a word like the is basically not likely to have any dependents at all anywhere . so you'd be surprised if it did . 
words like nouns can have dependents and they can have quite a few dependents but they're likely to have some kinds like determiners and adjectives on the left other kinds like prepositional phrases on the right verbs tend to have a lot of dependence . so different kinds of words have different kinds of patterns of dependence and so there's some information there we could hope to gather . okay yeah i guess i've already said the first point . in principle it's kind of really easy . so we're just gonna take every make a decision as to what word or root this word is a dependent of . and we do that with a few constraints . so normally we require that only one word can be a dependent of root and we're not going to allow any cycles . and if we do both of those things we're guaranteeing that we make the dependencies of a tree . and normally we want to make out dependencies a tree . and there's one other property i then wanted to mention that if you draw your dependencies as i have here so all the dependencies been drawn as loops above the words . it's different if you're allowed to put some of them below the words . whether you can draw them like this . so that they have that kind of nice none of them cross each other . or whether like these two that i've got here where they necessarily cross each other and i couldn't avoid them crossing each other . and what you'll find is in most languages certainly english the vast majority of dependency relationships have a nesting structure relative to the linear order . and if a dependency tree is fully nesting it's referred to as that you can lay it out in this plane and have sort of a nesting relationship . but there are few structures in english where you'd get things that aren't nested and yet crossing . and this sentence is a natural example of one . so i'll give a talk tomorrow on bootstrapping . so something that you can do with noun modifiers especially if they're kind of long words like bootstrapping or techniques of bootstrapping is you can sort of move them towards the end of the sentence right . 
i could have said i'll give a talk on bootstrapping tomorrow . but it sounds pretty natural to say i'll give a talk tomorrow on bootstrapping . but this on bootstrapping is still modifying the talk . and so that's referred to by linguists as right extraposition . and so when you get that kind of rightward movement of phrases you then end up with these crossing lines . and that gives you what's referred to as a non-projective dependency tree . so importantly it is still a tree if you sort of ignore the constraints of linear order and you're just drawing it out . there's a graph in theoretical computer science right it's still a tree . it's only when you consider this extra thing of the linear order of the words that you're then forced to have the lines across . and so that property which you don't actually normally see mentioned in theoretical computer science discussions of graphs is then this property that's referred to projectivity . recover the order of the words from a dependency tree . so given how i've defined dependency trees the strict answer is no . they aren't giving you the order at all . now in practice people write down the words of a sentence in order and have these crossing brackets right crossing arrows when they're non-projective . and obviously it's a real thing about languages that they have linear order . but as i've defined dependency structures you can't actually recover okay one more slide before we get to the intermission . yeah so in the second half of the class i'm gonna tell you about a method of dependency parsing . i just wanted to say very quickly there are a whole bunch about doing dependency parsing . so one very prominent way of doing dependency parsing is using dynamic programming methods which is normally what people have used for constituency grammars . a second way of doing it is to use graph algorithms . 
so a common way of doing dependency parsing you're using mst algorithms minimum spanning tree algorithms . and that's actually a very successful way of doing it . you can view it as kind of a constraint satisfaction problem . but the way we're gonna look at it is this fourth way which is these days most commonly called transition based-parsing though when it was first introduced it was quite often called deterministic dependency parsing . and the idea of this is that we're kind of greedily going to decide which word each word is a dependent of guided by having a machine learning classifier . so one way of thinking about this is so far in this class we only have two hammers . one hammer we have is word vectors and you can do a lot with word vectors . and the other hammer we have is how to build a classifier as a feedforward neural network with a softmax on top so it classifies between two various classes . and it turns out that if those are your two hammers you can do dependency parsing this way and it works really well . and so therefore that's a great approach for using in assignment two . and it's not just a great approach for assignment two . actually method four is the dominant way these days of doing dependency parsing because it has extremely good properties of scalability . that greedy word there is a way of saying this is a linear time algorithm which none of the other methods are . so in the modern world of web-scale parsing it's sort of become most people's favorite method . so i'll say more about that very soon . but before we get to that we have ajay doing our research spotlight with one last look back at word vectors . okay awesome so let's take a break from dependency parsing and talk about something we should know a lot about word embeddings . so for today's research highlight we're gonna be talking about a paper titled improving distributional similarity with lessons learned from word embeddings . and it's authored by levy et al . so in class we've learned two major paradigms for generating word vectors . 
we've learned count-based distributional models which essentially utilize a co-occurrence matrix to produce your word vectors . and we've learned svd which is singular value decomposition . and we haven't really talked about ppmi . but in effect it still uses that co-occurrence matrix to produce sparse vector encodings for words . we've also learned neural network-based models which you all should have lots of experience with now . and specifically we've talked about skip-gram negative sampling as well as cbow methods . and glove is also a neural network-based model . and the conventional wisdom is that neural network-based models are superior to count-based models . however levy et al proposed that hyperparameters and system design choices are more important not the embedding algorithms themselves . and so essentially what they do in their paper is propose a slew of hyperparameters that when implemented and tuned over the count-based distributional models pretty much approach the performance of neural network-based models to the point where there's no consistent better choice across the different tasks that they tried . and a lot of these inspired by these neural network-based models such as skip-gram . so if you recall which you all should be very familiar with this we have two hyperparameters in skip-gram . we have the number of negative samples that we're sampling as well as the unigram distributions smoothing exponent which we fixed at 3 over 4 . but it can be thought of as more of a system design choice . and these can also be transferred over to the account based variants . and i'll go over those very quickly . so the single hyper parameter that levy et al . impact in performance was context distribution smoothing which is analogous to the unigram distribution smoothing constant 3 over 4 here . and in effect they both achieved the same goal which is to sort of smooth out your distribution such that you're penalizing rare words . which interestingly enough the optimal alpha they found was exactly 3 over 4 which is the same as the skip-gram unigram smoothing exponent . 
they were able to increase performance by an average of three points across tasks on average which is pretty interesting . and they also propose shifted pmi which i'm not gonna get into the details of this . but this is analogous to the negative sampling choosing the number of negative samples in skip-gram . and they've also proposed a total of eight hyperparameters in total . and we've described one of them which is the context distribution smoothing . and this is a lot of data and if you're confused that's actually the conclusion that i want you to arrive at because clearly there's no trend here . so what the authors did was take all four methods tried three different windows and then test all the models across a different task . and those are split up into word similarity and analogy task . and all of these methods are tuned to find the best hyperparameters to optimize for the performance . and the best models are bolded and as you can see there's no consistent best model . so in effect they're challenging the popular convention that neural network-based models are superior to the count-based models . however there's a few things to note here . number one adding hyperparameters is never a great thing because now you have to train those hyperparameters which takes time . number two we still have the issues with count-based distributional models specifically with respect to the computational as well as performing svd . so the key takeaways here is that the paper challenges the conventional wisdom that neutral network-based models are in fact superior to count-based models . number two while model design is important hyperparameters are also key for achieving good results . so this implies specifically to doing a project instead of assignment four . you might implement the model but that might only take you half way there . some models to find your optimal hyperparameters might take days or even weeks to find . and finally my personal interest within ml is in deep representation learning . 
and this paper specifically excites displays that there's still lots of work to be done in the field . and so the final takeaway is challenge the status quo . okay and so now we're back to learning about how to build a transition based dependency parser . so maybe in 103 or compilers class formal languages class there's this notion of shift reduced parsing . how many of you have seen shift reduced parsing somewhere . they just don't teach formal languages the way they used to in the 1960s in computer science anymore . okay well i won't assume that you've all seen that before . okay essentially what we're going to have is i'll just skip these two slides and go straight to the pictures . because they will be much more understandable . mention the picture on this page that's a picture of joakim nivre . so joakim nivre is a computational linguist in uppsala sweden who pioneered this approach of transition based dependency parsing . he's one of my favorite computational linguists . i mean he was also an example going along with what ajay said of sort of doing something unpopular and out of the mainstream and proving that you can get it to work well . so at an age when everyone else was trying to build sort of fancy dynamic program parsers joakim said nono what i'm gonna do is i'm just gonna take each successive word and have a straight classifier that says what to do with that . and go onto the next word completely greedy cuz maybe that's kinda like what humans do with incremental sentence processing and i'm gonna see how well i can make that work . and it turned out you can make it work really well . so and then sort of transition based parsing has grown to this sort of really widespread dominant way of doing parsing . so it's good to find something different to do if everyone else is doing something it's good to think of something else that might be promising that you got an idea from . and i also like joakim because he's actually another person that's really interested in human languages and linguistics which actually seems to be a minority of the field of natural language processing when it comes down to it . okay so here's some more formalism but i'll skip that as well and i'll give you the idea of what an arc-standard transition-based dependency parser does . 
so what we're gonna do is were going to have a sentence we want to parse i ate fish and so we've got some rules for parsing which is the transition scheme which is written so small you can't possibly read it . so we have two things we have a stack and cartouche around that . and we start off parsing any sentence by putting it on the stack one thing which is our root symbol . okay and the stack has its top towards the right . and then we have this other thing which gets referred to as the buffer . and the buffer is the orange cartouche and the buffer is the sentence that we've got to deal with . and so the thing that we regard as the top of the buffer is the thing to the left off excessive words right . so the top of both of them is sort of at that intersection point between them . okay and so to do parsing under this transition-based scheme there are three operations that we can perform . we can perform they're called shift left-arc and right-arc . so the first one that we're gonna do is shift operation . all we do when we do a shift is we take the word that's on the top of the buffer and put it on the top of the stack . and then we can shift again and we take the word that's on the top of the buffer and put it on the top of the stack . remember the stack the top is to the right . the buffer the top is to the left . okay so there are two other operations left in this arc-standard transition scheme which were left arc and right arc . so what left arc and right arc are gonna do is we're going to make attachment decisions by adding a word as the dependent either to the left or to the right . okay so what we do for left arc is on the stack we say that the second to the top of the stack is a dependent of the thing that's the top of the stack . so i is a dependent of ate and we remove that second top thing from the stack . and so now we've got a stack with just [root] ate on it . 
but we collect up our decisions so we've made a decision that i is a dependent of ate and that's that said a that i am writing in small print off to the right . okay so we still had our buffer with fish on it . so the next thing we're gonna do is shift again and put fish on the stack . and so at that point our buffer is empty we've moved every word on to the stack in our sentence . and we have on it root ate fish okay . so then the third operation we have is right arc and right arc is just the opposite of left arc . so for the right arc operation we say the thing that's on the top of the stack should be made a dependent of the thing that's second to top on the stack . we remove it from the stack and we add an arc saying that . so we right arc so we say fish is a dependent of ate and we remove fish from the stack . we add a new dependency saying that fish is a dependent of ate . and then we right arc one more time so the dependent of the root . so we pop it off the stack and we're just left with root on the stack and we've got one new dependency saying that ate is a dependent of root . so at this point and i'll just mention right in reality there's i left out writing the buffer in a few of those examples there just because it was getting pretty crowded on the slide . but really the buffer is always there right it's not that the buffer disappeared and came back again it's just i didn't always draw it . so but in our end state we've got one thing on the stack and we've got nothing in the buffer . and that's the good state that we want to be in if we finish parsing our sentence correctly . and so we say okay we're in the finished state and we stop . and so that is almost all there is to arc-standard transition based parsing . right so we have a stack and our buffer and then on the side we have a set of dependency arcs a which starts off empty and we add things to . and we have this sort of set of actions which are kind of legal moves that we can make for parsing and so this was how things are . 
so we have a start condition root on the stack buffer is the sentence no arcs . we have the three operations that we can perform . here i've tried to write them out formally so the sort of vertical bar is sort of appends an element to a list operation . so this is sort of having wi as the first word on the buffer it's written the opposite way around for the stack because the head's on the other side . and so we can sort of do this shift operation of moving a word onto the stack and these two arc operations add a new dependency . and then removing one word from the stack and our ending condition is one be the root and an empty buffer . and so that's sort of the formal operations . so the idea of transition based parsing is that you have this sort of set of legal moves to parse a sentence in sort of a shift reduced way . i mean this one i referred to as arc-standard cuz it turns out there are different ways you can define your sets of dependencies . the one we'll use for the assignment and one that works pretty well . so i've told you the whole thing except for one thing which is this just gives you a set of possible moves . it doesn't say which move you should do when . and so that's the remaining thing that's left . and i have a slide on that . okay so the only thing that's left is to say gee at any point in time like we were here at any point in time you're in some configuration right . you've got certain things on there certain things in your buffer you have some set of arcs that you've already made . and which one of these operations do i do next . that nivre proposed is well what we should do is just build a machine learning classifier . since we have a tree bank with parses of sentences we can use those parses of sentences to see which sequence of operations would give the correct parse of a sentence . i am not actually gonna go through that right now . 
you can sort of work out deterministically the sequence of shifts and reducers that you need to get that structure . and it's indeed unique right that for each tree structure there's a sequence of shifts and left arcs and right arcs that will give you the right structure . so you take the tree you read off the correct operation sequence and therefore you've got a supervised classification problem . say in this scenario what you should do next is you should shift and so you're then building a classified to try to predict that . so in the early work that started off with nivre and others in the mid 2000s this was being done with conventional so maybe an svm maybe a perceptron a kind of maxent / soft max classifiers various things but sort of some classified that you're gonna use . so if you're just deciding between the operations shift left arc right arc you have got at most three choices . occasionally you have less because if there's nothing left on the buffer you can't shift anymore so then you'd only have two choices left maybe . but something i didn't mention when i was showing this is when i added to the arc set i didn't only say that fish is an object of ate . i said the dependency is the object of ate . and so if you want to include dependency labels the standard way of doing that is you just have sub types of left arc and right arc . if you have a approximately 40 different dependency labels . as we will in assignment two and in universal dependencies . you actually end up with the space of 81 way classification . names like left arc as an object . or left arc as an adjectival modifier . for the assignment you don't have to do that . for the assignment we're just doing un-type dependency trees . which sort of makes it a bit more scalable and easy for you guys . so it's only sort of a three way in most real applications it's really handy to have those dependency labels . and then what do we use as features . 
well in the traditional model you sort of looked at all the words around you . you saw what word was on the top of the stack . what was the part of speech of that word . what was the first word in the buffer . maybe it's good to look at the thing and what word and part of speech it is . so you're looking at a bunch of words . you're looking at some attributes of those words such as their part of speech . and that was giving you a bunch of features . which are the same kind of classic categorical sparse features of and people were building classifiers over that . so yeah the question is are most treebanks annotated with part of speech . we've barely talked about part of speech so far things like living things nouns and verbs . so the simplest way of doing dependency parsing as you're first writing a part of speech tag it or assign parts of speech to words . and then you're doing the syntactic structure of dependency parsing over a sequence of word part of speech tag pairs . though there has been other work part of speech tag prediction at the same time . which actually has some advantages because you can kind of explore . since the two things are associated you can get some advantages from doing it jointly . okay on the simplest possible model which was what nivre started to explore . you just took the next word ran your classifier . and said that's the object of the verb what's the next word . and you went along and just made these decisions . 
now you could obviously think gee maybe if i did some more searching and explore different alternatives i could do a bit better . and the answer is yes you can . so there's a lot of work in dependency parsing . which uses various forms of beam search where you explore different alternatives . and if you do that it gets a ton slower . and gets a teeny bit better in terms of your performance results . okay but especially if you start from the greediest end or you have a small beam . the secret of this type of parsing is it gives you extremely fast linear time parsing . because you're just going through your corpus no matter how big . so when people like prominent search engines in suburbs south of us want to parse the entire content of the web . they use a parser like this because it goes super fast . and so what was shown was these kind of greedy dependencies parses . their accuracy is slightly below the best dependency parses possible . but their performance is and the fact that they're sort of so fast and scalable . more than makes up for their teeny performance decrease . okay so then for the last few minutes i now want to get back to neural nets . okay so where are we at the moment . so at the moment we have a configuration where we have a stack and a buffer and parts of speech or words . and as we start to build some structure . the things that we've taken off we can kind of sort of think of them as starting to build up a tree as we go . 
as i've indicated with that example below . so the classic way of doing that is you could then say okay well we've got all of these features . like top of stack is word good or top of stack is word bad top of stack's part of speech as adjective . when you've got a combination of positions and words and parts of speech . you very quickly find that the number of features you have in your model extremely extremely large . but you know that's precisely how these kinds of parses were standardly made in the 2000s . so you're building these huge machine learning classifiers over sparse features . and commonly you even had features that were conjunctions of things . so you had features like the second word on the stack is has . and its tag is present tense verb . and the top word on the stack is good . and things like that would be one feature . and that's where you easily get into the ten million plus features . so even doing this already worked quite well . but the starting point from going on is saying well it didn't work completely great . that we wanna do better than that . and we'll go on and do that in just a minute . but before i do that i should mention just the evaluation of dependency parsing . evaluation of dependency parsing is actually very easy . cuz since for each word we're saying what is it a dependent of . 
that we're sort of making choices of what each word is a dependent of . which we get from our tree bank which is the gold thing . we're sort of essentially just counting how often we are right . and so there are two ways that that's commonly done . one way is that we just look at the arrows and ignore the labels . and that's often referred to as the uas measure unlabeled accuracy . or we can also pay attention to the labels . and say you're only right if and that's referred to as the las the labelled accuracy score . so the question is don't you have waterfall effects if you get something you do get some of that . because yes one decision will it's typically not so bad . because even if you mis-attach something like a prepositional phrase attachment . you can still get right all of the attachments inside noun phrase that's inside that prepositional phrase . and i mean actually dependency parsing evaluation suffers much less than doing cfg parsing which is worse in that respect . okay i had one slide there which i think i should skip . okay i'll skip on to neural ones . okay so people could build quite good machine learning dependency parsers on these kind of categorical features . but nevertheless there was a problems of doing that . so problem #1 is the features were just super sparse . that if you typically might have a tree bank that is an order about a million words and if you're then trying which are kinda different combinations of configurations . not surprisingly a lot of those configurations you've seen once or twice . 
so you just don't have any accurate model of what happens in different configurations . you just kind of getting these weak feature weights and crossing your fingers and hoping for the best . now it turns out that modern machine learning crossing your fingers works pretty well . but nevertheless you're suffering a lot from sparsity . okay the second problem is you also have an incompleteness problem because lots of configurations you'll see it run time will be different configurations that you just never happened to see the configuration . word on the stack and the top word of the stack speech or something . any kind of word pale i've only seen a small fraction of them . lot's of things you don't have features for . the third one is a little bit surprising . it turned out that when you looked at these symbolic dependency parsers and you ask what made them slow . what made them slow wasn't running your svm or your dot products in your logistic regression or things like that . all of those things were really fast . what these parsers were ending up spending 95% of their time doing is just computing these features and looking up their weights because you had to sort of walk around the stack and the buffer and sort of put together . a feature name and then you had to look it up in some big hash table to get a feature number and a weight for it . and all the time is going on that so even though there are linear time that slowed them down a ton . so in a paper in 2014 danqi and i developed this alternative where we said well let's just replace that all so that way we can have a dense compact feature representation and do classification . we'll have a relatively modest we'll use that to decide our next action . and so i want to spend the last few minutes sort of showing you how that works and this is basically question two of the assignment . okay and basically just to give you the headline this works really well . so this was sort of the outcome the first parser maltparser . 
so it has pretty good uas and las and it had this advantage that it was really fast . when i said that's been the preferred method i give you some contrast in gray . so these are two of the graph base parsers . so the graph based parsers have been somewhat more accurate but they were kind of like two orders in magnitude slower . so if you didn't wanna parse much stuff than you wanted accuracy you'd use them . but if you wanted to parse the web no one use them . and so the cool thing was that by doing this as neural network dependency parser we were able to get much better accuracy . we were able to get accuracy that was virtually as good as the best graph-based parsers at that time . and we were actually about to build a parser that works significantly faster than maltparser because of the fact that it wasn't spending all this time doing feature combination . it did have to do more vector matrix multiplies of course but that's a different story . okay so how did we do it . well so our starting point was distributed representation . so we're gonna use distributed representations of words . so similar words have close by vectors we've seen all of that . we're also going to use part in our pos we use part-of-speech tags and dependency labels . and we also learned distributed representations for those . that's kind of a cool idea cuz it's also the case that parts of speech some are more related than others . so if you have a fine grain part-of-speech set where you have plural nouns and proper names as different parts of speech from nouns singular you want to say so we also had distributed representations for those . so now we have the same kind of configuration . we're gonna run exactly the same transition based dependency parser . 
so the configuration is no different at all . but what we're going to extract from it is the starting point . just like nivre's maltparser but then what we're gonna do is for each of these positions like top of stack second top of stack buffer etc . words as sort of a 50 or 100 dimensional word vector representation of the kind that we've talked about . and so we get those representations for the different words as vectors and then what we're gonna do is just concatenate those into one longer vector . so any configuration of the parser is just being represented as the longest vector . well perhaps not that long our vectors are sort of more around 1000 not 10 million yeah . sorry the dependency of right the question is what's this dependency on feeding as an input . the dependency i'm feeding here as an import is when i previously built some arcs that are in my arc set i'm thinking maybe it'll be useful to use those arcs as well to help predict the next decision . so i'm using previous decisions on arcs as well to predict my follow-up decisions . okay so how do i do this . and this is essentially what you guys are gonna build . from my configuration i take things out of it . i get there embedding representations and i can concatenate them together and that's my input layer . i then run that through a hidden layer is a neural network feedforward neural network i then have from the hidden layer i've run that through a softmax layer and i get an output layer which is a probability distribution of my different actions in the standard softmax . and of course i don't know what any of these numbers are gonna be . so what i'm gonna be doing is i'm going to be using cross-entropy error and then back-propagating down to learn things . and this is the whole model and it learns super well and it produces a great dependency parser . i'm running a tiny bit short of time but let me just i think i'll have to rush this but i'll just say it . so non-linearities we've mentioned we haven't said very much about them and i just want to say a couple more something like a softmax . 
you can say that using a logistic function gives you a probability distribution . and that's kind of what you get in generalized linear models and statistics . in general though you want to say that . having these non-linearities sort of let's us do function approximation by putting together these various neurons that have some non-linearity . we can sorta put together little pieces like little wavelets to do functional approximation . and the crucial thing to notice is you have to use some non-linearity right . deep networks are useless unless you put something in between the layers right . if you just have multiple linear layers they could just be collapsed down into one product of linear transformations affine transformations is just an affine transformation . so deep networks without non-linearities do nothing okay . and so we've talked about logistic non-linearities . a second very commonly used non-linearity is the tanh non-linearity which is tanh is normally written a bit differently . but if you sort of actually do your little bit of math tanh is really the same as a logistic just sort of stretched and moved a little bit . and so tanh has the advantage that it's sort of symmetric around zero . and so that often works a lot better if you're putting it in the middle of a new neural net . but in the example i showed you earlier and for what you guys will be using for the dependency parser the suggestion to use for the first layer is this linear rectifier layer . and linear rectifier non-linearities are kind of freaky . they're not some interesting curve at all . linear rectifiers just map things to zero if they're negative and then linear if they're positive . and when these were first introduced i thought these were kind of crazy . i couldn't really believe that these were gonna work and do anything useful . 
the bigger your group the more we expect from the project . and you have to also write out exactly what each person in the project has done . you can actually use any kind of open source library and code that you want . but if you just take kaldi which is a speech recognition system and you say i did speech recognition . and then really all you did was download the package and run it then that's not very impressive . so the more you use the more you also have to be careful and say exactly what parts you actually implemented . and in the code you also have to submit your code so that we understand what you've done and the results are real . so this year we do want some language in there . last year i was a little more open . it could be the language of music and so on now . so we've got to have some natural language in there yeah . but other than that that can be done quite easily so projects you might want to do . and if you have a more theoretically inclined project where you really are just faking out some clever way of doing a sarcastic ready to sent or using different kinds of optimization functions . about leading the class to then as long as you at least applied it in one experiment to a natural language processing data set that would still be a pretty cool project . so you can also apply it to genomics data and to text data if you wanna have a little bit of that flavor . but there is gonna be at least one experiment where you apply it to a text data set . all right so now let's walk through the different kinds of projects that you might wanna consider and what might be entailed in such project to give you an idea . unless there are any other questions around the organization of the projects deadlines and so on . so let's start with the kind of simplest and all the other ones are sort of bonuses on top of that simple kind of project . and this is actually i think generally good advice not just for a class project but in general how to apply a deep learning algorithm to any kind of problem whether in academia or in industry or elsewhere . 
so let's assume you want to apply an existing neural network to an existing task . so you want to be able to take a long document and summarize into a short paragraph . now step one after you define your task is you have to define your dataset . and that is actually sadly in many cases in both industry and in academia an incredibly time intensive problem . to that is you just search for there's some people who've worked in summarization before . the nice thing is if you use an existing data set for instance from the document understanding conference duc here then other people have already applied some algorithms to it you'll have some base lines you know what kind of metric or evaluation is reasonable versus close to random . and so on cuz sometimes that's not always obvious . we don't always us just accuracy for instance . so in that case using an existing academic data set gets rid of a lot of complexity . however it is really fun if you actually come up with your own kind of dataset too . so maybe you're really excited about food and you want to prowl yelp or use a yelp dataset for restaurant review or something like that . so however when you do decide to do that you definitely have to check in with your mentor or with chris and me and others . because i sadly have seen several projects in the last couple of years where people have this amazing idea . and then they spent 80% of the time on their project on a web crawler getting not blocked from ip addresses writing multiple ip addresses having multiple machines and crawling . sometimes it's just the document they were hoping to get and crawl it's just a 404 page . and then they realize html and they filter that . and before you know it it's like they have like three more days left to do any deep learning for nlp . and so it has happened before so don't fall into that trap . if you do decide to do that check with us and try to before the milestone deadline . for sure have the data set ready so you can actually do deep learning for nlp cuz sadly we just can't give you a good grade for a deep learning for nlp class if you spend 95% of your time writing a web crawler and explaining your data set . 
so in this case for instance you might say all right i want to use wikipedia . you can actually download sort of already pre-crawled versions of it . maybe you want to say my intro paragraph is the summary of the whole rest of the article . not completely crazy to make that assumption but really you can be creative in this part . you can try to connect it to your own research or your own job if your a [inaudible] student or just any kind of interest that you have . to time it's really fun nlp combine with language of music with natural language and so on . so you can be creative here and we kind of value a little bit of the creativity this is like a task of data set we had never seen before and you actually gain some interesting linguistic insights or something . that is the cool part of the project right . any questions around defining a data set . all right so then you wanna define your metric . for instance you have maybe let's say you did something simpler like restaurant star rating classification . this is a review and i want to classify if this a four star review or a one star review or a two or three . and now you may have a class distribution where this is one star this is two stars three and four and now the majority are three . maybe that you troll kind of funny and are three star reviews . so this is just like number and maybe 90% of the things you called are in the third class . and then you write your report you're super excited it was a new data set you did well you crawled it quickly . and then all you give us is an accuracy metric so accuracy is total correct divided by total . and now let's say your accuracy is 90% . it's 90% accurate 90% of the cases gives you the ride star rating . you're essentially overfit to your dataset and your evaluation metric was completely bogus . 
it's hard to know whether they basically could have implemented a one line algorithm that's just as accurate as yours which is just no matter what the input return three . so hard to give a good grade on that and it's a very tricky trap to fall into . i see it all the time in industry and for young researchers and so on . so in this case you should've used does anybody know what kind f1 that's right . so and we'll go through some of these as we go through the class but it's very important to define your metric well . now for something as tricky as summarization this isn't where you're really just like this is the class this is the final answer . you have to actually either extract or generate a longer sequence . and there are a lot of different bleu's n-gram overlap or rouge share which is a recall-oriented understudy for gisting evaluation which essentially is just a metric to weigh differently how many n-grams are correctly overlapping between a human generated summary . for instance your wikipedia paragraph number one and whatever output your algorithm gives . so rouge is the official metric for summarization in different sub-communities and nop have their own metrics and it's important that you know what you're optimizing . so the machine translation for instance you might use bleu scores bleu scores are essentially also a type of n-gram overlap metric . if you have a skewed data set you wanna use f1 . and in some cases you can just use accuracy . and this is generally useful even if you're in industry and later in life you always wanna know what metric you're optimizing . it's hard to do well if you don't know the metric that you're optimizing for both in life and deep learning projects . all right so let's say you defined your metric now you need to split your dataset . and it's also very important step and it's also something that you can easily make sort of honest mistakes . again in advantage of taking pre-existing academic dataset is that in many cases it's already pre-split but not always . and you don't wanna look at your 1 week before the deadline . so let's say you have downloaded a lot of different articles and now you basically have 100% of some articles you wanna summarize . 
and normal split would be take 80% for training you take 10% for your validation and your development . the validation split or the development split or dev split or various other terms . and 10% for your final test split . and so the final one you ideally get a sense of how your algorithm would work in real life on data you've never seen before you didn't try to chew on your model like how many layers should i use how wide should each layer be . you'll try a lot of these things we'll describe these in the future . but it's very important to correctly split and why do i make such a fuss about that . well there too you might make mistakes . so let's say you have unused text and let's say you crawled it in such a way there's a lot of mistakes that you can make if you try to predict the soft market for instance don't do that it doesn't work . but in many cases you might say or there some temporal sequence . and now you basically have all your dataset and the perfect thing to do is actually do it like this you take 80% of let's say month january to may or something and then your final test split is from november . that way you know there's no overlap . but maybe you made a mistake and you said well i crawled it this way but so as sample an article from here and one from here and one from here . and then the random sample goes to the 80% of my training data . and now the test data and the development data might actually have some overlap . cuz if you're depending on how you chose your dataset maybe the another article which just like a slight addition like some update to an emerging story . and now the summary is almost exact same but the input document just changed a tiny bit . and you have one article in your training set and another one in your test set . but the test set article is really only one extra paragraph on an emerging story and the rest is exactly the same . so now you have an overlap of your training and your testing data . and so in general if this is your training data and this should be your test data . 
it should be not overlapping at all . and whenever you do really well you run your first experiment and you get 90 f1 . and things look just too good to be true sadly in many cases they are and you made some mistake where maybe your test set had some overlap for instance with your training data . it's very important to be a little paranoid about that when your first couple of experiments turn out just to be too good to be true . that can mean either your training your task is too simple or you made a mistake in splitting and defining your dataset . all right any questions around defining a metric or your dataset yeah . so if we split it temporally wouldn't we learn a different distribution . that is correct we would learn a different distribution these are non-stationary . and that is kinda true for a lot of texts but if you ideally when you built a deep learning system for an lp you want it to built it so that it's robust . it's robust to sum such changes over time . and you wanna make sure that when you run it in a real world setting on something you've never seen before it's doing something it will still work . and this was the most realistic way to capture how well it would work in real life . would it be appropriate to run both experiments as in both where you subsample randomly and then you subsample temporally for your . you could do that and the intuitive thing that is likely going to happen is if you sample randomly from all over the place then you will probably do better than if you have this sort of more strict kind of split . but running an additional experiment will rarely ever get you points subtracted . you can always run more experiments and we're trying really hard to help you get computing infrastructure and cloud compute . so you don't feel restricted with the number of experiments you run . all right now number 5 establish a baseline . so you basically wanna implement the simplest model first . this could just be a very simple logistic regression on unigrams or bigrams . 
then compute your metrics on your train data and your development data so overfitting or underfitting . let's say your loss is very very low on training . you do very well on training but you don't do very well on testing then you're in an over fitting regime . if you do very well on training and well on testing you're done you're happy . but if your training loss can't be lower so you're not even doing well on your training that often means your so it's very important to compute both the metrics on your training and your development split . and then and this is something we value a lot in this class too . and it's something very important for you in both research and industries like you wanna analyze your errors carefully for that baseline . and if the metrics are amazing and there are no errors you're done . probably a problem was too easy and you may wanna restart unless it's really a valuable problem for the world . and then maybe you can just really describe it carefully and you're done too . it is very important to not just go in and add lots of bells and whistles that you'll learn about in the next couple of weeks in this class and create this monster of a model . you want to start with something simple sanity check make sure you didn't make mistakes in splitting your data . you have the right kind of metric . and in many cases it's a good indicator for how successful your final project is if you can get this baseline in the first half of the quarter . cuz that means you figured out a lot of these potential issues here . and you kind of have your right data set . you know what the metric is you know what you're optimizing and everything is good . so try to get to this point as quickly as possible . cuz that is also not as interesting and you can't really use that much knowledge from the class . and now you can implement some existing neural network model that we taught you in class . 
for instance this window-based model if your task is named entity recognition . you can compute your metric again on your train and dev set . hopefully you'll see some interesting patterns such as usually train neural nets is quite easy in a sense that we lower the loss very well . and then we might not generalize as well in the development set . and then you'll play around with regularization techniques . and don't worry if some of the stuff i'm saying now is kind of confusing . if you want to do this we'll walk you through that as we're mentoring you through the project . and that's why each project has to have an assigned mentor that we trust . all right then you analyze your very important be close to your data . you can't give too many examples usually ever . and this is kind of the minimum bar for this class . so if you've done this well and there's an interesting dataset then your project is kind of in a safe haven . once you have a metric and everything looks good we still want you to visualize the kind of data even if it's a known data set . we wanted you to visualize it collect summary statistics . it's always good to know the distribution if you have different kinds of classes . you want to again very important look at the errors that your model is making . cuz that can also give you intuitions of what kinds of patterns can your deep learning algorithm not capture . maybe you need to add a memory component or maybe you need to have longer temporal kind of dependencies and so on . those things you can only figure out if you're close to your data and you look at the errors that your baseline models are making . and then we want you to analyze also different hyperparameters . 
a lot of these models have lots of choices . did we add the sigmoid to that score or is the second layer 100 dimensional or 200 dimensional . should we use 50 dimensional word vectors or 1000 dimensional word vectors . there are a lot of choices that you make . and it's really good in your first couple projects to try more and and sometimes if you're running out of time and only so much so many experiments you can run we can help you and use our intuition to guide you . but it's best if you do that a little bit yourself . and once you've done all of that now you can try different model variants and you'll soon see a lot of these kinds of options . we'll talk through all of them in the class . so now another kind of class project is you actually wanna implement a new fancy model . those are the kinds of things that will put you into potentially writing an academic paper peer review and at a conference and so on . the tricky bit of that is you kinda have to do all the other steps that i just described first . and then on top of that you know the errors that you're making . and now you can gain some intuition of why the existing models are flawed . and you come up with your own new model . if you do that you really wanna be in close contact with your mentor and some researchers unless you're a researcher yourself and you earned your phd . but even then you should chat with us from the class . you want to basically try to set up an infrastructure such that you can iterate quickly . you're like maybe i should add this new layer type to this part of my model . you want to be able to quickly iterate and see if that helps or not . so it's important and actually require a fair amount of software engineering skills to set up efficient experimental frameworks that and again you want to start with simple models and then go to more and more complex ones . 
so for instance in summarization you might start with something super simple like just average all your word vectors in the paragraph . and then do a greedy search of generating one word at a time . or even greedily searching for just snippets from the existing article in wikipedia and you're just copying certain snippets over . and then stretch goal is something more advanced would be lets you actually generate that whole summary . and so here are a couple of project ideas . but again we'll post the whole list of them with potential mentors from the nop group and the vision group and various other groups inside stanford . sentiment is also a fun data set . you can look at this url here for one of the preexisting data sets that a lot of people have worked on . all right so next week we'll look at some fun and fundamental linguistic tasks like syntactic parsing . and then you'll learn tensorflow and have some great tools under your belt . again with cs224n natural language processing with deep learning . so you're in for a respite or a change of pace today . so for today's lecture what we're principally going to look at is syntax grammar and dependency parsing . so my hope today is to teach you in one lecture enough about dependency grammars and parsing that you'll all be able to do the main part of assignment 2 successfully . so quite a bit of the early part of the lecture is giving a bit of background about syntax and dependency grammar . and then it's time to talk about a particular kind of dependency grammar transition-based also dependency parsing transition-based dependency parsing . and then it's probably only in the last kind of 15 minutes or so of the lecture that we'll then get back into specifically neural network content . talking about a dependency parser that danqi and i wrote a couple of years ago . okay so for general reminders i hope you're all really aware that assignment 1 is due today . and i guess by this stage you've either made good progress or you haven't . 
but to give my good housekeeping reminders i mean it seems like every year there are people that sort of blow lots of late days on the first assignment for no really good reason . and that isn't such a clever strategy [laugh] . so hopefully [laugh] you are well along with the assignment and can aim to hand it in before it gets to the weekend . okay then secondly today is also the day that the new assignment comes out . till the start of next week but we've got it up ready to go . and so that'll involve a couple of new things and in some respects probably for much of it you might not want to start it until after next tuesday's lecture . so two big things will be different for that assignment . big thing number one is we're gonna do assignment number two using tensorflow . and that's the reason why quite apart from exhaustion from assignment one why you probably you don't wanna start it on the weekend is because on tuesday tuesday's lecture's gonna be an introduction to tensorflow . so you'll really be more qualified then to start it after that . and then the other big different thing in assignment two is we get into natural language processing content . in particular you guys are going to build neural dependency parsers and the hope is that you can learn about everything that you need to know to do that today . the readings on the website if you don't get quite everything straight from me . we're going to sort of post hopefully tomorrow or on the weekend a kind of an outline of what's in assignment four so you can have sort of a more informed meaningful choice between whether you want to do assignment four or the area of assignment four if you do it is going to be question answering over the squad dataset . but we've got kind of a page and a half description to explain what that means so you can look out for that . but if you are interested in we'll encourage people to come and meet with one of the final project mentors or find some other well qualified person around here to be a final project mentor . so what we're wanting is that sort of everybody has met with their final project mentor before putting in an abstract . and that means it'd be really great for people to get started doing that as soon as possible . i know some of you have already talked to various of us . for me personally i've got final from 1 to 3 pm so i hope some people will come by for those . 
and again sort of as richard mentioned not everybody can possible have richard or me as the final project mentor . and besides there's some really big advantages of having some of the phd student tas as final project mentors . cuz really for things like spending time hacking on tensorflow they get to do it much more than i do . and so danqi kevin ignacio arun that they've had tons of experience doing nlp research using deep learning . and so that they'd also be great mentors and look them up for their final project advice . the final thing i just want to touch on is we clearly had a lot of problems i realize at keeping up and coping with people in office hours and queue status has just i'm sorry that that's been kind of difficult . i mean honestly we are trying to work and work out ways that we can do this better and we're thinking of sort of unveiling a few changes for doing things for the second assignment . if any of you peoples have any better advice as to how things could be organized so that they could work better feel free to send a message on piazza with suggestions of ways of doing it . i guess yesterday i ran down percy liang and said percy percy how do you do it for cs221 . do you have some big secrets to do this better . but unfortunately i seem to come away with no big secrets cuz he sort of said: "we use queue status and we use the huang basement" what else are you meant to do . so i'm still looking for that divine insight [laugh] that will tell me how to get this so if you've got any good ideas feel free to share . but we'll try to get this as much better under control as we can for the following weeks . okay any questions or should i just go into the meat of things . all right so what we're going to want to do today is work out how to put structures over sentences in some human language . all the examples i'm going to show is for english but in principle the same techniques you can apply for any language where these structures are going to sort of reveal how the sentence is made up . so that the idea is that sentences and parts of sentences have some kind of structure and there are sort of regular ways that people put sentences together . so we can sort of start off with very simple things that aren't yet sentences like "the cat" and "a dog" and they seem to kind of have a bit of structure . we have an article or what linguists often call a determiner that's followed by a noun . and then well for those kind of phrases which get called noun you can kind of make them bigger and there are sort of rules for how you can do that . 
so you can put adjectives in between the article and the noun . you can say the large dog or a barking dog or a cuddly dog and things like that . and well you can put things like what i call prepositional phrases after the noun so you can get things like "a large dog in a crate" or something like that . and so traditionally what linguists and natural language processors have wanted to do is describe the structure of human languages . and they're effectively two key tools that people have used to do this and one of these key tools and i think in general the only one you have seen a fraction of is to use what in computer science terms what is most commonly referred to as context free grammars which are often referred to by linguists as phrase structure grammars . and is then referred to as the notion of constituency and so for that what we are doing is writing these context free grammar rules and the least if you are standford undergrad or something like that . i know that way back in 103 you spent a whole lecture learning about context-free grammars and their rules . so i could start writing some rules that might start off saying a noun phrase and go to a determiner or a noun . then i realized that noun phrases would get a bit more complicated . and so i came up with this new rule that says- noun phrase goes to terminal optional adject of noun and then optional prepositional phrase wherefore prepositional phrase that's a preposition followed by another noun phrase . because i can say a crate or a large crate . or a large crate by the door . even further and i could say you know a large barking dog by the door in a crate . so then i noticed wow i can put in multiple adjectives there and i can stick on multiple prepositional phrases so i'm using that star the kinda clingy star that you also see see in regular expressions to say that you can have zero or any number of these . and then i can start making a bigger thing like talk to the cuddly dog . and well now i've got a verb followed by a prepositional phrase . and so i can sort of build up a constituency grammar . so that's one way of organizing the structure of sentences and you know in 20th dragging into 21st century america this has been i mean it's what you see mainly in your intro cs class when you get taught about regular languages and context free languages and context sensitive languages . hierarchy where noam chomsky did not actually invent the chomsky hierarchy to torture cs under grads with formal content to fill the scs 103 class . the original purpose of the chomsky hierarchy was actually to understand the complexity of human languages and to make arguments about their complexity . 
sorry it's also dominated sorta linguistics in america in the last 50 years through the work of noam chomsky . but if you look more broadly than that this isn't actually the dominate form of syntactic description that is being used for understanding of the structure of sentences . so there is this other alternative view of linguistic structure which is referred to as dependency structure and what your doing with dependency structure . is that you're describing the structure of a sentence by taking each word and saying what it's a dependent on . so if it's a word that kind of modifies or is an argument of another word that you're saying it's a dependent of that word . so barking dog barking is a dependent of dog because it's of a modifier of it . large barking dog large is a modifier of dog as well so it's a dependent of it . and dog by the door so the by the door is somehow a dependent of dog . and we're putting a dependency between words and we normally indicate those dependencies with arrows . structures over sentences that say how they're represented as well . and when right in the first class i gave examples of ambiguous sentences . a lot of those ambiguous sentences we can think about in terms of dependencies . so do you remember this one scientists study whales from space . and well why is it an ambiguous headline . well it's ambiguous because there's sort of two possibilities . so in either case there's the main verb study . and it's the scientist that's studying that's an argument of study the subject . and it's the whales that are being studied so that's an argument of study . but the big difference is then what are you doing with the from space . you saying that it's modifying study or are you saying it's modifying whales . 
quickly read the headline it sounds like it's the bottom one right . but [laugh] what the article was meant to be about was really that they were being able to use satellites to track the movements of whales . and so it's the first one where the from space is modifying . and so thinking about ambiguities of sentences can then be thought about many of them in terms of these dependency structures as to what's modifying what . and this is just a really common thing in natural language because these kind of questions of what modifies what really dominate a lot of questions of interpretation . so here's the kind of sentence you find when you're reading the wall street journal every morning . the board approved its acquisition by royal trustco limited of toronto for $27 a share at its monthly meeting . and as i've hopefully indicated by the square brackets if you look at the structure of this sentence it sort of starts off as subject verb object . the board approved its acquisition and then everything after that is a whole sequence of prepositional phrases . by royal trustco ltd of toronto for $27 a share at its monthly meeting . and well so then there's the question of what's everyone modifying . so the acquisition is by by royal trustco ltd is modifying the thing that immediately precedes that . and of toronto is modifying the company royal trustco limited so that's modifying the thing that comes immediately preceeding it . so you might think this is easy everything just modifies the thing that's coming immediately before it . so what's for $27 a share modifying . yeah so that's modifying the acquisition so then we're jumping back a few candidates and saying is modifying acquisition and then actually at it's monthly meeting . that wasn't the toronto the royal trustco ltd or the acquisition that that was when the approval was happening so that jumps all the way back up to the top . so in general the situation is that if you've got some stuff like a verb and getting these prepositional phrases . well the prepositional phrase can be modifying either this noun phrase or the verb . but then when you get to the second prepositional phrase . 
well there was another noun phrase inside this prepositional phrase . it can be modifying this noun phrase that noun phrase or the verb phrase . and then we get to another one . and you don't get sort of a completely free choice cuz you do get a nesting constraint . so once i've had for $27 a share referring back to the acquisition the next prepositional phrase has to in general refer to either the acquisition or approved . i say in general because and i'll actually talk about that later . but most of the time in english it's true . you have to sort of refer to the same one or further back so you get a nesting relationship . but i mean even if you obey that nesting relationship the result is that you get an exponential number of ambiguities in a sentence based on in the number of prepositional phrases you stick on the end of the sentence . and so the series of the exponential series you get of these catalan numbers . and so catalan numbers actually show up in a lot of places in that is somehow sort of similar if you're putting these constraints in you get catalan series . so are any of you doing cs228 . yeah so another place the catalan series turns up is that when you've got a vector graph and you're triangulating it the number of ways that you can triangulate your vector graph is also giving you catalan numbers . okay so human languages get very ambiguous . and we can hope to describe them on the basis of sort of looking at these dependencies . the other important concept i wanted to introduce at this point is this idea of full linguistics having annotated data in the form of treebanks . this is probably a little bit small to see exactly . but what this is is we've got sentences . these are actually sentences that come off yahoo answers . and what's happened is human beings have sat around and drawn in the syntactic structures of these sentences as dependency graphs and those things we refer to as treebanks . 
and so a really interesting thing that's happened starting around 1990 is that people have devoted a lot of resources to building up these kind of annotated treebanks and various other kinds of annotated linguistic resources that we'll talk about later . now in some sense from the viewpoint of sort of modern machine learning in 2017 that's completely unsurprising because all the time what we do is say we want labelled data so we can take our supervised classifier and chug on it and get good results . but in many ways it was kind of a surprising thing that happened which is sort of different to the whole of the rest of history right . cuz for the whole of the rest of the history it was back in this space of well to describe linguistic structure what we should be doing is writing grammar rules that describe what happens in linguistic structure . where here we're no longer even attempting to write grammar rules . we're just saying give us some sentences . and i'm gonna diagram these sentences and show you what their structure is . and tomorrow give me a bunch more and i'll diagram them for you as well . and if you think about it in a way that initially seems kind of a crazy thing to do cuz it seems like just putting structures over sentences one by one seems really really inefficient and slow . whereas if you're writing a grammar you're writing this thing that generalizes right . the whole point of grammar is that you're gonna write this one small finite grammar . and it describes an infinite number of sentences . and so surely that's a big labor saving effort . but slightly surprisingly but maybe it makes sense in terms of what's happened in machine learning that it's just turned out to be kind of super successful this building of explicit annotated treebanks . and it ends up giving us a lot of things . and i sort of mention a few of their advantages here . first it gives you a reusability of labor . but the problem of human beings handwriting grammars is that they tend to in practice be almost unreusable because everybody does it differently and has their idea of the grammar . and people spend years working on one and no one else ever uses it . where effectively these treebanks have been a really reusable tool that lots of people have then built on top of to build all kinds of natural language processing tools of part of speech taggers and parsers and things like that . 
they've also turned out to be a really useful resource actually for linguists because they give a kind of real languages are spoken complete with syntactic analyses that you can do all kinds of quantitative linguistics on top of . it's genuine data that's broad coverage when people just work with their intuitions as to what are the grammar rules of english . and so this is actually a better way to find out all of the things that actually happened . for anything that's sort of probabilistic or machine learning it gives some sort of not only what's possible but how frequent it is and what other things it tends to co-occur with and all that kind of distributional information that's super important . and crucially crucially crucially and we'll use this for assignment two it's also great because it gives you a way to evaluate any system that you built because this gives us what we treat as ground truth gold standard data . and then we can evaluate any tool on how good it is at reproducing those . and what i wanted to do now is sort of go through a bit more carefully for sort of 15 minutes what are dependency grammars and dependency structure . so we've sort of got that straight . i guess i've maybe failed to say yeah . i mentioned there was this sort of constituency context-free grammar viewpoint and the dependency grammar viewpoint . and what we're doing for assignment two is all dependencies . we will get back to some notions of constituency and phrase structure . you'll see those coming back in later classes in a few weeks' time . but this is what we're going to be doing today . and that's not a completely random choice . it's turned out that unlike what's happened in linguistics in most of the last 50 years in the last decade in natural language processing it's essentially been swept by the use of dependency grammars that people have found dependency grammars just a really suitable framework on which to build semantic representations to get out the kind of understanding of language that they'd like to get out easily . they enable the building of very fast efficient parsers as i'll explain later today . and so in the last sort of ten years you've just sort of seen this huge sea change in natural language processing . whereas if you pick up a conference volume around the 1990s it was basically all phrase structure grammars and one or two papers on dependency grammars . and if you pick up a volume now what you'll find out is that of the papers they're using syntactic representations dependency representations . 
phrase structure what's the phrase structure grammar that's exactly the same as the context-free grammar when a linguist is speaking . okay so what does a dependency syntax say . so the idea of dependency syntax is to say that the sort of model of syntax is we have relationships between lexical items words and only between lexical items . they're binary asymmetric relations which means we draw arrows . so the whole there is a dependency analysis of bills on ports and senator brownback republican of kansas . okay so that's a start normally hen we do dependency parsing we do a little bit more than that . so typically we type the dependencies by giving them a name for some grammatical relationship . so i'm calling this the subject and it's actually a passive subject . and then this is an auxiliary modifier republican of kansas is an appositional phrase that's coming off of brownback . and so we use this kind of typed dependency grammars . and interestingly i'm not going to go through it but there's sort of some interesting math that if you just have this although it's notationally very different from context-free grammar to a restricted kind of context-free grammar with one addition . but things become sort of a bit more different once you put in a typing of the dependency labels where i wont go into that in great detail right . so a substantive theory of dependency grammar for a language we're then having to make some decisions . so what we're gonna do is when we between two things and i'll just mention a bit more terminology . so we have an arrow and its got what we called the tail end of the arrow i guess . and the word up here is sort of the head . so bills is an argument of submitted were is an auxiliary modifier of submitted . and so this word here is normally referred to as the head or the governor or the superior or sometimes even the regent . and then the word at the other end of the arrow the pointy bit i'll refer to as the dependent but other words that you can sometimes see are modifier inferior subordinate . some people who do dependency grammar really get into these classist notions of superiors and inferiors but i'll go with heads and dependents . 
okay so the idea is you have a head of a clause and then the arguments of the dependence . and then when you have a phrase like by senator brownback republican of texas . being taken as brownback and then it's got words beneath it . and so one of the main parts of dependency grammars at the end of the day as to which words are heads and which words are then the dependents of the heads of any particular structure . so in these diagrams i'm showing you here the ones i showed you back a few pages here is analysis according to universal dependencies . so universal dependencies is a new tree banking effort which i've actually been very strongly involved in . that sort of started a couple of years ago and there are pointers in both earlier in the slides and on the website if you wanna go off and learn a lot about universal dependencies . i mean it's sort of an ambitious attempt to try and have a common dependency representation that works over a ton of languages . i could prattle on about it for ages and if by some off chance there's time at the end of the class i could . but probably there won't be so i won't actually tell you a lot about that now . but i will just mention one thing that probably you'll notice very quickly . and we're also going to be using this representation in the assignment that's being given out today the analysis of universal dependencies treats prepositions sort of differently to what you might have seen else where . if you've seen any many accounts of english grammar or heard references in some english classroom to have prepositions having objects . in universal dependencies prepositions don't have any dependents . of like they were case markers if you know any language like german or latin or hindi or something that has cases . so that the by is sort of treated as if it were a case marker of brownback . so this sort of a bleak modifier of by senator brownback . brownback here as the head with the preposition as sort of like a case marking dependent of by . and that was sort of done to get more parallelism across different languages of the world . other properties of old dependencies normally dependencies form a tree . 
so there are formal properties that goes along with that . that means that they've got a single-head they're acyclic and they're connected . so there is a sort of graph theoretic properties . yeah i sort of mentioned that really dependencies have dominated most of the world . the famous first linguist was panini who wrote his grammar of sanskrit really most of the work that panini did was kind of on sound systems and make ups of words phonology and morphology when we mentioned linguistic levels in the first class . and he only did a little bit of work on the structure of sentences . but the notation that he used for structure of sentences was essentially a dependency grammar of having word relationships being marked as dependencies . yeah so the question is well compare cfgs and pcfgs and do they dependency grammars look strongly lexicalized they're between words and does that makes it harder to generalize . i honestly feel i just can't do justice to that question right now if i'm gonna get through the rest of the lecture . but i will make two comments so i mean there's certainly the natural way to think of dependency grammars they're strongly lexicalized you're drawing relationships between words . whereas the simplest way of thinking of context-free grammars is you've got these rules in terms of categories like . noun phrase goes to determiner noun optional prepositional phrase . and so that is a big difference . but it kind of goes both ways . so normally when actually natural language processing people wanna work with context-free grammars they frequently lexicalize them so they can do more precise probabilistic prediction and vice versa . if you want to do generalization and dependency grammar you can still use at least notions of parts of speech to give you a level of generalization as more like categories . but nevertheless the kind of natural ways of sort of turning them into probabilities and machine learning models are quite different . though on the other hand there's sort of some results or sort of relationships between them . but i would think i'd better not go on a huge digression . that means to rather than just have categories like noun phrase to have categories like a noun phrase headed by dog and so it's lexicalized . 
let's leave this for the moment though please okay . okay so that's panini and there's a whole big history right . so essentially for latin grammarians what they did for the syntax of latin again not very developed . they mainly did morphology but it was essentially a dependency kind of analysis that was given . there was sort of a flowering of arabic grammarians in the first millennium and they essentially had a dependency grammar . i mean by contrast i mean really kind of context free grammars and constituency grammar only got invented almost in the second half of the 20th century . i mean it wasn't actually chomsky that originally invented them there was a little bit of earlier work in britain but only kind of a decade before . so there was this french linguist lucien tesniere he is often referred to as the father he's got a book from 1959 . dependency grammars have been very popular and more sorta free word order languages cuz notions sort of like context-free languages like english that have very fixed word order but a lot of other languages of the world have much freer word order . and that's often more naturally described with dependency grammars . interestingly one of the very first natural language parsers developed in the us was also a dependency parser . so david hays was one of the first us computational linguists . and one of the founders of the association for computational linguistics which is our main kind of academic association where we publish our conference papers etc . and he actually built in 1962 a dependency parser for english . okay so a lot of history of dependency grammar . so couple of other fine points to note about the notation . people aren't always consistent in which way they draw the arrows . i'm always gonna draw the arrows so they point go from a head to a dependent which is the direction which tesniere drew them . but there are some other people who draw the arrows the other way around . so they point from the dependent to the head . 
and so you just need to look and see what people are doing . the other thing that's very commonly done is you stick this pseudo-word wall or some other name like that and that kind of makes the math and formalism easy because then every sentence starts with root and something is a dependent of root . or turned around the other way if you think of what parsing a dependency grammar means is for every word in the sentence you're going to say what is it a dependent of because if you do that you're done . you've got the dependency structure of the sentence . and what you're gonna want to say is well it's either gonna be a dependent of some other word in the sentence or it's gonna be a dependent of the pseudo-word root which is meaning it's the head of the entire sentence . specifics of dependency parsing but the kind of thing that you should think about is well how could we decide which words are dependent on what . and there are certain various information sources that we can think about . so yeah it's sort of totally natural with the dependency representation to just think about word relationships . and that's great cuz that'll fit super well with what we've done already in distributed word representations . so actually doing things this way just fits well with a couple of tools we already know how to use . we'll want to say well discussion of issues is that a reasonable attachment as lexical dependency . and that's a lot of the information that we'll actually use but information that we'd also like to use . dependency distance so sometimes there are dependency relationships and sentences between words that is 20 words apart when you got some big long sentence and you're referring that back to some previous clause but it's kind of uncommon . most of dependencies are pretty short distance so you want to prefer that . many dependencies don't sort of so if you have the kind of dependencies that occur inside noun phrases like adjective modifier they're not gonna cross over a verb . it's unusual for many kinds of dependencies to cross over a punctuation so it's very rare to have a punctuation between a verb and a subject and things like that . so looking at the intervening material gives you some clues . and the final source of information is sort of thinking about heads and thinking how likely they are to have to dependence in what number and on what sides . so the kind of information there is right a word like the is basically not likely to have any dependents at all anywhere . so you'd be surprised if it did . 
words like nouns can have dependents and they can have quite a few dependents but they're likely to have some kinds like determiners and adjectives on the left other kinds like prepositional phrases on the right verbs tend to have a lot of dependence . so different kinds of words have different kinds of patterns of dependence and so there's some information there we could hope to gather . okay yeah i guess i've already said the first point . in principle it's kind of really easy . so we're just gonna take every make a decision as to what word or root this word is a dependent of . and we do that with a few constraints . so normally we require that only one word can be a dependent of root and we're not going to allow any cycles . and if we do both of those things we're guaranteeing that we make the dependencies of a tree . and normally we want to make out dependencies a tree . and there's one other property i then wanted to mention that if you draw your dependencies as i have here so all the dependencies been drawn as loops above the words . it's different if you're allowed to put some of them below the words . whether you can draw them like this . so that they have that kind of nice none of them cross each other . or whether like these two that i've got here where they necessarily cross each other and i couldn't avoid them crossing each other . and what you'll find is in most languages certainly english the vast majority of dependency relationships have a nesting structure relative to the linear order . and if a dependency tree is fully nesting it's referred to as that you can lay it out in this plane and have sort of a nesting relationship . but there are few structures in english where you'd get things that aren't nested and yet crossing . and this sentence is a natural example of one . so i'll give a talk tomorrow on bootstrapping . so something that you can do with noun modifiers especially if they're kind of long words like bootstrapping or techniques of bootstrapping is you can sort of move them towards the end of the sentence right . 
i could have said i'll give a talk on bootstrapping tomorrow . but it sounds pretty natural to say i'll give a talk tomorrow on bootstrapping . but this on bootstrapping is still modifying the talk . and so that's referred to by linguists as right extraposition . and so when you get that kind of rightward movement of phrases you then end up with these crossing lines . and that gives you what's referred to as a non-projective dependency tree . so importantly it is still a tree if you sort of ignore the constraints of linear order and you're just drawing it out . there's a graph in theoretical computer science right it's still a tree . it's only when you consider this extra thing of the linear order of the words that you're then forced to have the lines across . and so that property which you don't actually normally see mentioned in theoretical computer science discussions of graphs is then this property that's referred to projectivity . recover the order of the words from a dependency tree . so given how i've defined dependency trees the strict answer is no . they aren't giving you the order at all . now in practice people write down the words of a sentence in order and have these crossing brackets right crossing arrows when they're non-projective . and obviously it's a real thing about languages that they have linear order . but as i've defined dependency structures you can't actually recover okay one more slide before we get to the intermission . yeah so in the second half of the class i'm gonna tell you about a method of dependency parsing . i just wanted to say very quickly there are a whole bunch about doing dependency parsing . so one very prominent way of doing dependency parsing is using dynamic programming methods which is normally what people have used for constituency grammars . a second way of doing it is to use graph algorithms . 
so a common way of doing dependency parsing you're using mst algorithms minimum spanning tree algorithms . and that's actually a very successful way of doing it . you can view it as kind of a constraint satisfaction problem . but the way we're gonna look at it is this fourth way which is these days most commonly called transition based-parsing though when it was first introduced it was quite often called deterministic dependency parsing . and the idea of this is that we're kind of greedily going to decide which word each word is a dependent of guided by having a machine learning classifier . so one way of thinking about this is so far in this class we only have two hammers . one hammer we have is word vectors and you can do a lot with word vectors . and the other hammer we have is how to build a classifier as a feedforward neural network with a softmax on top so it classifies between two various classes . and it turns out that if those are your two hammers you can do dependency parsing this way and it works really well . and so therefore that's a great approach for using in assignment two . and it's not just a great approach for assignment two . actually method four is the dominant way these days of doing dependency parsing because it has extremely good properties of scalability . that greedy word there is a way of saying this is a linear time algorithm which none of the other methods are . so in the modern world of web-scale parsing it's sort of become most people's favorite method . so i'll say more about that very soon . but before we get to that we have ajay doing our research spotlight with one last look back at word vectors . okay awesome so let's take a break from dependency parsing and talk about something we should know a lot about word embeddings . so for today's research highlight we're gonna be talking about a paper titled improving distributional similarity with lessons learned from word embeddings . and it's authored by levy et al . so in class we've learned two major paradigms for generating word vectors . 
we've learned count-based distributional models which essentially utilize a co-occurrence matrix to produce your word vectors . and we've learned svd which is singular value decomposition . and we haven't really talked about ppmi . but in effect it still uses that co-occurrence matrix to produce sparse vector encodings for words . we've also learned neural network-based models which you all should have lots of experience with now . and specifically we've talked about skip-gram negative sampling as well as cbow methods . and glove is also a neural network-based model . and the conventional wisdom is that neural network-based models are superior to count-based models . however levy et al proposed that hyperparameters and system design choices are more important not the embedding algorithms themselves . and so essentially what they do in their paper is propose a slew of hyperparameters that when implemented and tuned over the count-based distributional models pretty much approach the performance of neural network-based models to the point where there's no consistent better choice across the different tasks that they tried . and a lot of these inspired by these neural network-based models such as skip-gram . so if you recall which you all should be very familiar with this we have two hyperparameters in skip-gram . we have the number of negative samples that we're sampling as well as the unigram distributions smoothing exponent which we fixed at 3 over 4 . but it can be thought of as more of a system design choice . and these can also be transferred over to the account based variants . and i'll go over those very quickly . so the single hyper parameter that levy et al . impact in performance was context distribution smoothing which is analogous to the unigram distribution smoothing constant 3 over 4 here . and in effect they both achieved the same goal which is to sort of smooth out your distribution such that you're penalizing rare words . which interestingly enough the optimal alpha they found was exactly 3 over 4 which is the same as the skip-gram unigram smoothing exponent . 
they were able to increase performance by an average of three points across tasks on average which is pretty interesting . and they also propose shifted pmi which i'm not gonna get into the details of this . but this is analogous to the negative sampling choosing the number of negative samples in skip-gram . and they've also proposed a total of eight hyperparameters in total . and we've described one of them which is the context distribution smoothing . and this is a lot of data and if you're confused that's actually the conclusion that i want you to arrive at because clearly there's no trend here . so what the authors did was take all four methods tried three different windows and then test all the models across a different task . and those are split up into word similarity and analogy task . and all of these methods are tuned to find the best hyperparameters to optimize for the performance . and the best models are bolded and as you can see there's no consistent best model . so in effect they're challenging the popular convention that neural network-based models are superior to the count-based models . however there's a few things to note here . number one adding hyperparameters is never a great thing because now you have to train those hyperparameters which takes time . number two we still have the issues with count-based distributional models specifically with respect to the computational as well as performing svd . so the key takeaways here is that the paper challenges the conventional wisdom that neutral network-based models are in fact superior to count-based models . number two while model design is important hyperparameters are also key for achieving good results . so this implies specifically to doing a project instead of assignment four . you might implement the model but that might only take you half way there . some models to find your optimal hyperparameters might take days or even weeks to find . and finally my personal interest within ml is in deep representation learning . 
and this paper specifically excites displays that there's still lots of work to be done in the field . and so the final takeaway is challenge the status quo . okay and so now we're back to learning about how to build a transition based dependency parser . so maybe in 103 or compilers class formal languages class there's this notion of shift reduced parsing . how many of you have seen shift reduced parsing somewhere . they just don't teach formal languages the way they used to in the 1960s in computer science anymore . okay well i won't assume that you've all seen that before . okay essentially what we're going to have is i'll just skip these two slides and go straight to the pictures . because they will be much more understandable . mention the picture on this page that's a picture of joakim nivre . so joakim nivre is a computational linguist in uppsala sweden who pioneered this approach of transition based dependency parsing . he's one of my favorite computational linguists . i mean he was also an example going along with what ajay said of sort of doing something unpopular and out of the mainstream and proving that you can get it to work well . so at an age when everyone else was trying to build sort of fancy dynamic program parsers joakim said nono what i'm gonna do is i'm just gonna take each successive word and have a straight classifier that says what to do with that . and go onto the next word completely greedy cuz maybe that's kinda like what humans do with incremental sentence processing and i'm gonna see how well i can make that work . and it turned out you can make it work really well . so and then sort of transition based parsing has grown to this sort of really widespread dominant way of doing parsing . so it's good to find something different to do if everyone else is doing something it's good to think of something else that might be promising that you got an idea from . and i also like joakim because he's actually another person that's really interested in human languages and linguistics which actually seems to be a minority of the field of natural language processing when it comes down to it . okay so here's some more formalism but i'll skip that as well and i'll give you the idea of what an arc-standard transition-based dependency parser does . 
so what we're gonna do is were going to have a sentence we want to parse i ate fish and so we've got some rules for parsing which is the transition scheme which is written so small you can't possibly read it . so we have two things we have a stack and cartouche around that . and we start off parsing any sentence by putting it on the stack one thing which is our root symbol . okay and the stack has its top towards the right . and then we have this other thing which gets referred to as the buffer . and the buffer is the orange cartouche and the buffer is the sentence that we've got to deal with . and so the thing that we regard as the top of the buffer is the thing to the left off excessive words right . so the top of both of them is sort of at that intersection point between them . okay and so to do parsing under this transition-based scheme there are three operations that we can perform . we can perform they're called shift left-arc and right-arc . so the first one that we're gonna do is shift operation . all we do when we do a shift is we take the word that's on the top of the buffer and put it on the top of the stack . and then we can shift again and we take the word that's on the top of the buffer and put it on the top of the stack . remember the stack the top is to the right . the buffer the top is to the left . okay so there are two other operations left in this arc-standard transition scheme which were left arc and right arc . so what left arc and right arc are gonna do is we're going to make attachment decisions by adding a word as the dependent either to the left or to the right . okay so what we do for left arc is on the stack we say that the second to the top of the stack is a dependent of the thing that's the top of the stack . so i is a dependent of ate and we remove that second top thing from the stack . and so now we've got a stack with just [root] ate on it . 
but we collect up our decisions so we've made a decision that i is a dependent of ate and that's that said a that i am writing in small print off to the right . okay so we still had our buffer with fish on it . so the next thing we're gonna do is shift again and put fish on the stack . and so at that point our buffer is empty we've moved every word on to the stack in our sentence . and we have on it root ate fish okay . so then the third operation we have is right arc and right arc is just the opposite of left arc . so for the right arc operation we say the thing that's on the top of the stack should be made a dependent of the thing that's second to top on the stack . we remove it from the stack and we add an arc saying that . so we right arc so we say fish is a dependent of ate and we remove fish from the stack . we add a new dependency saying that fish is a dependent of ate . and then we right arc one more time so the dependent of the root . so we pop it off the stack and we're just left with root on the stack and we've got one new dependency saying that ate is a dependent of root . so at this point and i'll just mention right in reality there's i left out writing the buffer in a few of those examples there just because it was getting pretty crowded on the slide . but really the buffer is always there right it's not that the buffer disappeared and came back again it's just i didn't always draw it . so but in our end state we've got one thing on the stack and we've got nothing in the buffer . and that's the good state that we want to be in if we finish parsing our sentence correctly . and so we say okay we're in the finished state and we stop . and so that is almost all there is to arc-standard transition based parsing . right so we have a stack and our buffer and then on the side we have a set of dependency arcs a which starts off empty and we add things to . and we have this sort of set of actions which are kind of legal moves that we can make for parsing and so this was how things are . 
so we have a start condition root on the stack buffer is the sentence no arcs . we have the three operations that we can perform . here i've tried to write them out formally so the sort of vertical bar is sort of appends an element to a list operation . so this is sort of having wi as the first word on the buffer it's written the opposite way around for the stack because the head's on the other side . and so we can sort of do this shift operation of moving a word onto the stack and these two arc operations add a new dependency . and then removing one word from the stack and our ending condition is one be the root and an empty buffer . and so that's sort of the formal operations . so the idea of transition based parsing is that you have this sort of set of legal moves to parse a sentence in sort of a shift reduced way . i mean this one i referred to as arc-standard cuz it turns out there are different ways you can define your sets of dependencies . the one we'll use for the assignment and one that works pretty well . so i've told you the whole thing except for one thing which is this just gives you a set of possible moves . it doesn't say which move you should do when . and so that's the remaining thing that's left . and i have a slide on that . okay so the only thing that's left is to say gee at any point in time like we were here at any point in time you're in some configuration right . you've got certain things on there certain things in your buffer you have some set of arcs that you've already made . and which one of these operations do i do next . that nivre proposed is well what we should do is just build a machine learning classifier . since we have a tree bank with parses of sentences we can use those parses of sentences to see which sequence of operations would give the correct parse of a sentence . i am not actually gonna go through that right now . 
you can sort of work out deterministically the sequence of shifts and reducers that you need to get that structure . and it's indeed unique right that for each tree structure there's a sequence of shifts and left arcs and right arcs that will give you the right structure . so you take the tree you read off the correct operation sequence and therefore you've got a supervised classification problem . say in this scenario what you should do next is you should shift and so you're then building a classified to try to predict that . so in the early work that started off with nivre and others in the mid 2000s this was being done with conventional so maybe an svm maybe a perceptron a kind of maxent / soft max classifiers various things but sort of some classified that you're gonna use . so if you're just deciding between the operations shift left arc right arc you have got at most three choices . occasionally you have less because if there's nothing left on the buffer you can't shift anymore so then you'd only have two choices left maybe . but something i didn't mention when i was showing this is when i added to the arc set i didn't only say that fish is an object of ate . i said the dependency is the object of ate . and so if you want to include dependency labels the standard way of doing that is you just have sub types of left arc and right arc . if you have a approximately 40 different dependency labels . as we will in assignment two and in universal dependencies . you actually end up with the space of 81 way classification . names like left arc as an object . or left arc as an adjectival modifier . for the assignment you don't have to do that . for the assignment we're just doing un-type dependency trees . which sort of makes it a bit more scalable and easy for you guys . so it's only sort of a three way in most real applications it's really handy to have those dependency labels . and then what do we use as features . 
well in the traditional model you sort of looked at all the words around you . you saw what word was on the top of the stack . what was the part of speech of that word . what was the first word in the buffer . maybe it's good to look at the thing and what word and part of speech it is . so you're looking at a bunch of words . you're looking at some attributes of those words such as their part of speech . and that was giving you a bunch of features . which are the same kind of classic categorical sparse features of and people were building classifiers over that . so yeah the question is are most treebanks annotated with part of speech . we've barely talked about part of speech so far things like living things nouns and verbs . so the simplest way of doing dependency parsing as you're first writing a part of speech tag it or assign parts of speech to words . and then you're doing the syntactic structure of dependency parsing over a sequence of word part of speech tag pairs . though there has been other work part of speech tag prediction at the same time . which actually has some advantages because you can kind of explore . since the two things are associated you can get some advantages from doing it jointly . okay on the simplest possible model which was what nivre started to explore . you just took the next word ran your classifier . and said that's the object of the verb what's the next word . and you went along and just made these decisions . 
now you could obviously think gee maybe if i did some more searching and explore different alternatives i could do a bit better . and the answer is yes you can . so there's a lot of work in dependency parsing . which uses various forms of beam search where you explore different alternatives . and if you do that it gets a ton slower . and gets a teeny bit better in terms of your performance results . okay but especially if you start from the greediest end or you have a small beam . the secret of this type of parsing is it gives you extremely fast linear time parsing . because you're just going through your corpus no matter how big . so when people like prominent search engines in suburbs south of us want to parse the entire content of the web . they use a parser like this because it goes super fast . and so what was shown was these kind of greedy dependencies parses . their accuracy is slightly below the best dependency parses possible . but their performance is and the fact that they're sort of so fast and scalable . more than makes up for their teeny performance decrease . okay so then for the last few minutes i now want to get back to neural nets . okay so where are we at the moment . so at the moment we have a configuration where we have a stack and a buffer and parts of speech or words . and as we start to build some structure . the things that we've taken off we can kind of sort of think of them as starting to build up a tree as we go . 
as i've indicated with that example below . so the classic way of doing that is you could then say okay well we've got all of these features . like top of stack is word good or top of stack is word bad top of stack's part of speech as adjective . when you've got a combination of positions and words and parts of speech . you very quickly find that the number of features you have in your model extremely extremely large . but you know that's precisely how these kinds of parses were standardly made in the 2000s . so you're building these huge machine learning classifiers over sparse features . and commonly you even had features that were conjunctions of things . so you had features like the second word on the stack is has . and its tag is present tense verb . and the top word on the stack is good . and things like that would be one feature . and that's where you easily get into the ten million plus features . so even doing this already worked quite well . but the starting point from going on is saying well it didn't work completely great . that we wanna do better than that . and we'll go on and do that in just a minute . but before i do that i should mention just the evaluation of dependency parsing . evaluation of dependency parsing is actually very easy . cuz since for each word we're saying what is it a dependent of . 
that we're sort of making choices of what each word is a dependent of . which we get from our tree bank which is the gold thing . we're sort of essentially just counting how often we are right . and so there are two ways that that's commonly done . one way is that we just look at the arrows and ignore the labels . and that's often referred to as the uas measure unlabeled accuracy . or we can also pay attention to the labels . and say you're only right if and that's referred to as the las the labelled accuracy score . so the question is don't you have waterfall effects if you get something you do get some of that . because yes one decision will it's typically not so bad . because even if you mis-attach something like a prepositional phrase attachment . you can still get right all of the attachments inside noun phrase that's inside that prepositional phrase . and i mean actually dependency parsing evaluation suffers much less than doing cfg parsing which is worse in that respect . okay i had one slide there which i think i should skip . okay i'll skip on to neural ones . okay so people could build quite good machine learning dependency parsers on these kind of categorical features . but nevertheless there was a problems of doing that . so problem #1 is the features were just super sparse . that if you typically might have a tree bank that is an order about a million words and if you're then trying which are kinda different combinations of configurations . not surprisingly a lot of those configurations you've seen once or twice . 
so you just don't have any accurate model of what happens in different configurations . you just kind of getting these weak feature weights and crossing your fingers and hoping for the best . now it turns out that modern machine learning crossing your fingers works pretty well . but nevertheless you're suffering a lot from sparsity . okay the second problem is you also have an incompleteness problem because lots of configurations you'll see it run time will be different configurations that you just never happened to see the configuration . word on the stack and the top word of the stack speech or something . any kind of word pale i've only seen a small fraction of them . lot's of things you don't have features for . the third one is a little bit surprising . it turned out that when you looked at these symbolic dependency parsers and you ask what made them slow . what made them slow wasn't running your svm or your dot products in your logistic regression or things like that . all of those things were really fast . what these parsers were ending up spending 95% of their time doing is just computing these features and looking up their weights because you had to sort of walk around the stack and the buffer and sort of put together . a feature name and then you had to look it up in some big hash table to get a feature number and a weight for it . and all the time is going on that so even though there are linear time that slowed them down a ton . so in a paper in 2014 danqi and i developed this alternative where we said well let's just replace that all so that way we can have a dense compact feature representation and do classification . we'll have a relatively modest we'll use that to decide our next action . and so i want to spend the last few minutes sort of showing you how that works and this is basically question two of the assignment . okay and basically just to give you the headline this works really well . so this was sort of the outcome the first parser maltparser . 
so it has pretty good uas and las and it had this advantage that it was really fast . when i said that's been the preferred method i give you some contrast in gray . so these are two of the graph base parsers . so the graph based parsers have been somewhat more accurate but they were kind of like two orders in magnitude slower . so if you didn't wanna parse much stuff than you wanted accuracy you'd use them . but if you wanted to parse the web no one use them . and so the cool thing was that by doing this as neural network dependency parser we were able to get much better accuracy . we were able to get accuracy that was virtually as good as the best graph-based parsers at that time . and we were actually about to build a parser that works significantly faster than maltparser because of the fact that it wasn't spending all this time doing feature combination . it did have to do more vector matrix multiplies of course but that's a different story . okay so how did we do it . well so our starting point was distributed representation . so we're gonna use distributed representations of words . so similar words have close by vectors we've seen all of that . we're also going to use part in our pos we use part-of-speech tags and dependency labels . and we also learned distributed representations for those . that's kind of a cool idea cuz it's also the case that parts of speech some are more related than others . so if you have a fine grain part-of-speech set where you have plural nouns and proper names as different parts of speech from nouns singular you want to say so we also had distributed representations for those . so now we have the same kind of configuration . we're gonna run exactly the same transition based dependency parser . 
so the configuration is no different at all . but what we're going to extract from it is the starting point . just like nivre's maltparser but then what we're gonna do is for each of these positions like top of stack second top of stack buffer etc . words as sort of a 50 or 100 dimensional word vector representation of the kind that we've talked about . and so we get those representations for the different words as vectors and then what we're gonna do is just concatenate those into one longer vector . so any configuration of the parser is just being represented as the longest vector . well perhaps not that long our vectors are sort of more around 1000 not 10 million yeah . sorry the dependency of right the question is what's this dependency on feeding as an input . the dependency i'm feeding here as an import is when i previously built some arcs that are in my arc set i'm thinking maybe it'll be useful to use those arcs as well to help predict the next decision . so i'm using previous decisions on arcs as well to predict my follow-up decisions . okay so how do i do this . and this is essentially what you guys are gonna build . from my configuration i take things out of it . i get there embedding representations and i can concatenate them together and that's my input layer . i then run that through a hidden layer is a neural network feedforward neural network i then have from the hidden layer i've run that through a softmax layer and i get an output layer which is a probability distribution of my different actions in the standard softmax . and of course i don't know what any of these numbers are gonna be . so what i'm gonna be doing is i'm going to be using cross-entropy error and then back-propagating down to learn things . and this is the whole model and it learns super well and it produces a great dependency parser . i'm running a tiny bit short of time but let me just i think i'll have to rush this but i'll just say it . so non-linearities we've mentioned we haven't said very much about them and i just want to say a couple more something like a softmax . 
you can say that using a logistic function gives you a probability distribution . and that's kind of what you get in generalized linear models and statistics . in general though you want to say that . having these non-linearities sort of let's us do function approximation by putting together these various neurons that have some non-linearity . we can sorta put together little pieces like little wavelets to do functional approximation . and the crucial thing to notice is you have to use some non-linearity right . deep networks are useless unless you put something in between the layers right . if you just have multiple linear layers they could just be collapsed down into one product of linear transformations affine transformations is just an affine transformation . so deep networks without non-linearities do nothing okay . and so we've talked about logistic non-linearities . a second very commonly used non-linearity is the tanh non-linearity which is tanh is normally written a bit differently . but if you sort of actually do your little bit of math tanh is really the same as a logistic just sort of stretched and moved a little bit . and so tanh has the advantage that it's sort of symmetric around zero . and so that often works a lot better if you're putting it in the middle of a new neural net . but in the example i showed you earlier and for what you guys will be using for the dependency parser the suggestion to use for the first layer is this linear rectifier layer . and linear rectifier non-linearities are kind of freaky . they're not some interesting curve at all . linear rectifiers just map things to zero if they're negative and then linear if they're positive . and when these were first introduced i thought these were kind of crazy . i couldn't really believe that these were gonna work and do anything useful . 
the bigger your group the more we expect from the project . and you have to also write out exactly what each person in the project has done . you can actually use any kind of open source library and code that you want . but if you just take kaldi which is a speech recognition system and you say i did speech recognition . and then really all you did was download the package and run it then that's not very impressive . so the more you use the more you also have to be careful and say exactly what parts you actually implemented . and in the code you also have to submit your code so that we understand what you've done and the results are real . so this year we do want some language in there . last year i was a little more open . it could be the language of music and so on now . so we've got to have some natural language in there yeah . but other than that that can be done quite easily so projects you might want to do . and if you have a more theoretically inclined project where you really are just faking out some clever way of doing a sarcastic ready to sent or using different kinds of optimization functions . about leading the class to then as long as you at least applied it in one experiment to a natural language processing data set that would still be a pretty cool project . so you can also apply it to genomics data and to text data if you wanna have a little bit of that flavor . but there is gonna be at least one experiment where you apply it to a text data set . all right so now let's walk through the different kinds of projects that you might wanna consider and what might be entailed in such project to give you an idea . unless there are any other questions around the organization of the projects deadlines and so on . so let's start with the kind of simplest and all the other ones are sort of bonuses on top of that simple kind of project . and this is actually i think generally good advice not just for a class project but in general how to apply a deep learning algorithm to any kind of problem whether in academia or in industry or elsewhere . 
so let's assume you want to apply an existing neural network to an existing task . so you want to be able to take a long document and summarize into a short paragraph . now step one after you define your task is you have to define your dataset . and that is actually sadly in many cases in both industry and in academia an incredibly time intensive problem . to that is you just search for there's some people who've worked in summarization before . the nice thing is if you use an existing data set for instance from the document understanding conference duc here then other people have already applied some algorithms to it you'll have some base lines you know what kind of metric or evaluation is reasonable versus close to random . and so on cuz sometimes that's not always obvious . we don't always us just accuracy for instance . so in that case using an existing academic data set gets rid of a lot of complexity . however it is really fun if you actually come up with your own kind of dataset too . so maybe you're really excited about food and you want to prowl yelp or use a yelp dataset for restaurant review or something like that . so however when you do decide to do that you definitely have to check in with your mentor or with chris and me and others . because i sadly have seen several projects in the last couple of years where people have this amazing idea . and then they spent 80% of the time on their project on a web crawler getting not blocked from ip addresses writing multiple ip addresses having multiple machines and crawling . sometimes it's just the document they were hoping to get and crawl it's just a 404 page . and then they realize html and they filter that . and before you know it it's like they have like three more days left to do any deep learning for nlp . and so it has happened before so don't fall into that trap . if you do decide to do that check with us and try to before the milestone deadline . for sure have the data set ready so you can actually do deep learning for nlp cuz sadly we just can't give you a good grade for a deep learning for nlp class if you spend 95% of your time writing a web crawler and explaining your data set . 
so in this case for instance you might say all right i want to use wikipedia . you can actually download sort of already pre-crawled versions of it . maybe you want to say my intro paragraph is the summary of the whole rest of the article . not completely crazy to make that assumption but really you can be creative in this part . you can try to connect it to your own research or your own job if your a [inaudible] student or just any kind of interest that you have . to time it's really fun nlp combine with language of music with natural language and so on . so you can be creative here and we kind of value a little bit of the creativity this is like a task of data set we had never seen before and you actually gain some interesting linguistic insights or something . that is the cool part of the project right . any questions around defining a data set . all right so then you wanna define your metric . for instance you have maybe let's say you did something simpler like restaurant star rating classification . this is a review and i want to classify if this a four star review or a one star review or a two or three . and now you may have a class distribution where this is one star this is two stars three and four and now the majority are three . maybe that you troll kind of funny and are three star reviews . so this is just like number and maybe 90% of the things you called are in the third class . and then you write your report you're super excited it was a new data set you did well you crawled it quickly . and then all you give us is an accuracy metric so accuracy is total correct divided by total . and now let's say your accuracy is 90% . it's 90% accurate 90% of the cases gives you the ride star rating . you're essentially overfit to your dataset and your evaluation metric was completely bogus . 
it's hard to know whether they basically could have implemented a one line algorithm that's just as accurate as yours which is just no matter what the input return three . so hard to give a good grade on that and it's a very tricky trap to fall into . i see it all the time in industry and for young researchers and so on . so in this case you should've used does anybody know what kind f1 that's right . so and we'll go through some of these as we go through the class but it's very important to define your metric well . now for something as tricky as summarization this isn't where you're really just like this is the class this is the final answer . you have to actually either extract or generate a longer sequence . and there are a lot of different bleu's n-gram overlap or rouge share which is a recall-oriented understudy for gisting evaluation which essentially is just a metric to weigh differently how many n-grams are correctly overlapping between a human generated summary . for instance your wikipedia paragraph number one and whatever output your algorithm gives . so rouge is the official metric for summarization in different sub-communities and nop have their own metrics and it's important that you know what you're optimizing . so the machine translation for instance you might use bleu scores bleu scores are essentially also a type of n-gram overlap metric . if you have a skewed data set you wanna use f1 . and in some cases you can just use accuracy . and this is generally useful even if you're in industry and later in life you always wanna know what metric you're optimizing . it's hard to do well if you don't know the metric that you're optimizing for both in life and deep learning projects . all right so let's say you defined your metric now you need to split your dataset . and it's also very important step and it's also something that you can easily make sort of honest mistakes . again in advantage of taking pre-existing academic dataset is that in many cases it's already pre-split but not always . and you don't wanna look at your 1 week before the deadline . so let's say you have downloaded a lot of different articles and now you basically have 100% of some articles you wanna summarize . 
and normal split would be take 80% for training you take 10% for your validation and your development . the validation split or the development split or dev split or various other terms . and 10% for your final test split . and so the final one you ideally get a sense of how your algorithm would work in real life on data you've never seen before you didn't try to chew on your model like how many layers should i use how wide should each layer be . you'll try a lot of these things we'll describe these in the future . but it's very important to correctly split and why do i make such a fuss about that . well there too you might make mistakes . so let's say you have unused text and let's say you crawled it in such a way there's a lot of mistakes that you can make if you try to predict the soft market for instance don't do that it doesn't work . but in many cases you might say or there some temporal sequence . and now you basically have all your dataset and the perfect thing to do is actually do it like this you take 80% of let's say month january to may or something and then your final test split is from november . that way you know there's no overlap . but maybe you made a mistake and you said well i crawled it this way but so as sample an article from here and one from here and one from here . and then the random sample goes to the 80% of my training data . and now the test data and the development data might actually have some overlap . cuz if you're depending on how you chose your dataset maybe the another article which just like a slight addition like some update to an emerging story . and now the summary is almost exact same but the input document just changed a tiny bit . and you have one article in your training set and another one in your test set . but the test set article is really only one extra paragraph on an emerging story and the rest is exactly the same . so now you have an overlap of your training and your testing data . and so in general if this is your training data and this should be your test data . 
it should be not overlapping at all . and whenever you do really well you run your first experiment and you get 90 f1 . and things look just too good to be true sadly in many cases they are and you made some mistake where maybe your test set had some overlap for instance with your training data . it's very important to be a little paranoid about that when your first couple of experiments turn out just to be too good to be true . that can mean either your training your task is too simple or you made a mistake in splitting and defining your dataset . all right any questions around defining a metric or your dataset yeah . so if we split it temporally wouldn't we learn a different distribution . that is correct we would learn a different distribution these are non-stationary . and that is kinda true for a lot of texts but if you ideally when you built a deep learning system for an lp you want it to built it so that it's robust . it's robust to sum such changes over time . and you wanna make sure that when you run it in a real world setting on something you've never seen before it's doing something it will still work . and this was the most realistic way to capture how well it would work in real life . would it be appropriate to run both experiments as in both where you subsample randomly and then you subsample temporally for your . you could do that and the intuitive thing that is likely going to happen is if you sample randomly from all over the place then you will probably do better than if you have this sort of more strict kind of split . but running an additional experiment will rarely ever get you points subtracted . you can always run more experiments and we're trying really hard to help you get computing infrastructure and cloud compute . so you don't feel restricted with the number of experiments you run . all right now number 5 establish a baseline . so you basically wanna implement the simplest model first . this could just be a very simple logistic regression on unigrams or bigrams . 
then compute your metrics on your train data and your development data so overfitting or underfitting . let's say your loss is very very low on training . you do very well on training but you don't do very well on testing then you're in an over fitting regime . if you do very well on training and well on testing you're done you're happy . but if your training loss can't be lower so you're not even doing well on your training that often means your so it's very important to compute both the metrics on your training and your development split . and then and this is something we value a lot in this class too . and it's something very important for you in both research and industries like you wanna analyze your errors carefully for that baseline . and if the metrics are amazing and there are no errors you're done . probably a problem was too easy and you may wanna restart unless it's really a valuable problem for the world . and then maybe you can just really describe it carefully and you're done too . it is very important to not just go in and add lots of bells and whistles that you'll learn about in the next couple of weeks in this class and create this monster of a model . you want to start with something simple sanity check make sure you didn't make mistakes in splitting your data . you have the right kind of metric . and in many cases it's a good indicator for how successful your final project is if you can get this baseline in the first half of the quarter . cuz that means you figured out a lot of these potential issues here . and you kind of have your right data set . you know what the metric is you know what you're optimizing and everything is good . so try to get to this point as quickly as possible . cuz that is also not as interesting and you can't really use that much knowledge from the class . and now you can implement some existing neural network model that we taught you in class . 
for instance this window-based model if your task is named entity recognition . you can compute your metric again on your train and dev set . hopefully you'll see some interesting patterns such as usually train neural nets is quite easy in a sense that we lower the loss very well . and then we might not generalize as well in the development set . and then you'll play around with regularization techniques . and don't worry if some of the stuff i'm saying now is kind of confusing . if you want to do this we'll walk you through that as we're mentoring you through the project . and that's why each project has to have an assigned mentor that we trust . all right then you analyze your very important be close to your data . you can't give too many examples usually ever . and this is kind of the minimum bar for this class . so if you've done this well and there's an interesting dataset then your project is kind of in a safe haven . once you have a metric and everything looks good we still want you to visualize the kind of data even if it's a known data set . we wanted you to visualize it collect summary statistics . it's always good to know the distribution if you have different kinds of classes . you want to again very important look at the errors that your model is making . cuz that can also give you intuitions of what kinds of patterns can your deep learning algorithm not capture . maybe you need to add a memory component or maybe you need to have longer temporal kind of dependencies and so on . those things you can only figure out if you're close to your data and you look at the errors that your baseline models are making . and then we want you to analyze also different hyperparameters . 
a lot of these models have lots of choices . did we add the sigmoid to that score or is the second layer 100 dimensional or 200 dimensional . should we use 50 dimensional word vectors or 1000 dimensional word vectors . there are a lot of choices that you make . and it's really good in your first couple projects to try more and and sometimes if you're running out of time and only so much so many experiments you can run we can help you and use our intuition to guide you . but it's best if you do that a little bit yourself . and once you've done all of that now you can try different model variants and you'll soon see a lot of these kinds of options . we'll talk through all of them in the class . so now another kind of class project is you actually wanna implement a new fancy model . those are the kinds of things that will put you into potentially writing an academic paper peer review and at a conference and so on . the tricky bit of that is you kinda have to do all the other steps that i just described first . and then on top of that you know the errors that you're making . and now you can gain some intuition of why the existing models are flawed . and you come up with your own new model . if you do that you really wanna be in close contact with your mentor and some researchers unless you're a researcher yourself and you earned your phd . but even then you should chat with us from the class . you want to basically try to set up an infrastructure such that you can iterate quickly . you're like maybe i should add this new layer type to this part of my model . you want to be able to quickly iterate and see if that helps or not . so it's important and actually require a fair amount of software engineering skills to set up efficient experimental frameworks that and again you want to start with simple models and then go to more and more complex ones . 
so for instance in summarization you might start with something super simple like just average all your word vectors in the paragraph . and then do a greedy search of generating one word at a time . or even greedily searching for just snippets from the existing article in wikipedia and you're just copying certain snippets over . and then stretch goal is something more advanced would be lets you actually generate that whole summary . and so here are a couple of project ideas . but again we'll post the whole list of them with potential mentors from the nop group and the vision group and various other groups inside stanford . sentiment is also a fun data set . you can look at this url here for one of the preexisting data sets that a lot of people have worked on . all right so next week we'll look at some fun and fundamental linguistic tasks like syntactic parsing . and then you'll learn tensorflow and have some great tools under your belt . again with cs224n natural language processing with deep learning . so you're in for a respite or a change of pace today . so for today's lecture what we're principally going to look at is syntax grammar and dependency parsing . so my hope today is to teach you in one lecture enough about dependency grammars and parsing that you'll all be able to do the main part of assignment 2 successfully . so quite a bit of the early part of the lecture is giving a bit of background about syntax and dependency grammar . and then it's time to talk about a particular kind of dependency grammar transition-based also dependency parsing transition-based dependency parsing . and then it's probably only in the last kind of 15 minutes or so of the lecture that we'll then get back into specifically neural network content . talking about a dependency parser that danqi and i wrote a couple of years ago . okay so for general reminders i hope you're all really aware that assignment 1 is due today . and i guess by this stage you've either made good progress or you haven't . 
but to give my good housekeeping reminders i mean it seems like every year there are people that sort of blow lots of late days on the first assignment for no really good reason . and that isn't such a clever strategy [laugh] . so hopefully [laugh] you are well along with the assignment and can aim to hand it in before it gets to the weekend . okay then secondly today is also the day that the new assignment comes out . till the start of next week but we've got it up ready to go . and so that'll involve a couple of new things and in some respects probably for much of it you might not want to start it until after next tuesday's lecture . so two big things will be different for that assignment . big thing number one is we're gonna do assignment number two using tensorflow . and that's the reason why quite apart from exhaustion from assignment one why you probably you don't wanna start it on the weekend is because on tuesday tuesday's lecture's gonna be an introduction to tensorflow . so you'll really be more qualified then to start it after that . and then the other big different thing in assignment two is we get into natural language processing content . in particular you guys are going to build neural dependency parsers and the hope is that you can learn about everything that you need to know to do that today . the readings on the website if you don't get quite everything straight from me . we're going to sort of post hopefully tomorrow or on the weekend a kind of an outline of what's in assignment four so you can have sort of a more informed meaningful choice between whether you want to do assignment four or the area of assignment four if you do it is going to be question answering over the squad dataset . but we've got kind of a page and a half description to explain what that means so you can look out for that . but if you are interested in we'll encourage people to come and meet with one of the final project mentors or find some other well qualified person around here to be a final project mentor . so what we're wanting is that sort of everybody has met with their final project mentor before putting in an abstract . and that means it'd be really great for people to get started doing that as soon as possible . i know some of you have already talked to various of us . for me personally i've got final from 1 to 3 pm so i hope some people will come by for those . 
and again sort of as richard mentioned not everybody can possible have richard or me as the final project mentor . and besides there's some really big advantages of having some of the phd student tas as final project mentors . cuz really for things like spending time hacking on tensorflow they get to do it much more than i do . and so danqi kevin ignacio arun that they've had tons of experience doing nlp research using deep learning . and so that they'd also be great mentors and look them up for their final project advice . the final thing i just want to touch on is we clearly had a lot of problems i realize at keeping up and coping with people in office hours and queue status has just i'm sorry that that's been kind of difficult . i mean honestly we are trying to work and work out ways that we can do this better and we're thinking of sort of unveiling a few changes for doing things for the second assignment . if any of you peoples have any better advice as to how things could be organized so that they could work better feel free to send a message on piazza with suggestions of ways of doing it . i guess yesterday i ran down percy liang and said percy percy how do you do it for cs221 . do you have some big secrets to do this better . but unfortunately i seem to come away with no big secrets cuz he sort of said: "we use queue status and we use the huang basement" what else are you meant to do . so i'm still looking for that divine insight [laugh] that will tell me how to get this so if you've got any good ideas feel free to share . but we'll try to get this as much better under control as we can for the following weeks . okay any questions or should i just go into the meat of things . all right so what we're going to want to do today is work out how to put structures over sentences in some human language . all the examples i'm going to show is for english but in principle the same techniques you can apply for any language where these structures are going to sort of reveal how the sentence is made up . so that the idea is that sentences and parts of sentences have some kind of structure and there are sort of regular ways that people put sentences together . so we can sort of start off with very simple things that aren't yet sentences like "the cat" and "a dog" and they seem to kind of have a bit of structure . we have an article or what linguists often call a determiner that's followed by a noun . and then well for those kind of phrases which get called noun you can kind of make them bigger and there are sort of rules for how you can do that . 
so you can put adjectives in between the article and the noun . you can say the large dog or a barking dog or a cuddly dog and things like that . and well you can put things like what i call prepositional phrases after the noun so you can get things like "a large dog in a crate" or something like that . and so traditionally what linguists and natural language processors have wanted to do is describe the structure of human languages . and they're effectively two key tools that people have used to do this and one of these key tools and i think in general the only one you have seen a fraction of is to use what in computer science terms what is most commonly referred to as context free grammars which are often referred to by linguists as phrase structure grammars . and is then referred to as the notion of constituency and so for that what we are doing is writing these context free grammar rules and the least if you are standford undergrad or something like that . i know that way back in 103 you spent a whole lecture learning about context-free grammars and their rules . so i could start writing some rules that might start off saying a noun phrase and go to a determiner or a noun . then i realized that noun phrases would get a bit more complicated . and so i came up with this new rule that says- noun phrase goes to terminal optional adject of noun and then optional prepositional phrase wherefore prepositional phrase that's a preposition followed by another noun phrase . because i can say a crate or a large crate . or a large crate by the door . even further and i could say you know a large barking dog by the door in a crate . so then i noticed wow i can put in multiple adjectives there and i can stick on multiple prepositional phrases so i'm using that star the kinda clingy star that you also see see in regular expressions to say that you can have zero or any number of these . and then i can start making a bigger thing like talk to the cuddly dog . and well now i've got a verb followed by a prepositional phrase . and so i can sort of build up a constituency grammar . so that's one way of organizing the structure of sentences and you know in 20th dragging into 21st century america this has been i mean it's what you see mainly in your intro cs class when you get taught about regular languages and context free languages and context sensitive languages . hierarchy where noam chomsky did not actually invent the chomsky hierarchy to torture cs under grads with formal content to fill the scs 103 class . the original purpose of the chomsky hierarchy was actually to understand the complexity of human languages and to make arguments about their complexity . 
sorry it's also dominated sorta linguistics in america in the last 50 years through the work of noam chomsky . but if you look more broadly than that this isn't actually the dominate form of syntactic description that is being used for understanding of the structure of sentences . so there is this other alternative view of linguistic structure which is referred to as dependency structure and what your doing with dependency structure . is that you're describing the structure of a sentence by taking each word and saying what it's a dependent on . so if it's a word that kind of modifies or is an argument of another word that you're saying it's a dependent of that word . so barking dog barking is a dependent of dog because it's of a modifier of it . large barking dog large is a modifier of dog as well so it's a dependent of it . and dog by the door so the by the door is somehow a dependent of dog . and we're putting a dependency between words and we normally indicate those dependencies with arrows . structures over sentences that say how they're represented as well . and when right in the first class i gave examples of ambiguous sentences . a lot of those ambiguous sentences we can think about in terms of dependencies . so do you remember this one scientists study whales from space . and well why is it an ambiguous headline . well it's ambiguous because there's sort of two possibilities . so in either case there's the main verb study . and it's the scientist that's studying that's an argument of study the subject . and it's the whales that are being studied so that's an argument of study . but the big difference is then what are you doing with the from space . you saying that it's modifying study or are you saying it's modifying whales . 
quickly read the headline it sounds like it's the bottom one right . but [laugh] what the article was meant to be about was really that they were being able to use satellites to track the movements of whales . and so it's the first one where the from space is modifying . and so thinking about ambiguities of sentences can then be thought about many of them in terms of these dependency structures as to what's modifying what . and this is just a really common thing in natural language because these kind of questions of what modifies what really dominate a lot of questions of interpretation . so here's the kind of sentence you find when you're reading the wall street journal every morning . the board approved its acquisition by royal trustco limited of toronto for $27 a share at its monthly meeting . and as i've hopefully indicated by the square brackets if you look at the structure of this sentence it sort of starts off as subject verb object . the board approved its acquisition and then everything after that is a whole sequence of prepositional phrases . by royal trustco ltd of toronto for $27 a share at its monthly meeting . and well so then there's the question of what's everyone modifying . so the acquisition is by by royal trustco ltd is modifying the thing that immediately precedes that . and of toronto is modifying the company royal trustco limited so that's modifying the thing that comes immediately preceeding it . so you might think this is easy everything just modifies the thing that's coming immediately before it . so what's for $27 a share modifying . yeah so that's modifying the acquisition so then we're jumping back a few candidates and saying is modifying acquisition and then actually at it's monthly meeting . that wasn't the toronto the royal trustco ltd or the acquisition that that was when the approval was happening so that jumps all the way back up to the top . so in general the situation is that if you've got some stuff like a verb and getting these prepositional phrases . well the prepositional phrase can be modifying either this noun phrase or the verb . but then when you get to the second prepositional phrase . 
well there was another noun phrase inside this prepositional phrase . it can be modifying this noun phrase that noun phrase or the verb phrase . and then we get to another one . and you don't get sort of a completely free choice cuz you do get a nesting constraint . so once i've had for $27 a share referring back to the acquisition the next prepositional phrase has to in general refer to either the acquisition or approved . i say in general because and i'll actually talk about that later . but most of the time in english it's true . you have to sort of refer to the same one or further back so you get a nesting relationship . but i mean even if you obey that nesting relationship the result is that you get an exponential number of ambiguities in a sentence based on in the number of prepositional phrases you stick on the end of the sentence . and so the series of the exponential series you get of these catalan numbers . and so catalan numbers actually show up in a lot of places in that is somehow sort of similar if you're putting these constraints in you get catalan series . so are any of you doing cs228 . yeah so another place the catalan series turns up is that when you've got a vector graph and you're triangulating it the number of ways that you can triangulate your vector graph is also giving you catalan numbers . okay so human languages get very ambiguous . and we can hope to describe them on the basis of sort of looking at these dependencies . the other important concept i wanted to introduce at this point is this idea of full linguistics having annotated data in the form of treebanks . this is probably a little bit small to see exactly . but what this is is we've got sentences . these are actually sentences that come off yahoo answers . and what's happened is human beings have sat around and drawn in the syntactic structures of these sentences as dependency graphs and those things we refer to as treebanks . 
and so a really interesting thing that's happened starting around 1990 is that people have devoted a lot of resources to building up these kind of annotated treebanks and various other kinds of annotated linguistic resources that we'll talk about later . now in some sense from the viewpoint of sort of modern machine learning in 2017 that's completely unsurprising because all the time what we do is say we want labelled data so we can take our supervised classifier and chug on it and get good results . but in many ways it was kind of a surprising thing that happened which is sort of different to the whole of the rest of history right . cuz for the whole of the rest of the history it was back in this space of well to describe linguistic structure what we should be doing is writing grammar rules that describe what happens in linguistic structure . where here we're no longer even attempting to write grammar rules . we're just saying give us some sentences . and i'm gonna diagram these sentences and show you what their structure is . and tomorrow give me a bunch more and i'll diagram them for you as well . and if you think about it in a way that initially seems kind of a crazy thing to do cuz it seems like just putting structures over sentences one by one seems really really inefficient and slow . whereas if you're writing a grammar you're writing this thing that generalizes right . the whole point of grammar is that you're gonna write this one small finite grammar . and it describes an infinite number of sentences . and so surely that's a big labor saving effort . but slightly surprisingly but maybe it makes sense in terms of what's happened in machine learning that it's just turned out to be kind of super successful this building of explicit annotated treebanks . and it ends up giving us a lot of things . and i sort of mention a few of their advantages here . first it gives you a reusability of labor . but the problem of human beings handwriting grammars is that they tend to in practice be almost unreusable because everybody does it differently and has their idea of the grammar . and people spend years working on one and no one else ever uses it . where effectively these treebanks have been a really reusable tool that lots of people have then built on top of to build all kinds of natural language processing tools of part of speech taggers and parsers and things like that . 
they've also turned out to be a really useful resource actually for linguists because they give a kind of real languages are spoken complete with syntactic analyses that you can do all kinds of quantitative linguistics on top of . it's genuine data that's broad coverage when people just work with their intuitions as to what are the grammar rules of english . and so this is actually a better way to find out all of the things that actually happened . for anything that's sort of probabilistic or machine learning it gives some sort of not only what's possible but how frequent it is and what other things it tends to co-occur with and all that kind of distributional information that's super important . and crucially crucially crucially and we'll use this for assignment two it's also great because it gives you a way to evaluate any system that you built because this gives us what we treat as ground truth gold standard data . and then we can evaluate any tool on how good it is at reproducing those . and what i wanted to do now is sort of go through a bit more carefully for sort of 15 minutes what are dependency grammars and dependency structure . so we've sort of got that straight . i guess i've maybe failed to say yeah . i mentioned there was this sort of constituency context-free grammar viewpoint and the dependency grammar viewpoint . and what we're doing for assignment two is all dependencies . we will get back to some notions of constituency and phrase structure . you'll see those coming back in later classes in a few weeks' time . but this is what we're going to be doing today . and that's not a completely random choice . it's turned out that unlike what's happened in linguistics in most of the last 50 years in the last decade in natural language processing it's essentially been swept by the use of dependency grammars that people have found dependency grammars just a really suitable framework on which to build semantic representations to get out the kind of understanding of language that they'd like to get out easily . they enable the building of very fast efficient parsers as i'll explain later today . and so in the last sort of ten years you've just sort of seen this huge sea change in natural language processing . whereas if you pick up a conference volume around the 1990s it was basically all phrase structure grammars and one or two papers on dependency grammars . and if you pick up a volume now what you'll find out is that of the papers they're using syntactic representations dependency representations . 
phrase structure what's the phrase structure grammar that's exactly the same as the context-free grammar when a linguist is speaking . okay so what does a dependency syntax say . so the idea of dependency syntax is to say that the sort of model of syntax is we have relationships between lexical items words and only between lexical items . they're binary asymmetric relations which means we draw arrows . so the whole there is a dependency analysis of bills on ports and senator brownback republican of kansas . okay so that's a start normally hen we do dependency parsing we do a little bit more than that . so typically we type the dependencies by giving them a name for some grammatical relationship . so i'm calling this the subject and it's actually a passive subject . and then this is an auxiliary modifier republican of kansas is an appositional phrase that's coming off of brownback . and so we use this kind of typed dependency grammars . and interestingly i'm not going to go through it but there's sort of some interesting math that if you just have this although it's notationally very different from context-free grammar to a restricted kind of context-free grammar with one addition . but things become sort of a bit more different once you put in a typing of the dependency labels where i wont go into that in great detail right . so a substantive theory of dependency grammar for a language we're then having to make some decisions . so what we're gonna do is when we between two things and i'll just mention a bit more terminology . so we have an arrow and its got what we called the tail end of the arrow i guess . and the word up here is sort of the head . so bills is an argument of submitted were is an auxiliary modifier of submitted . and so this word here is normally referred to as the head or the governor or the superior or sometimes even the regent . and then the word at the other end of the arrow the pointy bit i'll refer to as the dependent but other words that you can sometimes see are modifier inferior subordinate . some people who do dependency grammar really get into these classist notions of superiors and inferiors but i'll go with heads and dependents . 
okay so the idea is you have a head of a clause and then the arguments of the dependence . and then when you have a phrase like by senator brownback republican of texas . being taken as brownback and then it's got words beneath it . and so one of the main parts of dependency grammars at the end of the day as to which words are heads and which words are then the dependents of the heads of any particular structure . so in these diagrams i'm showing you here the ones i showed you back a few pages here is analysis according to universal dependencies . so universal dependencies is a new tree banking effort which i've actually been very strongly involved in . that sort of started a couple of years ago and there are pointers in both earlier in the slides and on the website if you wanna go off and learn a lot about universal dependencies . i mean it's sort of an ambitious attempt to try and have a common dependency representation that works over a ton of languages . i could prattle on about it for ages and if by some off chance there's time at the end of the class i could . but probably there won't be so i won't actually tell you a lot about that now . but i will just mention one thing that probably you'll notice very quickly . and we're also going to be using this representation in the assignment that's being given out today the analysis of universal dependencies treats prepositions sort of differently to what you might have seen else where . if you've seen any many accounts of english grammar or heard references in some english classroom to have prepositions having objects . in universal dependencies prepositions don't have any dependents . of like they were case markers if you know any language like german or latin or hindi or something that has cases . so that the by is sort of treated as if it were a case marker of brownback . so this sort of a bleak modifier of by senator brownback . brownback here as the head with the preposition as sort of like a case marking dependent of by . and that was sort of done to get more parallelism across different languages of the world . other properties of old dependencies normally dependencies form a tree . 
so there are formal properties that goes along with that . that means that they've got a single-head they're acyclic and they're connected . so there is a sort of graph theoretic properties . yeah i sort of mentioned that really dependencies have dominated most of the world . the famous first linguist was panini who wrote his grammar of sanskrit really most of the work that panini did was kind of on sound systems and make ups of words phonology and morphology when we mentioned linguistic levels in the first class . and he only did a little bit of work on the structure of sentences . but the notation that he used for structure of sentences was essentially a dependency grammar of having word relationships being marked as dependencies . yeah so the question is well compare cfgs and pcfgs and do they dependency grammars look strongly lexicalized they're between words and does that makes it harder to generalize . i honestly feel i just can't do justice to that question right now if i'm gonna get through the rest of the lecture . but i will make two comments so i mean there's certainly the natural way to think of dependency grammars they're strongly lexicalized you're drawing relationships between words . whereas the simplest way of thinking of context-free grammars is you've got these rules in terms of categories like . noun phrase goes to determiner noun optional prepositional phrase . and so that is a big difference . but it kind of goes both ways . so normally when actually natural language processing people wanna work with context-free grammars they frequently lexicalize them so they can do more precise probabilistic prediction and vice versa . if you want to do generalization and dependency grammar you can still use at least notions of parts of speech to give you a level of generalization as more like categories . but nevertheless the kind of natural ways of sort of turning them into probabilities and machine learning models are quite different . though on the other hand there's sort of some results or sort of relationships between them . but i would think i'd better not go on a huge digression . that means to rather than just have categories like noun phrase to have categories like a noun phrase headed by dog and so it's lexicalized . 
let's leave this for the moment though please okay . okay so that's panini and there's a whole big history right . so essentially for latin grammarians what they did for the syntax of latin again not very developed . they mainly did morphology but it was essentially a dependency kind of analysis that was given . there was sort of a flowering of arabic grammarians in the first millennium and they essentially had a dependency grammar . i mean by contrast i mean really kind of context free grammars and constituency grammar only got invented almost in the second half of the 20th century . i mean it wasn't actually chomsky that originally invented them there was a little bit of earlier work in britain but only kind of a decade before . so there was this french linguist lucien tesniere he is often referred to as the father he's got a book from 1959 . dependency grammars have been very popular and more sorta free word order languages cuz notions sort of like context-free languages like english that have very fixed word order but a lot of other languages of the world have much freer word order . and that's often more naturally described with dependency grammars . interestingly one of the very first natural language parsers developed in the us was also a dependency parser . so david hays was one of the first us computational linguists . and one of the founders of the association for computational linguistics which is our main kind of academic association where we publish our conference papers etc . and he actually built in 1962 a dependency parser for english . okay so a lot of history of dependency grammar . so couple of other fine points to note about the notation . people aren't always consistent in which way they draw the arrows . i'm always gonna draw the arrows so they point go from a head to a dependent which is the direction which tesniere drew them . but there are some other people who draw the arrows the other way around . so they point from the dependent to the head . 
and so you just need to look and see what people are doing . the other thing that's very commonly done is you stick this pseudo-word wall or some other name like that and that kind of makes the math and formalism easy because then every sentence starts with root and something is a dependent of root . or turned around the other way if you think of what parsing a dependency grammar means is for every word in the sentence you're going to say what is it a dependent of because if you do that you're done . you've got the dependency structure of the sentence . and what you're gonna want to say is well it's either gonna be a dependent of some other word in the sentence or it's gonna be a dependent of the pseudo-word root which is meaning it's the head of the entire sentence . specifics of dependency parsing but the kind of thing that you should think about is well how could we decide which words are dependent on what . and there are certain various information sources that we can think about . so yeah it's sort of totally natural with the dependency representation to just think about word relationships . and that's great cuz that'll fit super well with what we've done already in distributed word representations . so actually doing things this way just fits well with a couple of tools we already know how to use . we'll want to say well discussion of issues is that a reasonable attachment as lexical dependency . and that's a lot of the information that we'll actually use but information that we'd also like to use . dependency distance so sometimes there are dependency relationships and sentences between words that is 20 words apart when you got some big long sentence and you're referring that back to some previous clause but it's kind of uncommon . most of dependencies are pretty short distance so you want to prefer that . many dependencies don't sort of so if you have the kind of dependencies that occur inside noun phrases like adjective modifier they're not gonna cross over a verb . it's unusual for many kinds of dependencies to cross over a punctuation so it's very rare to have a punctuation between a verb and a subject and things like that . so looking at the intervening material gives you some clues . and the final source of information is sort of thinking about heads and thinking how likely they are to have to dependence in what number and on what sides . so the kind of information there is right a word like the is basically not likely to have any dependents at all anywhere . so you'd be surprised if it did . 
words like nouns can have dependents and they can have quite a few dependents but they're likely to have some kinds like determiners and adjectives on the left other kinds like prepositional phrases on the right verbs tend to have a lot of dependence . so different kinds of words have different kinds of patterns of dependence and so there's some information there we could hope to gather . okay yeah i guess i've already said the first point . in principle it's kind of really easy . so we're just gonna take every make a decision as to what word or root this word is a dependent of . and we do that with a few constraints . so normally we require that only one word can be a dependent of root and we're not going to allow any cycles . and if we do both of those things we're guaranteeing that we make the dependencies of a tree . and normally we want to make out dependencies a tree . and there's one other property i then wanted to mention that if you draw your dependencies as i have here so all the dependencies been drawn as loops above the words . it's different if you're allowed to put some of them below the words . whether you can draw them like this . so that they have that kind of nice none of them cross each other . or whether like these two that i've got here where they necessarily cross each other and i couldn't avoid them crossing each other . and what you'll find is in most languages certainly english the vast majority of dependency relationships have a nesting structure relative to the linear order . and if a dependency tree is fully nesting it's referred to as that you can lay it out in this plane and have sort of a nesting relationship . but there are few structures in english where you'd get things that aren't nested and yet crossing . and this sentence is a natural example of one . so i'll give a talk tomorrow on bootstrapping . so something that you can do with noun modifiers especially if they're kind of long words like bootstrapping or techniques of bootstrapping is you can sort of move them towards the end of the sentence right . 
i could have said i'll give a talk on bootstrapping tomorrow . but it sounds pretty natural to say i'll give a talk tomorrow on bootstrapping . but this on bootstrapping is still modifying the talk . and so that's referred to by linguists as right extraposition . and so when you get that kind of rightward movement of phrases you then end up with these crossing lines . and that gives you what's referred to as a non-projective dependency tree . so importantly it is still a tree if you sort of ignore the constraints of linear order and you're just drawing it out . there's a graph in theoretical computer science right it's still a tree . it's only when you consider this extra thing of the linear order of the words that you're then forced to have the lines across . and so that property which you don't actually normally see mentioned in theoretical computer science discussions of graphs is then this property that's referred to projectivity . recover the order of the words from a dependency tree . so given how i've defined dependency trees the strict answer is no . they aren't giving you the order at all . now in practice people write down the words of a sentence in order and have these crossing brackets right crossing arrows when they're non-projective . and obviously it's a real thing about languages that they have linear order . but as i've defined dependency structures you can't actually recover okay one more slide before we get to the intermission . yeah so in the second half of the class i'm gonna tell you about a method of dependency parsing . i just wanted to say very quickly there are a whole bunch about doing dependency parsing . so one very prominent way of doing dependency parsing is using dynamic programming methods which is normally what people have used for constituency grammars . a second way of doing it is to use graph algorithms . 
so a common way of doing dependency parsing you're using mst algorithms minimum spanning tree algorithms . and that's actually a very successful way of doing it . you can view it as kind of a constraint satisfaction problem . but the way we're gonna look at it is this fourth way which is these days most commonly called transition based-parsing though when it was first introduced it was quite often called deterministic dependency parsing . and the idea of this is that we're kind of greedily going to decide which word each word is a dependent of guided by having a machine learning classifier . so one way of thinking about this is so far in this class we only have two hammers . one hammer we have is word vectors and you can do a lot with word vectors . and the other hammer we have is how to build a classifier as a feedforward neural network with a softmax on top so it classifies between two various classes . and it turns out that if those are your two hammers you can do dependency parsing this way and it works really well . and so therefore that's a great approach for using in assignment two . and it's not just a great approach for assignment two . actually method four is the dominant way these days of doing dependency parsing because it has extremely good properties of scalability . that greedy word there is a way of saying this is a linear time algorithm which none of the other methods are . so in the modern world of web-scale parsing it's sort of become most people's favorite method . so i'll say more about that very soon . but before we get to that we have ajay doing our research spotlight with one last look back at word vectors . okay awesome so let's take a break from dependency parsing and talk about something we should know a lot about word embeddings . so for today's research highlight we're gonna be talking about a paper titled improving distributional similarity with lessons learned from word embeddings . and it's authored by levy et al . so in class we've learned two major paradigms for generating word vectors . 
we've learned count-based distributional models which essentially utilize a co-occurrence matrix to produce your word vectors . and we've learned svd which is singular value decomposition . and we haven't really talked about ppmi . but in effect it still uses that co-occurrence matrix to produce sparse vector encodings for words . we've also learned neural network-based models which you all should have lots of experience with now . and specifically we've talked about skip-gram negative sampling as well as cbow methods . and glove is also a neural network-based model . and the conventional wisdom is that neural network-based models are superior to count-based models . however levy et al proposed that hyperparameters and system design choices are more important not the embedding algorithms themselves . and so essentially what they do in their paper is propose a slew of hyperparameters that when implemented and tuned over the count-based distributional models pretty much approach the performance of neural network-based models to the point where there's no consistent better choice across the different tasks that they tried . and a lot of these inspired by these neural network-based models such as skip-gram . so if you recall which you all should be very familiar with this we have two hyperparameters in skip-gram . we have the number of negative samples that we're sampling as well as the unigram distributions smoothing exponent which we fixed at 3 over 4 . but it can be thought of as more of a system design choice . and these can also be transferred over to the account based variants . and i'll go over those very quickly . so the single hyper parameter that levy et al . impact in performance was context distribution smoothing which is analogous to the unigram distribution smoothing constant 3 over 4 here . and in effect they both achieved the same goal which is to sort of smooth out your distribution such that you're penalizing rare words . which interestingly enough the optimal alpha they found was exactly 3 over 4 which is the same as the skip-gram unigram smoothing exponent . 
they were able to increase performance by an average of three points across tasks on average which is pretty interesting . and they also propose shifted pmi which i'm not gonna get into the details of this . but this is analogous to the negative sampling choosing the number of negative samples in skip-gram . and they've also proposed a total of eight hyperparameters in total . and we've described one of them which is the context distribution smoothing . and this is a lot of data and if you're confused that's actually the conclusion that i want you to arrive at because clearly there's no trend here . so what the authors did was take all four methods tried three different windows and then test all the models across a different task . and those are split up into word similarity and analogy task . and all of these methods are tuned to find the best hyperparameters to optimize for the performance . and the best models are bolded and as you can see there's no consistent best model . so in effect they're challenging the popular convention that neural network-based models are superior to the count-based models . however there's a few things to note here . number one adding hyperparameters is never a great thing because now you have to train those hyperparameters which takes time . number two we still have the issues with count-based distributional models specifically with respect to the computational as well as performing svd . so the key takeaways here is that the paper challenges the conventional wisdom that neutral network-based models are in fact superior to count-based models . number two while model design is important hyperparameters are also key for achieving good results . so this implies specifically to doing a project instead of assignment four . you might implement the model but that might only take you half way there . some models to find your optimal hyperparameters might take days or even weeks to find . and finally my personal interest within ml is in deep representation learning . 
and this paper specifically excites displays that there's still lots of work to be done in the field . and so the final takeaway is challenge the status quo . okay and so now we're back to learning about how to build a transition based dependency parser . so maybe in 103 or compilers class formal languages class there's this notion of shift reduced parsing . how many of you have seen shift reduced parsing somewhere . they just don't teach formal languages the way they used to in the 1960s in computer science anymore . okay well i won't assume that you've all seen that before . okay essentially what we're going to have is i'll just skip these two slides and go straight to the pictures . because they will be much more understandable . mention the picture on this page that's a picture of joakim nivre . so joakim nivre is a computational linguist in uppsala sweden who pioneered this approach of transition based dependency parsing . he's one of my favorite computational linguists . i mean he was also an example going along with what ajay said of sort of doing something unpopular and out of the mainstream and proving that you can get it to work well . so at an age when everyone else was trying to build sort of fancy dynamic program parsers joakim said nono what i'm gonna do is i'm just gonna take each successive word and have a straight classifier that says what to do with that . and go onto the next word completely greedy cuz maybe that's kinda like what humans do with incremental sentence processing and i'm gonna see how well i can make that work . and it turned out you can make it work really well . so and then sort of transition based parsing has grown to this sort of really widespread dominant way of doing parsing . so it's good to find something different to do if everyone else is doing something it's good to think of something else that might be promising that you got an idea from . and i also like joakim because he's actually another person that's really interested in human languages and linguistics which actually seems to be a minority of the field of natural language processing when it comes down to it . okay so here's some more formalism but i'll skip that as well and i'll give you the idea of what an arc-standard transition-based dependency parser does . 
so what we're gonna do is were going to have a sentence we want to parse i ate fish and so we've got some rules for parsing which is the transition scheme which is written so small you can't possibly read it . so we have two things we have a stack and cartouche around that . and we start off parsing any sentence by putting it on the stack one thing which is our root symbol . okay and the stack has its top towards the right . and then we have this other thing which gets referred to as the buffer . and the buffer is the orange cartouche and the buffer is the sentence that we've got to deal with . and so the thing that we regard as the top of the buffer is the thing to the left off excessive words right . so the top of both of them is sort of at that intersection point between them . okay and so to do parsing under this transition-based scheme there are three operations that we can perform . we can perform they're called shift left-arc and right-arc . so the first one that we're gonna do is shift operation . all we do when we do a shift is we take the word that's on the top of the buffer and put it on the top of the stack . and then we can shift again and we take the word that's on the top of the buffer and put it on the top of the stack . remember the stack the top is to the right . the buffer the top is to the left . okay so there are two other operations left in this arc-standard transition scheme which were left arc and right arc . so what left arc and right arc are gonna do is we're going to make attachment decisions by adding a word as the dependent either to the left or to the right . okay so what we do for left arc is on the stack we say that the second to the top of the stack is a dependent of the thing that's the top of the stack . so i is a dependent of ate and we remove that second top thing from the stack . and so now we've got a stack with just [root] ate on it . 
but we collect up our decisions so we've made a decision that i is a dependent of ate and that's that said a that i am writing in small print off to the right . okay so we still had our buffer with fish on it . so the next thing we're gonna do is shift again and put fish on the stack . and so at that point our buffer is empty we've moved every word on to the stack in our sentence . and we have on it root ate fish okay . so then the third operation we have is right arc and right arc is just the opposite of left arc . so for the right arc operation we say the thing that's on the top of the stack should be made a dependent of the thing that's second to top on the stack . we remove it from the stack and we add an arc saying that . so we right arc so we say fish is a dependent of ate and we remove fish from the stack . we add a new dependency saying that fish is a dependent of ate . and then we right arc one more time so the dependent of the root . so we pop it off the stack and we're just left with root on the stack and we've got one new dependency saying that ate is a dependent of root . so at this point and i'll just mention right in reality there's i left out writing the buffer in a few of those examples there just because it was getting pretty crowded on the slide . but really the buffer is always there right it's not that the buffer disappeared and came back again it's just i didn't always draw it . so but in our end state we've got one thing on the stack and we've got nothing in the buffer . and that's the good state that we want to be in if we finish parsing our sentence correctly . and so we say okay we're in the finished state and we stop . and so that is almost all there is to arc-standard transition based parsing . right so we have a stack and our buffer and then on the side we have a set of dependency arcs a which starts off empty and we add things to . and we have this sort of set of actions which are kind of legal moves that we can make for parsing and so this was how things are . 
so we have a start condition root on the stack buffer is the sentence no arcs . we have the three operations that we can perform . here i've tried to write them out formally so the sort of vertical bar is sort of appends an element to a list operation . so this is sort of having wi as the first word on the buffer it's written the opposite way around for the stack because the head's on the other side . and so we can sort of do this shift operation of moving a word onto the stack and these two arc operations add a new dependency . and then removing one word from the stack and our ending condition is one be the root and an empty buffer . and so that's sort of the formal operations . so the idea of transition based parsing is that you have this sort of set of legal moves to parse a sentence in sort of a shift reduced way . i mean this one i referred to as arc-standard cuz it turns out there are different ways you can define your sets of dependencies . the one we'll use for the assignment and one that works pretty well . so i've told you the whole thing except for one thing which is this just gives you a set of possible moves . it doesn't say which move you should do when . and so that's the remaining thing that's left . and i have a slide on that . okay so the only thing that's left is to say gee at any point in time like we were here at any point in time you're in some configuration right . you've got certain things on there certain things in your buffer you have some set of arcs that you've already made . and which one of these operations do i do next . that nivre proposed is well what we should do is just build a machine learning classifier . since we have a tree bank with parses of sentences we can use those parses of sentences to see which sequence of operations would give the correct parse of a sentence . i am not actually gonna go through that right now . 
you can sort of work out deterministically the sequence of shifts and reducers that you need to get that structure . and it's indeed unique right that for each tree structure there's a sequence of shifts and left arcs and right arcs that will give you the right structure . so you take the tree you read off the correct operation sequence and therefore you've got a supervised classification problem . say in this scenario what you should do next is you should shift and so you're then building a classified to try to predict that . so in the early work that started off with nivre and others in the mid 2000s this was being done with conventional so maybe an svm maybe a perceptron a kind of maxent / soft max classifiers various things but sort of some classified that you're gonna use . so if you're just deciding between the operations shift left arc right arc you have got at most three choices . occasionally you have less because if there's nothing left on the buffer you can't shift anymore so then you'd only have two choices left maybe . but something i didn't mention when i was showing this is when i added to the arc set i didn't only say that fish is an object of ate . i said the dependency is the object of ate . and so if you want to include dependency labels the standard way of doing that is you just have sub types of left arc and right arc . if you have a approximately 40 different dependency labels . as we will in assignment two and in universal dependencies . you actually end up with the space of 81 way classification . names like left arc as an object . or left arc as an adjectival modifier . for the assignment you don't have to do that . for the assignment we're just doing un-type dependency trees . which sort of makes it a bit more scalable and easy for you guys . so it's only sort of a three way in most real applications it's really handy to have those dependency labels . and then what do we use as features . 
well in the traditional model you sort of looked at all the words around you . you saw what word was on the top of the stack . what was the part of speech of that word . what was the first word in the buffer . maybe it's good to look at the thing and what word and part of speech it is . so you're looking at a bunch of words . you're looking at some attributes of those words such as their part of speech . and that was giving you a bunch of features . which are the same kind of classic categorical sparse features of and people were building classifiers over that . so yeah the question is are most treebanks annotated with part of speech . we've barely talked about part of speech so far things like living things nouns and verbs . so the simplest way of doing dependency parsing as you're first writing a part of speech tag it or assign parts of speech to words . and then you're doing the syntactic structure of dependency parsing over a sequence of word part of speech tag pairs . though there has been other work part of speech tag prediction at the same time . which actually has some advantages because you can kind of explore . since the two things are associated you can get some advantages from doing it jointly . okay on the simplest possible model which was what nivre started to explore . you just took the next word ran your classifier . and said that's the object of the verb what's the next word . and you went along and just made these decisions . 
now you could obviously think gee maybe if i did some more searching and explore different alternatives i could do a bit better . and the answer is yes you can . so there's a lot of work in dependency parsing . which uses various forms of beam search where you explore different alternatives . and if you do that it gets a ton slower . and gets a teeny bit better in terms of your performance results . okay but especially if you start from the greediest end or you have a small beam . the secret of this type of parsing is it gives you extremely fast linear time parsing . because you're just going through your corpus no matter how big . so when people like prominent search engines in suburbs south of us want to parse the entire content of the web . they use a parser like this because it goes super fast . and so what was shown was these kind of greedy dependencies parses . their accuracy is slightly below the best dependency parses possible . but their performance is and the fact that they're sort of so fast and scalable . more than makes up for their teeny performance decrease . okay so then for the last few minutes i now want to get back to neural nets . okay so where are we at the moment . so at the moment we have a configuration where we have a stack and a buffer and parts of speech or words . and as we start to build some structure . the things that we've taken off we can kind of sort of think of them as starting to build up a tree as we go . 
as i've indicated with that example below . so the classic way of doing that is you could then say okay well we've got all of these features . like top of stack is word good or top of stack is word bad top of stack's part of speech as adjective . when you've got a combination of positions and words and parts of speech . you very quickly find that the number of features you have in your model extremely extremely large . but you know that's precisely how these kinds of parses were standardly made in the 2000s . so you're building these huge machine learning classifiers over sparse features . and commonly you even had features that were conjunctions of things . so you had features like the second word on the stack is has . and its tag is present tense verb . and the top word on the stack is good . and things like that would be one feature . and that's where you easily get into the ten million plus features . so even doing this already worked quite well . but the starting point from going on is saying well it didn't work completely great . that we wanna do better than that . and we'll go on and do that in just a minute . but before i do that i should mention just the evaluation of dependency parsing . evaluation of dependency parsing is actually very easy . cuz since for each word we're saying what is it a dependent of . 
that we're sort of making choices of what each word is a dependent of . which we get from our tree bank which is the gold thing . we're sort of essentially just counting how often we are right . and so there are two ways that that's commonly done . one way is that we just look at the arrows and ignore the labels . and that's often referred to as the uas measure unlabeled accuracy . or we can also pay attention to the labels . and say you're only right if and that's referred to as the las the labelled accuracy score . so the question is don't you have waterfall effects if you get something you do get some of that . because yes one decision will it's typically not so bad . because even if you mis-attach something like a prepositional phrase attachment . you can still get right all of the attachments inside noun phrase that's inside that prepositional phrase . and i mean actually dependency parsing evaluation suffers much less than doing cfg parsing which is worse in that respect . okay i had one slide there which i think i should skip . okay i'll skip on to neural ones . okay so people could build quite good machine learning dependency parsers on these kind of categorical features . but nevertheless there was a problems of doing that . so problem #1 is the features were just super sparse . that if you typically might have a tree bank that is an order about a million words and if you're then trying which are kinda different combinations of configurations . not surprisingly a lot of those configurations you've seen once or twice . 
so you just don't have any accurate model of what happens in different configurations . you just kind of getting these weak feature weights and crossing your fingers and hoping for the best . now it turns out that modern machine learning crossing your fingers works pretty well . but nevertheless you're suffering a lot from sparsity . okay the second problem is you also have an incompleteness problem because lots of configurations you'll see it run time will be different configurations that you just never happened to see the configuration . word on the stack and the top word of the stack speech or something . any kind of word pale i've only seen a small fraction of them . lot's of things you don't have features for . the third one is a little bit surprising . it turned out that when you looked at these symbolic dependency parsers and you ask what made them slow . what made them slow wasn't running your svm or your dot products in your logistic regression or things like that . all of those things were really fast . what these parsers were ending up spending 95% of their time doing is just computing these features and looking up their weights because you had to sort of walk around the stack and the buffer and sort of put together . a feature name and then you had to look it up in some big hash table to get a feature number and a weight for it . and all the time is going on that so even though there are linear time that slowed them down a ton . so in a paper in 2014 danqi and i developed this alternative where we said well let's just replace that all so that way we can have a dense compact feature representation and do classification . we'll have a relatively modest we'll use that to decide our next action . and so i want to spend the last few minutes sort of showing you how that works and this is basically question two of the assignment . okay and basically just to give you the headline this works really well . so this was sort of the outcome the first parser maltparser . 
so it has pretty good uas and las and it had this advantage that it was really fast . when i said that's been the preferred method i give you some contrast in gray . so these are two of the graph base parsers . so the graph based parsers have been somewhat more accurate but they were kind of like two orders in magnitude slower . so if you didn't wanna parse much stuff than you wanted accuracy you'd use them . but if you wanted to parse the web no one use them . and so the cool thing was that by doing this as neural network dependency parser we were able to get much better accuracy . we were able to get accuracy that was virtually as good as the best graph-based parsers at that time . and we were actually about to build a parser that works significantly faster than maltparser because of the fact that it wasn't spending all this time doing feature combination . it did have to do more vector matrix multiplies of course but that's a different story . okay so how did we do it . well so our starting point was distributed representation . so we're gonna use distributed representations of words . so similar words have close by vectors we've seen all of that . we're also going to use part in our pos we use part-of-speech tags and dependency labels . and we also learned distributed representations for those . that's kind of a cool idea cuz it's also the case that parts of speech some are more related than others . so if you have a fine grain part-of-speech set where you have plural nouns and proper names as different parts of speech from nouns singular you want to say so we also had distributed representations for those . so now we have the same kind of configuration . we're gonna run exactly the same transition based dependency parser . 
so the configuration is no different at all . but what we're going to extract from it is the starting point . just like nivre's maltparser but then what we're gonna do is for each of these positions like top of stack second top of stack buffer etc . words as sort of a 50 or 100 dimensional word vector representation of the kind that we've talked about . and so we get those representations for the different words as vectors and then what we're gonna do is just concatenate those into one longer vector . so any configuration of the parser is just being represented as the longest vector . well perhaps not that long our vectors are sort of more around 1000 not 10 million yeah . sorry the dependency of right the question is what's this dependency on feeding as an input . the dependency i'm feeding here as an import is when i previously built some arcs that are in my arc set i'm thinking maybe it'll be useful to use those arcs as well to help predict the next decision . so i'm using previous decisions on arcs as well to predict my follow-up decisions . okay so how do i do this . and this is essentially what you guys are gonna build . from my configuration i take things out of it . i get there embedding representations and i can concatenate them together and that's my input layer . i then run that through a hidden layer is a neural network feedforward neural network i then have from the hidden layer i've run that through a softmax layer and i get an output layer which is a probability distribution of my different actions in the standard softmax . and of course i don't know what any of these numbers are gonna be . so what i'm gonna be doing is i'm going to be using cross-entropy error and then back-propagating down to learn things . and this is the whole model and it learns super well and it produces a great dependency parser . i'm running a tiny bit short of time but let me just i think i'll have to rush this but i'll just say it . so non-linearities we've mentioned we haven't said very much about them and i just want to say a couple more something like a softmax . 
you can say that using a logistic function gives you a probability distribution . and that's kind of what you get in generalized linear models and statistics . in general though you want to say that . having these non-linearities sort of let's us do function approximation by putting together these various neurons that have some non-linearity . we can sorta put together little pieces like little wavelets to do functional approximation . and the crucial thing to notice is you have to use some non-linearity right . deep networks are useless unless you put something in between the layers right . if you just have multiple linear layers they could just be collapsed down into one product of linear transformations affine transformations is just an affine transformation . so deep networks without non-linearities do nothing okay . and so we've talked about logistic non-linearities . a second very commonly used non-linearity is the tanh non-linearity which is tanh is normally written a bit differently . but if you sort of actually do your little bit of math tanh is really the same as a logistic just sort of stretched and moved a little bit . and so tanh has the advantage that it's sort of symmetric around zero . and so that often works a lot better if you're putting it in the middle of a new neural net . but in the example i showed you earlier and for what you guys will be using for the dependency parser the suggestion to use for the first layer is this linear rectifier layer . and linear rectifier non-linearities are kind of freaky . they're not some interesting curve at all . linear rectifiers just map things to zero if they're negative and then linear if they're positive . and when these were first introduced i thought these were kind of crazy . i couldn't really believe that these were gonna work and do anything useful . 