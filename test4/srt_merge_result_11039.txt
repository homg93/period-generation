okay well welcome to cs224n in linguistics 284 . thank you for everyone who's here that's involved and also the people who don't fit in here and the people who yeah it's totally amazing the number of people who've signed up to do this class and so in some sense it seems like you don't need any advertisements for why the combination of natural language process and deep learning is a good thing to learn about . but nonetheless today this class is really going to give some of that advertisement so i'm christopher manning . gonna start off by saying a bit of stuff about what natural language processing is and what deep learning is and then after that we'll spend a few minutes on the course logistics . and a word from my co-instructor richard . and then get through some more material on why is language understanding difficult and then starting to do an intro to deep learning for nlp . so we've gotten off to a rocky start today cause i guess we started about ten minutes late because of that fire alarm going off . fortunately there's actually not a lot of hard content in this first lecture . this first lecture is really to explain what an nlp class is and say some motivational content about how and why deep learning is changing the world . that's going to change immediately on the thursday lecture because for the thursday lecture is then we're gonna start with sort of vectors and derivatives and chain rules and all of that stuff . so you should get mentally prepared for that change of level between the two lectures . so natural language processing that's the sort of computer scientist's name for the field . essentially synonymous with computational linguistics which is sort of the linguist's name of the field . and so it's in this intersection of computer science and linguistics and artificial intelligence . where what we're trying to do is get computers to do clever things with human languages to be able to understand and express themselves in human languages so natural language processing counts as a part of artificial intelligence . and there are obviously other important parts of artificial intelligence of doing computer vision and robotics reasoning and so on . but language has had a very special part of artificial intelligence and that's because that language has been this very distinctive properties of human beings and we think and go about the world largely in terms of language . so lots of creatures around the planet have pretty good vision systems but human beings are alone for language . and when we think about how we express our ideas and go about doing things that language is largely our tool for thinking and our tool for communication . so it's been one of the key technologies that people have thought about in artificial intelligence and it's the one that we're going to look at today . 
so our goal is how can we get computers to process or understand human languages in order to perform tasks that are useful . so that could be things like making appointments or buying things or it could be more highfalutin goals of sort of understanding the state of the world . and so this is a space in which there's starting to be a huge amount of commercial activity in various directions some of things like making appointments . a lot of it in the direction of question answering . so luckily for people who do language the arrival of mobile has just been super super friendly in terms of the importance of language has gone way way higher . and so now really all of the huge google assistant facebook and cortana . but what they're furiously doing is putting out products that use natural language to communicate with users . and that's an extremely compelling thing to do . it's extremely compelling on phones because phones have these dinky little keyboards that are really hard to type things on . and a lot of you guys are very fast at texting i know that but really a lot of those problems are much worse for a lot of other people . so it's a lot harder to put in chinese characters than it is to put in english letters . it's a lot harder if you're elderly . it's a lot harder if you've got low levels of literacy . but then there are also being new vistas opening up . so amazon has had this amazing success with alexa which is really shown the utility of having devices that are just ambient in the environment and that again you can communicate with by talking to them . as a quick shout-out for apple i mean really launching siri . it was essentially apple taking the bet on saying we can turn human language into consumer technology that really did set off this arms race every other company is now engaging on . okay i just sort of loosely said meaning . one of the things that we'll talk about more is meaning is a kind of a complex hard thing and it's hard to know what it means to understand fully meaning . at any rate that's certainly a very tough goal which people refer to as ai-complete and it involves all forms of our understanding of the world . 
so a lot of the time when we say understand the meaning we might be happy if we sort of half understood the meaning . and we'll talk about different ways that we can hope to do that . okay so one of the other things that we hope that you'll get in this class is sort of a bit of appreciation for human language and what it's levels are and how it's processed . now obviously we're not gonna do a huge amount of that if you really wanna learn a lot about that . there are lots of classes that you can take in the linguistics department and learn much more about it . but i really hope you can at least sort of get a bit of a high level of understanding . so this is kind of the picture that people traditionally have given for levels of language . and then you're doing phonetic and phonological analysis to understand that speech . and then there's some processing that's done there which has sort of been a bit marginal from a linguistics point of view ocr working out the tokenization of the words . but then what we do is go through a series of processing steps where we work out complex words like incomprehensible it has the in in front and the ible at the end . and that sort of morphological analysis the parts of words . and then we try and understand the structure of sentences that syntactic analysis . so if i have a sentence like 'i sat on the bench' that 'i' is the subject of the verb 'sat' and the 'on the bench' is the location . and that's semantic interpretation's working out the meaning of sentences . but simply knowing the meaning of the words of a sentence isn't understand human language . a lot is conveyed by the context in which language is used . and so that then leads into areas like pragmatics and discourse processing . so in this class where we're gonna spend most of our time is in that middle piece of syntactic analysis and semantic interpretation . and that's sort of bulk of our natural language processing class . we will say a little bit right at the top left where this discussion speech signal analysis . 
and interestingly that was actually the first place where deep learning really proved itself as super super useful for tasks involving human language . okay so applications of natural language processing are now really spreading out thick and fast . and every day you're variously using applications of natural language processing . so at the low level there are things like spell checkings or doing the kind of autocomplete on your phone . so that's a sort of a primitive language understanding task . variously when you're doing web searches your search engine is considering synonyms and things like that for you . and well that's also a language understanding task . interested in is trying to push our language understanding computers up to more complex tasks . so some of the next level up kind of tasks that we're actually gonna want to have computers look at text information be it websites newspapers or whatever . and get the information out of it to actually understand the text well enough that they know what it's talking about to at least some extent . and so that could be things like expecting particular kinds of information like products and their prices or people and what jobs they have and things like that . or it could be doing other related tasks to understanding the document such as working out the reading level or intended audience of the document . or whether this tweet is saying something positive or negative about this person company band or whatever . and then going even a higher level than that what we'd like our computers to be able to do is complete whole level language understanding tasks . and some of the prominent tasks of that kind that we're going to talk about . machine translation going from one human language to another human language . building spoken dialogue systems so you can chat to a computer and have a natural conversation just as you do with human beings . or having computers that can actually exploit the knowledge of the world wikipedia and other sources . intelligently answer questions for you like a know everything human being could . okay and we're starting to see a lot of those things actually being used regularly in industry . 
so every time you're doing a search in little places there are bits of natural language processing and natural language understanding happening . so if you're putting in forms of words with endings your search engine's considering taking them off . synonyms are being considered and things like that . similarly when you're being matched for advertisements . but what's really exciting is that we're now starting to see much bigger applications of natural language processing being commercially successful . so in the last few years there's just been amazing amazing advances in machine translation there have been amazing advances in speech recognition so that we just now get hugely good performance in speech recognition even on our cell phones . products like sentiment analysis they have become hugely commercially important right . it depends on your favorite industries but there are lots of wall street journal firms that every hour of the day are scanning news articles looking for sentiment about companies to make buy and sell decisions . and just recently really over the last 12 months there's been this huge growth of interest in how to build chatbots and dialog agents for and that sort of seems like it's growing to become a huge new industry . so in just a couple of minutes i want to say that corresponding things about deep learning . let me just say a minute about what's special about human language . maybe we'll come back to this but i think it's interesting to have a sense of right at the beginning . so there's an important difference between language and most other kinds of things that people think of when they do signal processing and data mining and so for most things there's just sort of data that's either the world out there . it has some kind of pick up some visual system for it . or someone's sort of buying products at the local safeway . and then someone else is picking up the sales log and saying let me analyze this and see what i can find right . so it's just sort of all this random data and then then someone's trying to make sense of it . so fundamentally human language isn't like that . human language isn't just sort of a massive data exhaust that you're trying to process into something useful . human language almost all of it is that there's some human being who actually had some information they wanted to communicate . 
and they constructed information to other human beings . form of sending a particular message to other people . okay and an amazing fact about human language is it's this very complex system that somehow two three four year old kids amazingly can start to pick it up and use it . so there's something good going on there . another interesting property of language is that language is actually what you could variously call a discrete symbolic or categorical signaling system . so we have words for concepts like rocket or violin . and basically we're communicating with other people via symbols . there are some tiny exceptions for expressive signaling so you can distinguish saying i love it versus i love it . but 99% of the time it's using these symbols to communicate meaning . and presumably that came about in a sort of ee information theory sense . because by having symbols they're very reliable units that can be signaled reliably over a distance . and so that's an important thing to be aware of right . so if symbols aren't just some invention of logic or classical ai . but then when we move beyond that there's actually something interesting going on . so when human beings communicate with language that although what they're wanting to communicate involves symbols . that the way they communicate those symbols is using a continuous substrate . and a really interesting thing about language is you can convey exactly the same message by using different continuous substrates . you can put stuff on a piece of paper and then you have a vision problem . you can also use sign language to communicate . and that's a different kind of continuous substrate . 
so all of those can be used . but there's sort of a symbol underlying all of those different encodings . okay so what the picture we have is that the communication medium is continuous . so the dominant idea in most of science and artificial intelligence was to sort of project the symbol system of language into our brains . and think of brains as symbolic processors . but that doesn't actually seem to have any basis in what brains are like . brains is that they're completely continuous systems as well . and so the interesting idea that's been emerging out of this work in deep learning is to say no what we should be doing is also thinking of our brains as having continuous patterns of activation . and so then the picture we have is that we're going from continuous to symbolic back to continuous every time that we use language . it also points out one of the problems of doing language understanding that we'll come back to a lot of times . so in languages we have huge vocabularies . so languages have tens of thousands of words minimum . and really languages like english with a huge scientific vocabulary have hundreds of thousands of words in them . if you start counting up all of the morphological forms you can argue some languages have an infinite number of words cuz they have productive morphology . but however you count it means we've got this huge problem of sparsity and that's one of the big problems that we're gonna have to deal with . okay now i'll change gears and say a little bit of an intro to deep learning . so deep learning has been this area that has erupted over the sort of this decade . and i mean it's just been enormously enormously exciting how deep learning has succeeded and how it has expanded . so really at the moment it seems like every month you see in the tech news that there's just amazing new improvements that are coming out from deep learning . computer vision systems the next month it's machine translation that's vastly improved . 
the month after that people are working out how to get computers to produce their own artistry that's incredibly realistic . then the month after that people are producing new text-to-speech systems that sound amazingly lifelike . i mean there's just been this sort of huge dynamic of progress . so what is underlying all of that . so well as a starting point deep learning it's part of machine learning . so in general it's this idea of how can we get computers to learn stuff automatically rather than just us having to tell them things and coding by hand in the kind of traditional write computer program to tell it what you want it to do . but deep learning is also profoundly different to the vast majority of what happened in machine learning in the 80s 90s and 00s . and this central difference is that for most of traditional machine learning if i call it that . so this is all of the stuff like decision trees logistic regressions naive bayes support vector machines and any of those sort of things . essentially the way that we did things was what we did was have a human being who looked carefully at a particular problem and worked out what was important in that problem . and then designed features that would be useful features for handling the problem that they would then encode by hand . normally by writing little bits of python code or something like that to recognize those features . they're probably a little bit small to read but over on the right-hand side these are showing some features for finding person names company names and so on in text . and this is just the kind of system i've written myself . so well if you want to know whether a word is a company you'd wanna look whether it was capitalized so it turns out that looking at the words to the left and right would be useful to have features for it turns out that looking at substrings of words is useful cause they're kind of common patterns of letter sequences that indicate names of people versus of names of companies . so you put in features for substrings . if you see hyphens and things that's an indicator of some things . you put in a feature for that . so you keep on putting in features and commonly these kind of systems would end up with millions of hand-designed features . and that was essentially how google search was done until about 2015 as well right . 
they liked the word signal rather than feature . but the way you improved google search was every month some bunch of engineers came up with some new signal . that they could show with an experiment that if you added in these extra features google search got a bit better . and [inaudible] a degree and things would get a bit better . but the thing to think about is well this was advertised as machine learning but what was the machine actually learning . so the human being was learning a lot about the problem right . they were looking at the problem hard doing lots of data analysis developing theories and learning a lot about what was important for this property . it turns out that the only thing the machine was doing was numeric optimization . so once you had all these signals what you're then going to be doing was building a linear classifier . which meant that you were putting a parameter weight in front of each feature . and the machine learning system's job was to adjust those numbers so as to optimize performance . and that's actually something that computers are really good at doing numeric optimization and it's something that human beings are actually less good at . cuz humans if you say here are 100 features each one to maximize performance . well they've got sort of a vague idea but they certainly can't do that as well as a computer can . so that was useful but is doing numeric optimization is that what machine learning means . it doesn't seem like it should be . okay so what we found that in practice machine learning was sort of 90% human beings working out how to describe data and work out important features . and only sort of 10% the computer running this learning numerical optimization algorithm . okay so how does that differ with deep learning . so deep learning works is part of this field that's called representation learning . 
and the idea of representation learning is to say we can just feed to our computers raw signals from the world whether that's visual signals or language signals . and then the computer can automatically by itself come up with good intermediate representations that will allow it to do tasks well . so in some sense it's gonna be inventing its own features in the same way that in the past the human being was inventing the features . so precisely deep learning the real meaning of the word deep learning is the argument that you could of learned representations . and that you'd be able to outperform other methods of learning by having multiple layers of learned representations . that was where the term deep learning came from . nowadays half the time deep learning just means you're using neural networks . and the other half of the time it means there's some tech reporter writing a story and it's vaguely got to do with intelligent computers and all other bets are off . so with the kind of coincidence where sort of deep learning really means neural networks a lot of the time we're gonna be part of that . so what we're gonna focus on in this class is different kinds of neural networks . so at the moment they're clearly the dominant family of ways in which people have reached success in doing deep learning . but it's not the only possible way that you could do it that people have certainly looked at trying to use various other kinds of probabilistic models and other things in deep architectures . and i think that may well be more of that work in the future . what are these neural networks that we are talking about . that's something we'll come back to and talk a lot about both on thursday and next week . i mean you noticed a lot of these neural terminology . i mean in some sense if you're kind of coming from a background of statistics or something like that you could sort of say neural networks they're kind of nothing really more than stack logistic regressions or perhaps more generally kinda stacked generalized linear models . neuroscience in some cases so that's not a big focus on this class at all . but on the other hand there's something very qualitatively different that by the kind of architectures that people are building now for these complex stacking of neural unit architectures you end up with a behavior and a way of thinking and a way of doing things that's just hugely different than anything that was coming before in earlier statistics . we're not really gonna take a historical approach we're gonna concentrate on methods that work well right now . 
if you'd like to read a long history of deep learning though i'll warn you it's a pretty dry and boring history there's this very long arxiv paper by j체rgen schmidhuber that you could look at . okay so why is deep learning exciting . so in general our manually designed features tend to be overspecified incomplete take a long time to design and validate and only get you to a certain level of performance at the end of the day . where the learned features are easy to adapt fast to train and they can keep on learning so that they get to a better level of performance than we've been able to achieve previously . so deep learning ends up providing this sort of very flexible almost universal learning framework which is just great for representing all kinds of information . linguistic information but also world information or visual information . it can be used in both supervised fashions and unsupervised fashions . the real reason why deep learning is exciting to most people is it has been working . so starting from approximately 2010 there were initial successes where deep learning were shown to work far better than any of the traditional machine learning methods that have been used for the last 30 years . but going even beyond that what has just been totally stunning is over the last six or seven years there's just been this amazing ramp in which deep learning methods have been getting better at just an amazing speed . which is actually sort of being maybe i'm biased but i'd actually just say it's unprecedented in terms of seeing a field that has been progressing quite so quickly in its ability to be sort of rolling out better methods of doing things month on month . and that's why you're sort of seeing all of this huge industry excitement new products and you're all here today . so why has deep learning succeeded so brilliantly . and i mean this is actually a slightly more subtle and in some sense not quite so uplifting a tale . because when you look at a lot of the key techniques that we use for deep learning were actually invented in the 80s or 90s . we're using a lot of stuff that was done in the 80s and 90s . and somehow they didn't really take off then . well it turns out that actually some of the difference the difference is just that technological advances have happened that make this all possible . so we now have vastly greater amounts of data available because of our online society where just about everything is available as data . and having vast amounts of data really favors deep learning models . 
in the 80s and 90s there sort of wasn't really enough compute power to do deep learning well . so having sort of several has just made it that we can now build systems that work . i mean in particular there's been this amazing confluence that deep learning has proven to be just super well suited to the kind of parallel vector processing that's available now for very little money in gpus . so there's been this sort of marriage between deep learning and of stuff to have happened . so that's actually quite a lot of what's going on . but it's not the only thing that's going on and it's not the thing that's leading to this sort of things keeping on getting better and better month by month . i mean people have also come up with better ways of learning they've come up with much better ways of doing end-to-end joint system learning . they've come up with much better ways of transferring information between domains and between contexts and things . so there are also a lot of new algorithms and algorithmic advances and they're sort of in some sense the more exciting stuff that we're gonna focus on for more of the time . okay so really the first big breakthrough in deep learning was in speech recognition . it wasn't as widely heralded as the second big breakthrough in deep learning . but this was really the big one that started . george dahl working with geoff hinton started showing on tiny datasets that they could do exciting things with deep neural networks for speech recognition . so george dahl then went off to microsoft and then fairly shortly after that another student from toronto went to google and they started building big speech recognition systems that use deep learning networks . and speech recognition's a problem that's been worked on for decades by hundreds of people . and there was this sort of fairly standardized technology of using gaussian mixture models for the acoustic analysis and hidden markov models and blah blah blah . which people have been honing for decades trying to improve a few percent a year . and what they were able to show was by changing from that to using deep learning models for doing speech recognition that they were immediately able to get just these enormous decreases in word error rate . about a 30% decrease in word error rate . then the second huge example of which ended up being a much bigger thing in terms of everybody noticing it was in the imagenet computer vision competition . 
so in 2012 again students of geoff hinton at toronto set about building a computer vision system of doing imagenet task of classifying objects into categories . and that was again a task that had been run for several years . and performance seemed fairly stalled with traditional computer vision methods and running deep neural networks on gpus that they were able to get an over one-third error reduction in one fell swoop . and that progress is continued through the years but we won't say a lot on that here . okay that's taken me a fair way . so let's stop for a moment and do the logistics and i'll say more about deep learning and nlp . okay so this class is gonna have two instructors . i'm chris manning and i'm a stanford faculty then the other one is richard who's the chief scientist of faith of salesforce and so i'll let him say a minute or two hello . i guess just a brief little bit about myself . in 2014 i graduated i got my phd here with chris and enring in deep learning for nlp . and then almost became a professor but then started a little company built an ad platform did some research . and then earlier last year we got acquired by salesforce which is how i ended up there . i've been teaching cs224d the last two years and super excited to merge to two classes . lectures so you'll see a lot of me . we've got many really wonderful competent great tas for this class . yeah so normally i go through all the tas but there are sort of so many both of them and you that maybe i won't go through them all but maybe they could all just sort of stand up for a minute if you're a ta in the class . they're all in that corner okay yeah so at this point i mean apologies about the room capacity . so the fact of the matter is if this class is being kind of videoed and broadcast this is sort of the largest scpd classroom that they record in . so there's no real choice for this this is the same reason that this is where 221 is and this is where 229 is . but it's a shame that there aren't enough seats for everybody sorry about that . 
it will be available shortly after each class also as a video . in general for the other information look at the website but there's a couple things that i do just wanna say a little bit about prerequisites and work to do . and we'll expect you to know and if you don't know you should start working out what you don't know and what to do about it very quickly . so the first one is we're gonna do the assignments in python so proficiency in python there's a tutorial on the website not hard to learn if you do something else . essentially python has just become the lingua franca of nearly all the deep learning toolkits so that seems the thing to use . it'll start turning up on thursday and even more next week . sort of basic probability and statistics you don't need to know anything fancy about martingales or something i don't either . but you should know the elements of that stuff . and then we're gonna assume you know some fundamentals of machine learning . so if you've done 221 or 229 that's fine . all of that content but we sort of assume that you've seen loss functions and you have some idea about how you do optimization with gradient okay so in terms of what we hope to teach the first thing is an understanding of and ability to use effective modern methods for deep learning . so we'll be covering all the basics but especially an emphasis on the main methods that are being used in nlp which is things like recurrent networks attention and things like that . some big picture understanding of human languages and the difficulties in understanding and producing them . and then the third one is essentially the intersection of those two things . so the ability to build systems for important nlp problems . and you guys will be building some of those for the various assignments . so in terms of the work to be done this is it . and then at the end there's this bigger thing where you sort of have a choice between either you can come up with your own exciting world shattering final project and propose it to us . and we gotta make sure every final project has a mentor which can either be richard or me one of the tas or someone else who knows stuff about deep learning . or else we can give you an exciting project and so there'll be sort of a default final project otherwise known as assignment 4 . 
there's gonna be a final poster session . so every team for the final project you're gonna have teams up to three for the final project has to be at the final poster session . now we thought about having it in our official exam slot but that was on friday afternoon and so we decided people might not like that . so we're gonna have it in the tuesday early afternoon session which is when the language class exams are done . so no offense to languages but we're assuming that none of you are doing first year intensive language classes . or at least you better find a teammate who isn't . note that each assignment has to be handed in within three days so we can grade it . yeah okay yeah so assignment 1 we're gonna hand out on thursday so for that assignment it's gonna be pure python except for using the numpy library which is kinda the basic vector and matrices library . and people are gonna do things it's a really important educational skill that you've actually done things and gotten it to work from scratch . and you really know for are because you've calculated them . and because you've implemented them and you've found that you can calculate derivatives and implement them and the thing does actually learn and work . if you've never done this the whole thing's gonna seem like black magic ever after . so it's really important to actually work through it by yourself . but nevertheless one of what things that's being transforming deep learning is that there are now these very good software packages which actually make it crazily easy to build deep learning models . that you can literally take one of these libraries and sort of write 60 lines of python and you can be training a state-of-the-art deep learning system that will work super well providing you've got the data to train it on . and that's sort of actually been an amazing development over the last year or two . and so for assignments 2 and in particular we're gonna be using tensorflow which is the google deep learning library which is sort of well google's very close to us . but it's also very well engineered and has sort of taken off as the most used library now . but there really are a whole bunch of other good libraries for deep learning . and i mentioned some of them below . 
okay do people have any questions on class organization . or anything else up until now or do i just power on . i'm gonna do is repeat all questions so they'll actually work on the video . so the question is how are our assignments gonna be submitted . they're gonna be submitted electronically online instructions will be on the first assignment . but yeah everything has to be electronic what we use in gradescope for the grading . for written stuff if you wanna hand write it you have to scan it for yourself and submit it online . so the question was are the slides on the website . the slides were on the website before the class began and we're gonna try and keep that up all quarter . so you should just be able to find them cs224n.stanford.edu . yeah so that was on the logistics if you're doing assignment four . it's partly different and partly the same so if you're doing the default assignment four and we'll talk all about final projects in a couple of weeks . you don't have to write a final project proposal or talk to a mentor because we've designed the project for you as a starting off point of the project . but on the other hand otherwise it's the same . so it's gonna be an open ended project in which there are lots of things that you can try to make the system better and we want you to try and we want you to be able to report on what are the different exciting things you've tried whether they did or didn't make your system better . and so we will be expecting people doing assignment four to also write up and present a poster on what they've done . so we've already setup the piazza and we attempted to enroll all the enrolled students so hopefully if you're an involved student there's somewhere in your junk mailbox or in one of those places a copy of a piazza announcement . i think most people maybe especially computer scientist going into this just don't understand why nlp is hard . it's just a sequence of words and they've been dealing with programming languages . and you're just gonna read the sequence the words . 
it turns out it's hard for a bunch of reasons because human languages aren't like programming languages . so human languages are just all ambiguous . programming languages are constructed to be unambiguous that's why they have rules like you can . and else goes with the nearest 'if' and you have to get the indentation right in python . human languages aren't like that so human languages are when there's an 'else' just interpret it with whatever 'if' makes most sense to the hearer . and when we do reference in programming language we use variable names like x and y and this variable . whereas in human languages we say things like this and that and she and you're just meant to be able to figure out from context who's being talked about . but that's a big problem but it's perhaps not even the biggest problem . the biggest problem is that humans use language as an efficient communication system . and the way they do that is by not saying most things right . when you write a program we say everything that's needed to get it to run . where in a human language you leave out most of the program because you think that your listener will be able to work out which code should be there right . so it's sorta more a code the listener is meant to be able to fill in the rest of the program . we kinda actually communicate very fast by human language right . the rate at which we can speak . but the reason why it works efficiently is we can say minimal messages . the rest with their world knowledge common sense knowledge and contextual knowledge of the situation . so as sort of a profound version of why natural language is hard: i really like this xkcd cartoon but you definitely can't read and i can barely read on the computer in front of me . it says actually a lot about why natural language understanding is hard . so the two women speaking to each other . 
one says 'anyway i could care less' and the other one says 'i think you mean you couldn't care less saying you could care less implies you care to some extent' and the other one says 'i don't know' and then continues . we're these unbelievably complicated beings drifting through a void trying in vain to connect with one another by blindly flinging words out in to the darkness . every trace of phrasing and spelling and tone and timing carries countless signals and contexts and subtexts and more . language isn't a formal system of language it's glorious chaos . you can never know for sure what any words will mean to anyone . all you can do is try to get better at guessing how your words affect people . so you have a chance of finding the ones that will make them feel something like you want them to feel . i assume you're giving me tips on how you interpret words because you want me to feel less alone . if so then thank you that means a lot . but if you're just running my sentences passed some mental check list so you can show off how well you know it then i could care less . comic there's actually a lot of profound content there as to what human language understanding is like and what the difficulties of it are . but that's probably a bit hard to do in detail so i'm just gonna show you some you get lots of ambiguities including funny ambiguities in natural language . so here are a couple of here's one of my favorites that came out recently from time magazine . the pope's baby steps on gays no that's not how you meant to interpret this . you're meant to interpret this as the pope's baby steps on gays . so a question i mean why do you get those two interpretations . what is it about human language and english here about english that allows you to have these two interpretations . what are the different things going on . is anyone game to give an explanation of how we okay yeah right . i'll repeat the explanation as i go . 
you started off with saying it was idiomatic and some sense baby steps is sort of an sort of a metaphor an idiom where baby steps is meaning little steps like a baby would take but i mean before you even get to that you can kind of just think a large part of this is just a structural ambiguity which then governs the rest of it . so one choice is that you have this noun phrase of the pope's baby and then you start interpreting it as a real baby . and then steps is being interpreted as a verb . so something we find in a lot of languages including english is the same word can have fundamentally different roles . he and the verbal interpretation verb steps would be being used as a verb . but the other reading is as you you can put nouns together and make noun compounds very freely in english . computer people do it all the time right . as soon as you've got something like disk drive enclosure or network interface hub or something like that you're just nailing nouns together to make big nouns . so you can put together baby and steps as two nouns and make baby steps as a noun phrase . and then you can make the pope's baby steps is a larger noun phrase . and then you're getting this very different interpretation . but simultaneously at the same time you're also changing the meaning of baby . so in one case the baby was this metaphorical baby and then in the other one it's a perhaps counter-factually it's a literal baby . let's do at least one more of that . boy paralyzed after tumor fights back to gain black belt . not how you're meant to read it . you're meant to read it as boy paralyzed after tumor fights back to gain black belt . someone suggested missing punctuation and if to some extent that's true . and to some extent make readings clearer in some cases . but there are lots of places where there are ambiguities in language where it's just not usual standard to put in punctuation to disambiguate . 
and indeed if you're the kind of computer scientist who feels like you want to start putting matching parentheses around pieces of human language to make the unclear interpretation much clearer you're not then a typical language user anymore . yeah so this is sort of the ambiguities are in the syntax of the sentence . so when you have this 'paralyzed' verb of the sentence so . the boy is paralyzed then all of after tumor fights back to gain black belt is then this sort of subordinate clause of saying when it happened . and so then the 'tumor' is or you can have this can also be what's called a passive participle . so it's introducing a participial phrase of 'paralyzed after tumor' . and so that can then be a modifier of the boy in the same way an adjective can young boy fights back to gain black belt . it could be boy paralyzed after tumor fights back to gain black belt . and then it's the boy that's the subject of fights . okay i have on this slide a couple more examples but i think i won't go through them in detail since i'm sort of behind as things are going . okay so what i wanted to get into a little bit of for the last bit of class until my time runs out is to introduce this idea of deep learning and nlp . and so i mean essentially this is combining the two things that we've been talking about so far deep learning and nlp . so we're going to use the ideas of deep learning neural networks representation learning and we're going to apply them to problems in language understanding natural language processing . and so in the last couple of years especially this is just an area that's sorta really starting to take off and just for the rest of today's about what are some of the stuff happening where they're at a very high level and that'll sort of prepare for thursday starting to dive right into the specifics . and so that so there is so different different classifications you can look at . so on the one hand deep learning is being applied to lots of different levels of language that things like speech words syntax semantics . it's been applied to lots of different sort of tools algorithms that we use for natural language processing . so that's things like labeling words for part-of-speech finding person and organization names or coming up with syntactic structures of sentences . and then it's been applied to lots of language applications that put a lot of this together . so things that i've mentioned before like machine translation sentiment analysis dialogue agents . 
and one of the really really interesting things is that deep learning models have been giving a very unifying method of using the same tools and technologies to understand a lot of these problems . so yes there are some specifics of different problems . but something that's been quite stunning in the development of deep learning is that there's actually been a very small toolbox of key techniques which have turned out to be just vastly applicable with enormous accuracy to just many many problems . which actually includes not only many many language problems but also most of the rest of what happens in deep learning whether it's looking at vision problems or applying deep learning through any other kind of signal analysis knowledge representation or anything that you see these few key tools being used to solve all the problems . and what is somewhat embarrassing for human beings part is that typically they're sort of working super well much better than the techniques that human beings had previously slaved on for decades developing without very much customization for different tasks . okay so deep learning and language it all starts off with word meaning and so this is a very central idea gonna develop starting off with the second class . so what we're gonna do with words is say were going to represent a word in particular we're going to represent the meaning of the word . so here's my vector for the word expect . and so i made that whatever it is an 8-dimensional vector i think since that was good for my slide . but really we don't use much that small vectors . so minimally we might use something like 25-dimensional vectors . commonly we might be using something and if we're really going to town because we wanna have the best ever system doing something we might be using a 1000-dimensional vector or something like that . so when we have vectors for words that means we're placing words in a high-dimensional vector space . and what we find out is when we have these methods for learning word vectors from deep learning and place words into these high-dimensional vector spaces these act as wonderful semantic spaces . so words with similar meanings will cluster together in the vector space but actually more than that . we'll find out that there are directions in the vector space that actually tell you about components and meaning . human beings is that they're not very good at looking at high-dimensional spaces . so for the human beings we always have to project down onto two or three dimensions . and so in the background you can see a little bit of a word cloud of a 2d projection of a word vector space which you can't read at all . but we could sort of start to zoom in on it . 
and then you get something that's just about readable . so in one part of the space this is where country words are clustering . and in another part of the space this is where you're seeing verbs clustering . and you're seeing kind of it's grouping together verbs that mean most similarly . so 'come' and 'go' are very similar 'say' and 'think' are similar 'think' and 'expect' are similar . 'expecting' and 'thinking' are actually similar to 'seeing things' a lot of the time because people often use see as an analogy for think . okay so the question is what do the axes in these vector spaces mean . and in some sense the glib answer is nothing . so when we learn these vector spaces well actually we have these 300 d vectors . and they have these axes corresponding to those vectors . and often in practice we do sort of look at some of those elements in along the axes and see if we can interpret them because it's easy to do . but really there's no particular reason to think that elements and meaning should follow those vector lines . in the vector space and so they don't necessarily mean anything . when we wanna do a 2d projection like this what we're then using is some method to try and most faithfully get out some of the main meaning from the high dimensional vector space so we can show it to you . so the simplest method that many of you might have seen before in other places is doing pca doing a principal components analysis . there's another method that we'll get to called t-sne which is kind of a non-linear dimensionality reduction which is commonly used . but these are just to try and give human beings some sense of what's going on . and it's important to realize that any of these low dimensional projections can be extremely extremely misleading right . because they are just leaving out a huge amount of the information that's actually in the vector space . here's i'm just looking at closest words to the word frog . 
i'm using the glove embeddings that we did at stanford and we'll talk about more in the next couple of lectures . so frogs and toad are the nearest words which looks good . but if we then look at these other words that we don't understand it turns out that they're also names for other pretty kinds of frogs . so these word meaning vectors are a great basis of starting to do things . but i just wanna give you a sense for the last few minutes that we can do a lot beyond that . and the surprising thing is we're gonna keep using some of these vectors . so traditionally if we're looking at complex words like uninterested we might just think of them as being made up as morphemes of sort of smaller symbols . but what we're gonna do is say well no . we can also think of parts of words as vectors that represent the meaning of those parts of words . and then what we'll wanna do is build a neural network which can compose the meaning of larger units out of these smaller pieces . that was work that minh-thang luong and richard did a few years ago at stanford . deep learning for is to make syntactic pauses that find out the structure of sentences . so danqi chen who's over there is one of the tas for the class . so something that she worked on a couple of years ago was doing neural network methods for dependency parsing . and essentially if you've seen any of the recent google announcements with their parsey mcparseface and syntax net . that essentially what that's using is a more honed and larger version of the technique that danqi introduced . so once we've got some of the structure of sentences we then might want to understand the meaning of sentences . and people have worked on the meaning of sentences for decades . and i certainly don't wanna belittle other ways of working but in the terms of doing deep learning for nlp in this class i also wanna give a sense of how we'll do things differently . so the traditional way of doing things which is commonly lambda calculus calculus-based semantic theories . 
that you're giving meaning functions for individual words by hand . and then there's a careful logical algebra for how you combine together the meanings of words to get kind of semantic expressions . which have also sometimes been used for programming languages where people worked programming languages . but that's not what we're gonna do here . what we're gonna do is say well if we start off with the meaning of words being vectors we'll make meanings for phrases which are also vectors . and then we have bigger phrases and sentences also have their meaning being a vector . and if we wanna know what the relationships between meanings of sentences or between sentences and the world such as a visual scene the way we'll do that is we'll try to learn a neural network that can make those decisions for us . so we can use it for all kinds of semantics . this was actually one of the pieces of work that richard did while he was a phd student and so this was trying to do a much better careful real meaning representation and understanding of the positive and negative sentiments of sentences by actually working out which parts of sentences have different meanings . so the sentences this movie doesn't care about cleverness wit or any other kind of intelligent humor and the system is actually very accurately able to work out well there's all of this positive stuff down here right . it's all very positive and that's the kind of thing a traditional sentiment analysis system would fall apart on and just say this is a positive sentence . is noticing that there's this movie doesn't care at the beginning and is accurately deciding the overall sentiment for the sentence is negative . okay i'm gonna run out of time so i'll skip a couple of things but things that've been super exciting . so there's this enormous excitement now about trying to build chat bots dialogue agents . of having speech and language understanding interfaces that humans can interact with mobile computers . there's alexa and other things like that with and i think it's fair to say that the state of the technology at the moment is that speech recognition has made humongous advances right . has been going on for decades and as someone involved with language technology i'd been claiming to people from the 1990s no speech recognition is really good . we've worked out really good speech recognition systems . but the fact of the matter is they were sorta not very good and real human beings would not use them if they had any choice because the accuracy was just so low . whereas in the last few years neural network-based deep learning speech recognition systems have become amazingly good . 
i think i mean maybe this isn't true of the young people in this room apart from me . but i think a lot of people don't actually realize how good that they've gotten . because i think that there are a lot of people that try things out in 2012 and decide they're pretty reasonable but not fantastic and so i encourage all of you if you don't regularly use speech recognition to go home and try saying some things to your phone . and i think it's now just amazing how well the speech recognition works . and then your phone has no idea what you're saying and so it says would you like me to google that for you . so the big problem and the centerpiece of the kind of stuff that we're working on in this class is well how can we actually make the natural language understanding equally good . and so that's a big concentration that what we're going to work on . one place that's actually have any of you played with google's inbox program on cell phones . so one cool but very simple example of a deployed deep learning dialogue agent is google inbox's suggested replies . so you having recurrent neural network that's going through the message and is then suggesting three replies to your message to send back to the other person . and you know although there are lots of concerns in that program of sort of they're careful how they're doing it . actually often the replies it comes up with are really rather good . if you're looking to cut down on your email load give google inbox a try and you might find that actually you can reply to quite a bit of your email using it . wanted to mention before finishing was machine translation . so machine translation this is actually when natural language processing started . where natural language processing started was it was the beginning of the cold war . americans and russians alarmed that each other knew too much about something they couldn't understand what people were saying . and coming off of the successes of code breaking in world war ii people thought we can just get our computers to do language translation . and in the early days it worked really terribly and things started to get a bit better in the 2000s and i presume you've all seen kind of classic google translate and that's a lot of half worked . you could sorta get the gist of what it's saying but it still worked very terribly . 
whereas just in the last couple of years really only starting in 2014 there's then started to be use of end-to-end trained deep learning systems to do machine translation which is then called neural machine translation . and it's certainly not the case that all the problems in mt are solved there's still lots of work to do to improve machine translation . but again this is a case in which just overnight replacing the 200 person years of work on google translate with a new deep learning based machine translation system has overnight produced a huge improvement in translation quality . and there was a big long article about that in the new york times magazine a few weeks ago that you might've seen . and so rather than traditional approaches to translation where again just running a big deep recurrent neural network where a source sentence generating vector represent the sentence so far . and then once it's gone to the end of the sentence it then starts to generate so generating words in sequence in the translation kind of neural language models and that is also a key technology that we use in a lot of things that we do . the kind of google inbox recurrent neural network and in the generation side of a neural machine translation system . okay so we've gotten to i just have one more minute and try and get us out of here not too late even though we started late . i mean the final thing i want to say it's just sort of to emphasize the fact the amazing thing that's happening here is it's all vectors right . we're using this for all representations of language whether it's sounds parts of words words sentences conversations they're all getting turned into these real value vectors . and that's something that we'll talk about a lot more . i'll talk about it for word vectors on thursday and richard will talk a lot more about the vectors next time . i mean that's something that appalls many people but i think it's important to realize it's actually something a lot more subtle than many people realize . you could think that there's no structure in this big long vector of numbers . but equally you could say well i could reshape that vector and i could turn into a matrix or a higher order array which we call a tensor . or i could say different parts of it or directions of it represent different kinds of information . it's actually a very flexible data structure with huge representational capacity and that's what deep learning systems really take advantage of in all that they do . so in terms of what we're gonna do today i mean i think it's gonna be a little bit muddled up and going forwards and backwards . officially in the syllabus today's lecture is sequence to sequence models and attention . and next tuesday's lecture is machine translation . 
but really richard already started saying some things about machine translation last week . and so i thought for various reasons it probably makes sense to also be saying more stuff about machine translation today . but i am gonna cover the main content of what was meant to be in today's lecture and talk about attention today . and that's a really useful thing to know about . i mean almost certainly if you're gonna be doing anything in the space of sort of reading comprehension question answering models such as for instance assignment four . but also kinds of things that a whole bunch of people have proposed for final projects you definitely wanna know about and use attention . but then i actually thought i'd do a little bit of and i want to go back and actually say a bit more about these kind of gated models like the grus and lstms that become popular lately . and try and have a bit more of a go at saying just a little bit more about well why do people do this and why does it work . so we'll mix around between those topics . but somehow over these two weeks of classes we're doing recurrent models attention mt and all those kinds of things . and your dear tas and me spent all last night grading that midterm . so we're sort of 99% over with the midterm . there's a slight catch that a couple of people haven't done it yet because of various complications . so essentially next tuesday is when we're gonna be able to be sort of releasing solutions to the midterm and handing them back . the highest score was extremely high 90s . but yeah overall we're pretty pleased with how people did in it . i just thought i should mention one other issue which i will say sort of send a piazza note about . i mean i know that a few people were quite unhappy with the fact that some students kept on writing after the official end of the exam . and i mean i totally understand that . because the fact of the matter is these kind of short midterm exams and many people feel like they could do more if they had more time . 
i mean on the other hand i honestly feel like i don't know quite what to do about this problem . both richard and me came from educational traditions where we had exam proctors . and when it was time to put your pens down you put your pens down or else dire consequences happen to you . whereas my experience at stanford is that every exam i've ever been in at stanford there are people who keep writing until you forcibly remove the exam out of their hands . and so there seems to be a different tradition here . and in theory this is meant to be student regulated by the honor code but we all know that there are some complications there as well . so it's not that i'm not sensitive to the issue . and you know really exactly what i said to the tas before the end of the exam is so it's a real problem at stanford people going on writing so could everyone get in the room as quickly as possible and collect everyone's exams to minimize that problem . but obviously it's a little bit difficult when there are 680 students . and i think basically we have to proceed with that . apologies that we were a bit late getting that out . though with the midterm it wouldn't have made much difference . we have put a little bit of extension to assignment three . i guess we're really nervous about giving more extension to assignment three . not because we don't want you to have time to do assignment three but just because we realized that anything we do is effectively stealing days away from the time you have to do the final project or assignment four . so we don't wanna do that too much . we hope that assignment three isn't too bad . and the fact that you can do it in teams can help and that that won't be such a problem . another thing that we want people to do but are a bit behind on but i hopefully can get in place tomorrow is giving people access to microsoft azure to be able to use gpus to do the assignments . we really do want people to do that for assignment three . 
since it's just great experience to have and will be useful to know about for then going on for assignment four and the final project . so we hope we can have that in place imminently . and it really will allow you to do things much quicker for assignment three . so the kind of models that you're building for assignment three should run at least an order of magnitude sort of ten 12 times or something faster if you're running them on a gpu rather than a cpu . so look forward to hearing more about that . the final reminder i want to mention is i'm really really encouraging people to come to final project office hours for discussion . richard was really disappointed how few people came to talk to him about final projects on tuesday after the exam . now maybe that's quite understandable why no one turned up . but at any rate moving forward from here i really really encourage you to do that . so i have final project office hours tomorrow from one to three . richard is gonna be doing it again next tuesday . the various other phd students having their office hours . so really do for the rest of quarter try and get along to those . and check in on projects as often as possible . and in particular make really really sure that either next week or the week after that you do talk to your project mentor to find out their advice on the project . okay so let's get back into machine translation . and i just thought i'd sort of say a couple of slides of how important is machine translation . now really a large percentage of the audience of these stanford classes are not american citizens . so probably a lot of you realize that machine translation is important . but for the few of you that are native-born american citizens . 
i think a lot of native-born americans are sort of very unaware of the importance of translation because they live in an english-only world . where most of the resources for information are available in english and america is this sort of a big enough place that you're not often dealing with stuff outside the rest of the world . but really in general for humanity and commerce translation in general and machine translation in particular are just huge things right . that for places like the european union to run is completely dependent on having translation happen so it can work across the many languages of the european union . so the translation industry is a $40 billion a year industry . and that's basically the amount that's spent on human translation because most of what's done as machine translation at the moment is in the form of free services and so it's a huge issue in europe it's growing in asia lots of needs in every domain as well as commercial there's social government and military needs . and so the use of machine translation has itself become a huge thing . so google now translate over 100 there are a lot of people that are giving google stuff to translate . it's then important for things like having social connections . so i mean in 2016 last year facebook rolled out their own homegrown machine translation . prior to that they've made use of other people's translation but essentially what they had found was that the kind of commercial machine translation offerings didn't do a very good job at translating social chit chat . and the fact of the matter is that doing a better job at that is sufficiently important to a company like facebook that they're developing their own in house one of the quotes that came along with that was when they were testing it and turned off the machine translation for some users that they really went nuts that lots of people really do actually depend on this . so ebay makes extensive use of machine translation to enable cross-border trade . so that if you are going to be able to successfully sell products in different markets well you have to be able to translate the descriptions into things that people can read . okay and so that then leads us into what we're gonna be focusing on here which is neural machine translation . and so neural machine translation or nmt is sort of a commonly used slogan name . and it's come to have a sort of a particular meaning that's slightly more than neural plus machine translation . that neural machine translation is used to mean what we want to do is build one big neural network which we can train the entire end-to-end machine translation process in and optimize end to end . and so systems that do that are then what are referred to as an mt system . so that the kind of picture here is that we're going to have this big neural network . 
it's gonna take input text that's somehow going to encode into neural network vectors . it's then gonna have a decoder and out would come text at the end . and so we get these encoder-decoder architectures . before getting into the modern stuff to tell you about the archaeology of neural networks . neural networks had sorta been very marginal or dead as a field for a couple of decades . and so i think a lot of the time people these days think of deep learning turned up around 2012 with the imagenet breakthroughs . and boy has it been amazing since then . but really there have been earlier ages of neural networks . and in particular there's a boom in the use of neural networks in the second half of the 80s into the early 90s which corresponds to when rumelhart and mcclelland so that's the james mcclelland that's still in the psych department at stanford pioneered or re-pioneered the use of neural networks partly as also as a computing tool . and many of the technologies really the math of them are worked out during that period . so it was in the 80s there was really worked out of how to do general back propagation algorithms for multi-layer neural networks . and it was also during that period when people so algorithms like backpropagation through time were worked out in this period in the late 80s often by people who were psychologists cognitive scientists rather than hard core cs people in those days . and so also in that period was actually when neural mt in having these encoder decoder architectures for doing translation was first tried out . the systems that were built were incredibly primitive and limited which partly reflects the computational resources of those days . but they still were in coder/decoder architectures . so as far as i've been able to work out the first neural mt system was this system that was done by bob allen in 1987 the very first international conference on neural networks and so he constructed 3000 english/spanish sort of a 30 to 40 word vocabulary and the sentences were actually kind of constructed based on the grammar it wasn't actually kind of we'll just collect together human language use but you know you sort of had sentences like this with some variation of word order and things like that . and he built this simple encoded decoded network that you can see on the right that was not a recurrent model . representation of the sequence of words in a sentence and the sentences were only short and then were pumped through that . a few years after that lonnie chrisman . lonnie chrisman is actually a guy who lives in the bay area . 
he works at a tech firm still to this day . so lonnie chrisman in the early 90s then developed a more sophisticated neural network architecture for doing encoded decoder mt architecture . so he was using this model called raams recursive auto associative memories which were developed in the early 90s . not worth explaining the details of them . but a raam is in some sense kind of like recurrent network of the kind that we've already started to look at . and so that then leads into our modern that richard already mentioned . where we're having perhaps a recurrent network that's doing the encoding and then another recurrent network there's then decoding out in another language . and where in reality they're not normally as simple as this and we have more layers and more stuff and it all gets more complicated . i just wanted to mention quickly a couple more things about the space of these things . so you can think of what these encoder decoder architectures are as a conditional recurrent language model . so if we want to generate a translation we're encoding the source so we're producing a y from the source . and then from that y we're going to decode we're going to run a recurrent neural network to produce the translation . and so you can think of that decoder there as a conditional recurrent language model . so it's essentially being a language model that's generating forward as a recurrent language model . and the only difference from any other kind of recurrent or neural language model is that you're conditioning on one other thing and that's the only architecture difference . so if we then look down into the details a little bit there are different ways that you can do the encoder . the most common way to do the encoder has been with these gated recurrent units whether the grus or the lstms which are another kind of gated recurrent unit that richard talked about last time . i mean people have tried other things . i mean the modern resurgence of neural machine translation actually the very first paper that tried to do it was this paper by nal kalchbrenner and phil blunsom who now both work at deepmind . and they actually for their encoder they were using a recurrent sequence of convolutional networks . 
not the kind of gated recurrent networks that we talked about . and sometime later in the course we'll talk a bit more about convolutional networks and they're not nearly as much used in language they're much much more used in vision . and so if next quarter you do cs231n and get even more neural networks then you'll spend way more of the time on convolutional networks . but the one other idea i sort of wanted to just sort of put out there is sort of another concept to be aware of . so we have this y that we've encoded the source with . and then there's this so for the models that we've shown up until now and that richard had we calculated up to here . and we just used the y as the starting point of the hidden layer and then we started to decode . so this was effectively the google tradition of the way of doing it the model that sutskever et al proposed in 2014 . and so effectively if you're doing it this way you're putting most of the pressure on the forget gates not doing too much forgetting . because you have the entire knowledge of the source sentence here . and you have to make sure you're carrying enough of it along through the network . that you'll be able to continue to access the source sentence's semantics all the way through your generation of the target sentence . so it's especially true in that case that you will really lose badly if you got something at having a medium term memory . and you can do much better with something like an lstm . which is much more able to maintain a medium term memory with the sort of ideas that richard started to talk about . but that isn't actually the only way of doing it . and so the other pioneering work was work that was done at the university of montreal by kyunghyun cho and colleagues and that wasn't actually the way they did it . the way they did it was once they'd calculated the y as the representation of the source . they fed that y into every time step during the period of generation . so when you were generating at each state you were getting a hidden of just your language model . 
and then you were getting two inputs . you were getting one input which was the previous word the x_t . and then you were getting a second input which was the y that you were conditioning on . so you were directly feeding that conditioning in at every time step . and so then you're less dependent on having to sort of preserve it along the whole sequence . that seems to be a useful idea . and so that's actually the idea that will come back when i talk about attention . that attention is again going to give us a different mechanism of getting at the input when we need it . and to being able to condition on it . let me just sort of give you a couple more pictures and a sense of how exciting neural so for machine translation there are a couple of prominent evaluations of machine translation that are done . but i mean i think the most prominent one has been done by what's called the workshop on machine translation . and so this is showing results from that . and the university of edinburgh's traditionally been one of the strongest universities at doing machine translation . and so what we can see from these results up is good of machine translation quality . so we have the phrase-based syntactic machine translation systems which is the kind of thing that you saw on google translate until november 2016 . that although they work reasonably there is sort of a feeling that although they are a pioneering a good use of large data machine learning systems . so there really was very little progress in phrase-based machine translation systems in recent years . until neural machine translation came along the idea that people were most actively exploring was building syntax-based statistical machine translation systems which made more use of the structure of language . they were improving a little bit more quickly but not very quickly . how quickly kind of partly depends on how you draw that line . 
you believe 2015 was a fluke or whether i should draw the line as i have in the middle between them . but you got slightly more slope then not a lot . but so compared to those two things i mean actually just this amazing thing happened with neural machine translation . so it was only in 2014 after the wmt evaluation that people started playing with . could we build an end-to-end neural machine translation system . but then extremely quickly people were able to build these systems . and so by 2016 they were clearly winning in the workshop and machine translation . in terms of how much slope you have for improvement that the slope is extremely high . and indeed the numbers are kind of continuing to go up too in the last year . as i say in the next slide . that so neural mt really went from this sort of fringe research activity of let's try this and see if it could possibly work in 2014 . to two years later it had become this is the way that you have to do machine translation . because it just works better than everything else . so i'll say more about machine translation . but i thought i'd just highlight at the beginning well why do we get these big wins from neural machine translation . and i think there are maybe sort of four big wins . at any rate this is my attempt at dividing it up . so the first big win is the fact that you're just training these models end-to-end . so if you can train all parameters of the model simultaneously for one target driven loss function . that's just proved a really powerful notion . 
and indeed i think quite a lot of the success of deep learning systems is that because we have these sort of big computational flow graphs that we can optimize everything over in one big back propagation process . so it's easy to do end to end training . but that's been a very productive way to do end to end training . and it's the end to end training more than neural nets are magical . i think sometimes they're just given enormous amounts of power to these systems . but there are other factors as well . so as we stressed a lot these distributed representations are actually just worth a ton . so that they allow you to kind of share statistical strength between similar words similar phrases . and you can exploit that to just get better predictions and that's given a lot of improvement . a third big cause of improvement has been these neural mt systems are just much better at exploiting context . so richard briefly mentioned traditional language models . so those were things like four gram and five gram models which were just done on counts of how often sequences of words occurred . and those were very useful parts of machine translation systems . but the reality was that the language models on the generation side only used a very short context . and when you are translating words and phrases that the standard systems did that completely context free . so the neural machine translation systems are just able to use much more context and that means that they can do a lot better . and there's an interesting way go together in a productive way . so precisely the reason why your machine translation systems can practically use much more context . is because there are these distributed representations that allow you to share statistical strength . effectively you could never use more context in traditional systems . 
because you were using these one-hot representations of words . and therefore you couldn't build more than five gram models usefully because you were just being killed by the sparseness of the data . and then the fourth thing that really related to all of one two or three . but i think it's just sort of worth calling out . is something really powerful that's happened in the last couple of years with neural nlp methods . is that they've proven to just be extremely good for generating fluent text . so i think it's fair to say that the field of sort of natural language generation was sort of fairly moribund in the 2000s decade . because although there were sort of simple things that you can do writing a printf that's a text generation method . better than that with grammar driven text generation and so on . but there really were not a lot of good ideas as how to produce really good high quality natural language generation . whereas it's just proven extremely easy and productive . to do high-quality natural language generation using because it's very easy for them to use big contexts condition on other goals at the same time and they work really well . and so one of the big reasons why neural machine translation has been so successful and the results look very good . in fact it's sometimes the case that the actual quality of the translation is worse . that the quality of the generation in terms of fluency is much better . it's also worth knowing what's not on that list . so one thing that's not on that list that's a good thing black box component models for things like reordering and transliteration and things like that . and traditional statistical mt systems have lots of these separate components . you had lexicalized reordering components and distortion models and this models and that models . and getting rid of all of that with this end to end system is great . 
there are some other things that are not so great . that our current nmt models really make no use of any kind of explicit syntax or semantics . you could sort of say well maybe some interesting stuff is happening inside the word vectors and maybe it is . sorry there are current hidden state vectors and maybe it is but it's sort of unclear . but actually this is something that has started to be worked on . there have been a couple of papers that have come out just this year . where people are starting to put more syntax into neural machine translation models and are getting gains from doing so . so i think that's something that will revive itself . also another huge failing of machine translation has been a lot of the errors . that higher level textual by machine translation systems . so those are things of sort of discourse structure clause linking anaphora and things like that . yeah so that's been the general picture . before going on one of the things we haven't done very much of in this class . is actually looking at linguistic examples and having language on slide . so i thought i'd do at least one sentence of machine translation . and i kind of guessed that the highest density of in my audience is chinese . and this is my one sentence test set for chinese to english machine translation . so i guess back in the mid 2000s we were doing chinese to english machine translation . and one of the sentences that we translated terribly was this sentence . and ever since then i've been using this as my one sentence evaluation set . 
so i guess this sentence it actually comes from jared diamond's book guns germs and steel . so in a sense it's sort of a funny one since it's starting with the chinese translation of jared diamond's text . and then we're trying to translate it back into english but never mind . so this is the 1519 year there were 600 spanish people and their landing in mexico . and the first bit i want to focus on is then this next bit here . the several million population of the aztec empire . and so what you get in chinese is so here's our "aztec empire" . so in general in chinese all modifiers of a noun are appearing before the noun . and chinese has this really handy little morpheme right here the [foreign] . this is saying the thing that comes before it shown in that brownish color is a modifier of this noun that follows it . and this one's saying the sort of several million population . and there's this very specific linguistic marker that tells you how you're meant to translate it . and then after that we then got the part here where then we've got so first time confronted them losses two-thirds . and so that's just sort of tacked on to the end of the sentence so soldiers in the first clash . this is just an interesting thing in how translation works . so you could in an english translation try and tack that onto the end of the sentence and sort of say "losing two thirds of their soldiers in the first clash" or "and they lost two thirds of their but neither of those sound very good in english . so below here what we have is the reference translation which is where we got some competent human to translate this . and so interestingly what they did and i think correctly actually here is that they decide it would actually be much better to make this into two sentences . and so they put in a period and then they made a second sentence . okay so i won't tell you a bad translation but every year since i've been running since this sentence through google and so i'll show you the google translations . 
so on 2009 this is what google produced . but if we go in particular to this focus part millions of people to conquer the aztec empire . and well it's getting some of the words right but it's completely not making any use of the structure of the sentence in chinese . the first two-thirds of soldiers against their loss . okay so we can go on to 2011 . i left some of them out so he font size stayed vaguely readable . so it changes a bit but not really . millions of people to conquer the aztec empire . the initial loss of soldiers two-thirds of their encounters . so that last bit may be a fraction better but the rest of it is no better . in 2013 it seemed like they might have made a bit of progress . 1519 600 spaniards landed in mexico to conquer the aztec empire hundreds of million of people . the fact that you can read the to conquer the aztec empire has mean the spaniards sort of means it might have made some progress but then after that they just dump the hundreds of millions and so it's really not quite clear what that's doing but it sort of seemed like whatever that change that was just kind of luck because in 2014 it sort of switch back 1519 600 spaniards landed in mexico millions of people to conquer the aztec empire the first two-thirds of the loss of soldiers they clash . and not only that interestingly when i ran it again in 2015 and 2016 the translation didn't change at all . so i don't know what all the people were doing on the google mt translation team in 2015 and 2016 but they definitely weren't making progress in chinese translation . and i think this sort of reflects as if the feeling that the system wasn't really progressing . that they sort of built the models and mined all the data they could for their chinese english mt system that wasn't getting any better . so then in late 2016 google rolled out their neural machine translation system which you're gonna hear more about in a moment . and there's actual and distinct signs of progress . so in 1519 600 spaniards landed in mexico . 
so the beginning of it is a lot better 'cause the whole time it'd just been plunking down 1519 and 600 which wasn't a very promising beginning . in the chinese there's no word for "in" right . so this character here is "year" right . so it's sort of 1519 year 600 people spanish people right . but clearly in english you wanna be putting in it in there and say in 1519 . but somehow google it never manage to get that right where you might have thought it could . in 1519 comma great beginning and it continues much better 600 spaniards landed in mexico to conquer the millions of people of the aztec empire this is getting really good . neural machine translation is much much better . but there is still some work to do . i guess this last part is kind of difficult in a sense the way it's so tacked on to the end of the sentence . but you're right it still isn't working very well for that cuz they've just tacked on the first confrontation they killed two-thirds which sort of it seems to be the wrong way around because two-thirds of the aztecs . so there's still work to be done from proving neural machine translation . but i do actually think that that's showing very genuine progress and that's in general what's been shown . it's been aggressively rolled out by industry . so actually the first people who rolled out neural machine translation was microsoft . so in february 2016 microsoft launched neural machine translation on android phones no less . and another of the huge selling points of neural machine translation systems is that they're actually massively more compact . so that they were able to build a neural machine translation system that actually ran on the cellphone . and actually that's a very useful use case 'cause the commonest time when people want machine translation is when they're not in their home country . but a lot of people don't actually have cell plans that work in foreign countries at decent prices . 
and so it's really useful to be able to run your mt system just on the phone . and that was sort of essentially never possible with the huge kind of look up tables of phrase based systems it is now possible . systran is a veteran old mt company that also launched this system . and then google launched their neural machine translation system either of the two predecessors including some huge overclaims of equaling human translation quality . which we've just seen still isn't true based on my one sentence test set that they still have some work to do . but on the other hand they did publish a really interesting paper on the novel research that they've done on neural machine translation . and so for the research highlight today emma is gonna talk about that . multi lingual nmt system which enables zero shot translation . so as we have seen in the lecture this is the standard architecture for an nmt system which you have an encoder and a decoder . however this thin architecture supports only bilingual translation meaning that we can have only one specific source language and one specific target language . so what if you want to have a system that's able to do multilingual translation . meaning that we can have multiple source languages and multiple target languages . so previously people have proposed the first one they proposed to have multiple different encoders and multiple different decoders . where each pair correspond to one specific pair of source and target languages . and the second one that proposed to have a shared encoder that works for one specific source language but have different decoders to decode into different target languages . and they also have proposed the third one is they have multiple different encoders to work for different source languages and wants a single shared decoder to work for more specific target language . so what's so special about google's multilingual nmt system . so first of all it's really simple because here we only need one single model that is able to translate from different source languages to different target languages and because of the simplicity the system can trivially scale up to more language pairs . and second the system improves the translation quality for low resource language . so because the progress of the model are shared implicitly and so the model is forced to generalize across language boundaries . 
so it's observed that if we train the language that has very little training data with a language pair that has a lot of training data in one single model the translation quality for the low-resourced language is significantly improved . and also the system is able to perform zero-shot translation . meaning that the model can inclusively translate for the language pairs it has never seen during training time . for example if we train a model on portuguese to english and english to spanish data the model is able to generate reasonable translation for portuguese to spanish directly . without seeing any data for the language pair during training time . and this is the architecture for the models . as we can see this is kind of the state-of-the-art nmt system . where we have multiple stacked layers of lstms for both decoders and encoders and those applied attention mechanism which we will talk about later in a lecture . so what is the magic here that enables the system to do a multilingual translation . so it turns out instead of trying to modify the architecture they instead modified the input data by adding the special artificial token at the beginning of every input sentence to indicate what target language you want to translate to . so for example if you wanna translate from english to spanish we simple add this <2es> token to indicate that spanish is the target language . and after adding this artificial token we simply just put together all of the multi-lingual data and just start training . with this simple trick the state-of-the-art performance for english to german french to english and german to english translation . and they have comparable performance for english to french translation . so and here's a little more detail about a zero-shot translation . so during training time we train a model on portuguese to english and english to spanish data . but during test time we ask the model to perform portuguese to spanish translation directly . and it's shown here that the model is able to have comparable performance as the phrase based machine translation system . and also the nmt system with bridging . and also with a little bit of incremental training . 
meaning that we add a little bit of data for the portuguese to spanish translation . the model is able to surpass all of the other models listed above . i think that actually is a really amazing result . i mean in some sense it's actually realizing a long-held so a traditional problem with machine translation has always been that if you'd like to be able to translate between a lot of languages or you're then in a product space of number of systems right . so if you'd like to support around 80 languages as google does . that if you wanna allow translation between any pairs straightforwardly you have to build 6400 machine translation systems . and that's a lot of machine translation systems . so if something was being bridged what that effectively meant for google was you were translating twice via an intermediate language where the intermediate language was normally english . so the goal has for a long time has been in mt is to achieve this dream of an interlingua . so that if you had an interlingua in the middle you have to translate each language to and from the interlingua so you only need 80 encoders and 80 decoders so it's then the number of languages . and that has sort of never been very successful which is why effectively of these bilingual systems but this system is now sort of illustrating how you can actually have the encodings of neural mt system be an effective interlingua . okay so now on to the main technical is introducing this idea of attention . so what's the problem we want to deal with . so if we're into the sort of encoder-decoder model . we have this problem because our only representation of fixed-dimensional representation y which was sort of the state and so we need to kind of carry that through our entire generation of our translation sentence . and that seems like it might be a difficult thing to do and indeed what was shown was that was indeed a difficult thing to do and so what people found is that this initial neural worked well on short sentences . but if you tried to use them to translate very long sentences that their performance started to tank and i'll show you some numbers on that later; and so the idea that people came up with and this idea was actually first proposed for vision but was then moved over and tried for neural machine translation by kyunghyun cho and colleagues at montreal was to say well instead of saying that our y that we generate from is just the last hidden states why don't we say all of the hidden states of the entire encoding process are available to us . and so we sort of have this pool of source states that we can draw from to do the translation . and so then when we're translating any particular word we then want to work out which of those ones to draw from . so effectively the pool of source states becomes kind of like a random access memory which the neural network is then going to be able to retrieve from as needed when it wants to do its translation . 
and it'll find some stuff from it and use it for translating each word . and so attention for neural machine translation is one specific instantiation of this but in general this sort of builds into a bigger concept that has actually been a very exciting concept in recent neural networks research and i know at least a couple of groups are interested in doing for their final projects is this idea of can we augment neural networks with a memory on the side . so that we cannot only lengthen our short term memory with an lstm but we can actually have a much longer term memory that we can access stuff from as we need it . and attention is a simple form of doing that . and then some of the more recent work like neural turing machines is trying to do more sophisticated forms of read-write memories augmenting neural networks . okay so if we want to retrieve as needed you could think of that as saying okay well out of all of this pool of source states we want to be looking at where in the input we want to retrieve stuff from . so effectively after we've said je and we wanting to translate the next word . we should be working out well where in here do we want to be paying attention to decide what to translate next . and if it's french we wanna be translating the am next . and so our attention model effectively sort of becomes like an alignment model . 'cause it's saying well which part of the source are you next gonna be translating . so you've got this implicit alignment between the source and the translation . and that just seems a good idea 'cause that's even what human translators do . it's not that a human translator reads the whole of a big long sentence and says okay got it . and then starts furiously scribbling down the translation right . they're looking back at the source as they translate and are translating different phrases of it . week the idea that in training statistical models that one of the first steps was you worked out these word alignments between the source and the target . and that was used to extract phrases that gave you kind of phrases to use in a statistical phrase based system . here we're not doing that it's rather just at translation time by process of using this attention model . we're implicitly making connections between source and target which gives us a kind of alignment . 
but nevertheless it effectively means that we're building this end-to-end neural machine translation system that's doing alignments and translation as it works . so it achieves this nmt vision and you do get these good alignments . so we're using this kind of on the right structure where we're sort of filling in where and so you can look at where attention was laid when you're producing a translation translating here from french to english . and you can see that this model which is a model from people at montreal is doing a good job at deciding where to place attention . so it's starting off with the agreement on the and then the interesting part is that sort of french typically has adjectival modifiers after the head down . so this is the zone economic european which you have to flip in english to get the european economic area . and so it's kind of correctly modelling that flip in deciding where to pay attention in the source . okay so that looks good how do we go about doing that . so what we're gonna be doing is we've started to generate and we wanna generate the next word . state to decide where to access our random access memory which is all the blue stuff . and so well we haven't yet generated the hidden state for the next word so it seems like our only good choice is to use i think i skipped one . okay the only good choice is to use the previous hidden state as the basis of attention . and that's what we do and then what we're gonna do is come up with some score that combines it and and commonly people are only using the highest level of the hidden state for attention and decides where to pay attention . will score each position and saying where to pay attention . and i'll get back to the scoring functions in a minute . and so the model that they proposed was we get a score for and then what we're gonna do is sort of build a representation which combines all of the memories weighted by the score . so what we're gonna do is we're going to say okay we'll take those scores and we'll do our standard trick . we'll stick them through a softmax function and that will then give us a probability distribution of how much attention to pay to the different places in the source . and so then we're going to combine okay then we're going to combine together all of the hidden states of the encoder weighted by how much so that we're taking these are each hidden state of the encoder the amount of attention you're paying to that position . and then you're just calculating a weighted sum and that then gives us a context vector . 
so now rather than simply using the last hidden state as our representation of all of meaning hidden states of the encoder as our representation of meaning . and at different points in to pay attention in different places . and so now what we're gonna do is based on what we were . now what we're gonna do is based and so the previous hidden state and the next and the previous word of the decoder . but also conditioned on this context vector we're then gonna generate the next word . okay so then the question is well how do we actually score that . and at this point we need some kind of attention function that decides how to work out the score . and a very simple idea you could use for that is just to say well let's take the dot product between the decoded hidden state and an encoded the hidden state . and we wanna find the ones that are similar cuz that means we're in the right ballpark of words that have the same meaning and generate from that . and that's a possible thing that you could do . the one that was proposed by the people in montreal was this bottom one . where we're effectively using a single layer of neural net just like the kind of functions that we've been using everywhere else inside our lstm . so we're taking the concatenation of the two hidden states . we're multiplying the biometrics putting it through a tanh function . and then multiplying that by another vector where both the v and w are learned . and using that as an attention function . and so that's what they did in their work and that worked pretty well . in the work we did at stanford so principally thang luong's work that we proposed using a different attention function which is the one in the middle . which is this bilinear attention function successful and widely adopted . so here it's kind of like the top one where you're doing a dot product . 
but you're sticking in between the dot product a mediating matrix w . and so that matrix can effectively put on different parts of the dot product . to sort of have an idea of where to pay attention . and that's actually turned out to be a model that works kind of well . and i think there's a reason why it works kind of well . is kind of have interaction terms that look at h_t and h_s together . and even the dot product kind of has this interaction between h_t and h_s . and this is a more sophisticated way of getting an interaction between h_t and h_s . whereas if you're using this model with only a single layer of neural network you don't actually get interactions between h_t and h_s . because you've got the sort of two parts of this vector and a separate part of this matrix . and then you put it through a tanh but that just rescales it element-wise . and then you multiply it by a vector but that just rescales it element-wise . so there's no place that h_t and h_s actually interact with each other . and that's essentially the same problem of the sort of classic result that you can't get an xor function out of a one layer perceptron is things to interact with each other . so this is a very simple low parameter way in which you can actually have interaction terms . it seems to work really well for attention functions . it's not the only way that you could do it . another way that you could do things that a couple of papers have used is to say well gee a one way neural net's just not enough . let's make it a two layer feedforward network . and then we could have arbitrary interactions again like the xor model . 
and a couple of people have also played with that . another thing that has been explored for attention that i'll just mention . so the simple model of attention you've got this attention function . that spreads attention over the entire source encoding . and you've got a weighting on it . that's kind of simple it's easy to learn . it's potentially unpleasant computationally if you've got very long sequences . because that means if you start thinking about your back prop algorithm that you're back propagating into everywhere all the time . so people have also looked some at having local attention models . where you're only paying attention to a subset of the states at one time . and that's more of an exact notion of retrieving certain things from memory . and that can be good especially for long sequences . it's not necessarily compellingly better just for the performance numbers so far . okay so here's a chart that shows you how some of the performance works out . so what we see is that this red model has no attention . and so this shows the result that a no attention model works reasonably well up to sentences of about length 30 . but if you try and run a no attention machine translation system on sentences beyond length 30 . performance just starts to drop off quite badly . and so in some sense this is the glass half full story . the glass half full is actually lstms are just miraculous at remembering things . 
i mean i think quite to many peoples' surprise you can remember out to about length 30 which is actually pretty stunning . but nevertheless there's magic and there's magic . and you don't get an infinite memory . and if you're trying translate sentences that are 70 words long . you start to suffer pretty badly with the basic lstm model oops okay . so then the models that are higher up is then showing models with attention and i won't go through all the details . the interesting thing is that even for these shorter sentences . actually there are a lot of gains from putting attention into the models . that it actually does just let you do a much better job of working out where to focus on at each generation step . but the most dramatic result is essentially these curves turn into flat lines there's a little bit of a peak here maybe . but essentially you can be translating out to 70 word sentences without your performance going downhill . the one thing that you might think freaky about all of these charts is that they all go downhill for very short sentences . that it turns out that the things that are in this kind of data which is european parliament data actually . they just aren't sentences like i love my mum which is a four word sentence that has a really simple grammatical structure . word things that they're normally things like titles of x . or that there are half sentences that were cut off in the middle and things like that . so that they're sort of weirdish stuff and that's why that tends to prove hard to translate . okay here are just a couple of examples of giving you again some examples of translations . so we've got a source a human reference translation . then down at the bottom we have the lstm model . 
and above it it's putting in attention . so for this sentence it does a decent job the base model of translating it except for one really funny fact . it actually sticks in here a name that has nothing whatsoever to do with the source sentence . actually notice quite a bit in neural machine translation systems . that they are actually very good language models . so that they generate sentences that are good sentences of the target language . but they don't necessarily pay very much attention to what the source sentence was . a name goes there stick in some name . and let's get on with generating it's got nothing to do with the source sentence . that gets better in the other example where it actually generates the right name . here's a much more complex example where there's various stuff going on . one thing to focus on though is that the source has this "not incompatible" whereas the base model translates that as "not compatible" which is the opposite semantics . whereas our one here we're then getting "the incompatible" . i mean in particular one of the things that they do wrong is "safety and security" . where in the translation we have exactly the same words so it's of the form a and a . now really safety and security have a fairly similar meaning . so it's not actually so unreasonable to translate either of those words with this word . but clearly you don't want to translate safety and security as safety and safety . so this idea of attention has been a great idea . another idea that's been interesting is the idea of coverage . 
that when you're attending you want to make sure you've attended to different parts of the input and that was actually an idea that sort of again first came up in vision . so people have done caption generation where you're wanting to generate a caption that summarizes a picture . and so one of the things you might wanna do is when you're paying attention to different places you wanna make sure you're paying attention to the different main parts . so you both wanna pay attention to the bird . and you wanna pay attention to the background so you're producing a caption that's something like "a bird flying over a body of water" . and so that's an idea that people have also worked on in the neural mt case . so one idea is an idea of doing an attention in both directions . so there's a horizontal attention and a vertical attention . and you're wanting to make sure you've covered things in both directions . and in general something interesting that's been happening is in the last roughly a year i guess . that essentially people have been taking a number of the ideas that have been explored in other approaches to machine translation and building them into more linguistic attention functions . so one idea is this idea of coverage . but actually if you look in the older literature for word alignments well there are some other ideas in those older machine translation word alignment models . some of the other ideas were an idea of position . so normally attention or alignment isn't completely sort of random in the sentence . normally although there's some reordering stuff near the beginning of the source sentence goes somewhere near the beginning of the translation and stuff somewhere near the end of the source sentence goes towards the end of the translation . and that's an idea you can put in to your attention model as well . and a final idea here is fertility . fertility is sort of the opposite of coverage . it's sort of saying it's bad if you pay attention to the same place too often . 
because sometimes one word is gonna be translated with two words or three words in the target language that happens . but if you're translating one word with six words in your generated translation that probably means that you've ended up repeating yourself and that's another of the mistakes of sometimes neural machine translations systems can make that they can repeat themselves . and so people have started to build in those ideas of fertility as well . any questions or people good with the attention . so the question is that when we're we were just we were just doing and another thing that we could do is actually put in the previous word the xt . and also put that into the attention function . i mean one answer is to say yes of course you could . and you could go off and try that . and see if you could get value from it . i suspect it's less likely that that's really going to work because i think a lot of the time is that the hidden state to a fair degree . is still representing the word it actually has the advantage that it's kind of a context-disambiguated representation of the words . so one of the really useful things that lstms do is that they're sort of very good at word-sense disambiguation because you start with a word representation . which is often the kind of average of different senses and meanings of a word . and the lstm can use its in this context i should be representing this word in this way . and you kind of get this word sense disambiguation . that the hidden state records enough about the meaning of the word and actually improves on it by some of this using of context that i'm a little doubtful whether that would give gains . on the other hand i'm not actually aware of someone that's tried that . so it's totally in the space of someone could try it and see if you could get value from it . yes there's a very good reason to use an lstm as your generator even if you're going to do attention . which is the most powerful part of these neural machine translation systems remains the fact that you've got this neural language model as your generator which is extremely powerful and good as a fluent text generator . 
and that's still being powered by the lstm of the decoder . the power you get from the lstm at better remembering the sort of longer short-term memory is really useful as a language model for generation . giving you huge value and you'd be much worse off without it . i mean the thing that you could wonder is in this picture i'm still feeding the final state in to initialize the lstm for the decoder . do you need to do that or could you just cross that off and start with a zero hidden state and do it all with the attention model . so in this simple case if you sort of are making a hard decision to pay attention to only a couple of places that then kills differentiability . and so the easiest way to sort of keep everything nice and simply differentiable is just to say use global attention . put some attention weight on each position that's differentiable the whole way through . so if you're making a hard decision here traditionally the most correct way to do this properly and train the model is to say okay we have to do this as reinforcement learning . learning system lets you get around the non-differentiability . and then you're in this space of deep reinforcement learning which has been very popular lately . and there are a couple of papers that have used local attention which have done it using reinforcement learning training . so in the paper that thang did that's not what he did . he sort of i think it's true to say that to some extent he sort of fudged it seemed to work okay for him . but i mean this is actually an area in which there's been some recent work in which people have explored methods which in some sense continuing this tradition of fudging by putting it on the more of a theoretical footing and so an idea that's been explored quite a bit in recent work is to say in the forward model we're going to be making some discreet to pay attention to . in the backwards model we?셱e going to be using a soft approximation of those decisions we will then do the back propagation using that . so that kind of idea is you are working out say where to pay attention and you are choosing the states with the sort of a high need for attention is a hard decision but in the backwards model you are then having a sort of soft attention still and you are training with that . and so that leads into ideas like the straight through estimator which has been explored by yoshua bengio's group and other recent ideas of gumbel-softmaxes and things like that . and that's actually sort of been worked out as another way to explain another way to train these not which is in some ways easier than i'll go on . there was one other last thing i did want to sort of squeeze in for the end of today is i just wanted to say a little bit about what's . 
okay so assuming that at source time we've got our source sentence we're gonna make use of . and decoders that really our decoders are just saying okay here's the meaning we want convey produce a sentence that expresses that meaning and how can we do that decoding successfully . and i just sort of wanted to mention for couple minutes what are the options and how do they work . so one thing in theory we could do is say okay well let's just explore every possible sequence of words we can generate up to a certain length . let's score every one of them with our model and pick the best one . so we'd literally have an exhaustive search of possible translations . well that's obviously completely impossible to do . because not only is that exponential in the length of what we generate we have this enormous vocabulary . it's not even like we're doing exponential on a base of two or three . we're doing exponential on the base of 100000 or something like that . so the obvious idea and the first thing that people do is -- sorry . i'll get to the obvious one next . the second thing the probabilistically nice and good thing to do is to do a sampling based approach . which is a sort of a succesive sampling . so it's sometimes referred to as ancestral sampling . so what we're doing then is we've generated up to word t-1 and then saying okay . based on our model we have a probability distribution over the t-th word . and so we sample from that probability distribution one symbol at a time . and we keep on generating one word at a time until we generate our end of end of sentence symbol . so we generate a word and then based on what we have now we do a probabilistic sample of the next word and we continue along . 
so if you are a theoretician that's the right practical thing to do because if you are doing that you've gotten not only an efficient model of generating unlike the first model but you've got one that's unbiased asymptotically exact great model . if you're a practical person this is not a very great thing to do because what comes out is very high variants and decode the same sentence . okay so the practical easy thing to do which is the first thing that everybody really does is a greedy search . so we've generated up to the t minus one word . we use our model we work out what's the most likely word to generate next and we choose it and then we repeat that over and generate successive next words so that's then a greedy search . we're choosing best thing given the preceding subsequence . but that doesn't guarantee us the best whole sentence because we can go wrong in any of a number of ways because of our greedy decisions . so if you want to do a bit better than that which people commonly do the next thing that you think about trying is then doing a beam search . so for a beam search we're up to word t-1 and we say gee what are the five most likely words to generate next . and we generate all of them and we have a beam of five . and then when we go on to generate word t plus one we say for each of those sequences up to length t what are the five is the t plus first word and we generate all of them and well then we've got 25 hypotheses and if we kept on doing that we'd again be exponential but with a smaller base . so what we do is say well out of those 25 which are the five best ones . and we keep those five best ones . and then we generate five possibilities from each of those for the t plus two time . size k hypotheses and we head along and do things . so as k goes to infinity that becomes unbiased . but in practice our k is small so it is biased . it doesn't necessarily monotonically improve as you increase k but in practice it usually does up to some point at least . it turns out that often there's a limit to how big you can go for it improving which might even be quite small . because sometimes you actually tend to get worse if your model is not very good and you explore things further down . 
that your efficiency is going down in k squared . so as soon as you're at a beam of 10 you're 2 orders of magnitude slower than the greedy search but nevertheless it gives good gains . so this is from work again of kyunghyun cho's . so in the middle here we have the greedy decoding . and we're getting these numbers like 15.5 and 16.66 so something i haven't actually done yet is explain the machine translation evaluation and that's something i'll actually do in the next lecture . but big is good for these scores . so what you see is that if you sort of sample 50 translations and go with the best one although that gives you some improvement over the greedy one best . the amount of improvement it gives you isn't actually very much because there's such a vast space it's quite likely that most of your 50 examples are sampling something bad . on the other hand if you're using a fairly modest beam of size five or ten that's actually giving you a very good and noticeable gain much bigger than you're getting from the ancestral sampling . and so that's basically the state of the art for neural machine translation is people do beam search with a small beam . the good news about that actually is in statistical phrase space machine translation people always used a very large beam . a beam size of size 100 or 150 and really people would have liked to use larger . apart from where it's just computationally too difficult . but what people found with neural machine translation systems is small beams like 5 or 10 actually work extremely well and don't work much better . okay and so that gives us sort of beam search with a small beam as the de facto standard in nmt . okay that's it for today and we'll have more of these things on next tuesday . we're back with we're into week six now and lecture 11 . this is basically the third now last of our lectures . it's sort of essentially concentrating on what we can do with recurrent models and sequence to sequence architectures . i thought what i'd do in the first part of the lecture is have one more attempt at explaining some of the ideas about grus and lstms and where do they come from and how do they work . 
i'd sort of decide to do that anyway on the weekend just because i know that when i first started seeing some of these gated models that it took a long time for them to make much sense to me and not just seem like a complete surprise and mystery . that's the way they work so i hope i can do a bit of good at explaining that one more time . that feeling was reconfirmed when we started seeing some of the people who've filled in the midterm survey so thanks to all the people who filled it in . for people who haven't i'm still happy to have you fill it in over the last couple of days . while there were a couple of people who put lstms in the list of concepts they felt that they understood really well . dozens of people put lstms and grus into the list of concepts they felt kind of unsure about . this first part is for you and if you're one of the ones that already understand it really well i guess you'll just have to skip ahead to the second part . then we'll have the research highlight which should be fun today . and then so moving on from that it's then completing saying a bit more about machine translation . it's a bit that we sort of had skipped and probably should have explained earlier which is how do people evaluate machine translation systems . because we've been showing you numbers and graphs and so on and never discussed that . and then i wanna sort of say a bit more about a couple of things that come up when trying to build new and in some sense this is sort of done on the weed stuff it's not that this is sort of one central concept that you can possibly finish your neural networks class without having learned . but on the other hand i think that all of these sort of kind of things that come up if you are actually trying to build something where you've actually got a deep learning system that you can use to do useful stuff in the world and that they're useful good new concepts to know . the midterm we have got it all graded . and our plan is that we are going to return it to the people who are here after class . where in particular there's another event that's on here after class so where we're going to return it after class is outside the door . that you should be able to find tas with boxes of midterms and be able to return them . assignment three yeah so this has been a little bit of a stretch for everybody on assignment three i realized because sort of the midterm got in the way and people got behind . and we've also actually we're hoping to be sort of right ready to go with giving people gpu resources on azure and that's kinda've gone behind they're trying to work on that right now so with any luck maybe by the end of today we might have the gpu resources part in place . i mean at any rate you should absolutely be getting start on the assignment and writing the code . 
but we also do really hope that before you finish this assignment you take a chance to try out azure docker and getting stuff working on gpus because that's really good experience to have . then final projects the thing that we all noticed about our office hours last week after the midterm is that barely anybody came to them . we'd really like to urge for this week please come along to office hours again . and especially if you're doing really like you to turn up and talk to us about your final projects and in particular tonight after class and a bit of dinner which is again we're going be doing unlimited office hours . feel free to come and see him and possibly even depending on how you feel about it you might even go off and have dinner first and then come back and see him to spread things out a little bit . are there any questions people are dying to know or do i head straight into content at that point . basically i wanted to sort of spend a bit of time going through again the sort of ideas of where did these kinds of fancy recurrent units come from . what are they going to try and achieve and how do they go about doing it . our starting point is what we have with a recurrent neural network is that we've got something that's evolving through time . and at the end of that we're at some point in that here where time t plus n . and then what we want to do is have some sense of well this stuff that we saw at time t is that affecting what happens at time t plus n . that's the kind of thing of is it the fact that we saw at time t this verb squash that is having some effect on the n words later that this is being someone saying the word window because this is some kind of association between squashing and windows or is that completely irrelevant . we wanna sort of measure how what you're doing here affects what's happening maybe six eight ten words later . and so the question is how can we achieve that and how can we achieve it . and what richard discussed and there was some sort of complex math here which i'm not going to explain again in great detail . but what we found is if we had a basic recurrent neural network what we're doing at each time step in the basic recurrent neural network is we've got some hidden state and we're multiplying it by matrix and then we're adding some stuff to do with the input and then we go onto next time stamp where we're multiplying that hidden state by the same matrix again and adding some input stuff and then we go onto the time step and we model . multiplying that hidden stuff by the same matrix again . it keeping on doing these matrix multiplies and when you keep on doing these matrix multiplies you can and the trouble you get into is if your gradient is going to zero you kind of can't tell whether that means that actually what happened in words ago is having no effect on what you're seeing now . or whether it is you hadn't set all of the things in your matrixes norm exactly right and so that the gradient is going to zero because it's vanishing . this is where the stuff about eigenvalues but kind of the problem is with . 
basic rna sort of a bit too much like having to land your aircraft on the aircraft carrier or something like that . that if you can get things just the right size things you can land on the aircraft carrier but if somehow your eigenvalues are a bit too small then you have vanishing gradients . and if they're a bit too large you have exploding gradients and you sort of it's very hard to get it right and so this this naive transition function seems to be the cause of a lot of the problems . with the naive transition function in particular what it means is that sorta we're doing this sequence of matrix multipliers . so we're keeping on multiplying by matrix at each time step . and so that means that when we're then trying to learn . how much effect things have on our decisions up here . we're doing that by backpropagating through this whole sequence of intermediate nodes . and so the whole idea of all of these gated recurrent models is to say well somehow we'd like to be able to get more direct evidence of the effect of early time steps on much later time steps without having to do this long sequence matrix multiplies which almost certainly . give us the danger of killing off the evidence . so essentially what we wanna have is we want to kinda consider the time sequence that's our straight line . we also want to allow these shortcut connections so ht can directly affect ht +2 because if we could do that we then when we're backpropagating we'll then be able to measure in the backward phase the effect of ht on ht + 2 . and therefore we would be much more likely to learn these long term dependencies . so i'm gonna do the kinda gated recurrent units first and then kinda build onto lstms which are even more complex . so essentially that's what we're doing in the gated recurrent unit . and we're only making it a little bit more complex by saying well rather than just uniformly putting in stuff from time -1 and time -2 maybe we can have adaptive shortcut connections where we're deciding how much attention to pay to the past as well as to the present . and so that's essentially what you get with the gated recurrent unit . so the key equation of the gated recurrent unit is this first one . so it's sort of saying well we're going to do the normal neural network recurrent units stuff that's the stuff in green . so for the stuff in green we take the current input and multiply it by a matrix . 
we take the previous hidden statement and multiply it by a matrix . we add all of those things with a bias and put it through a tanh that's exactly the standard recurrent neural network update . so we're going to do that candidate update just like a regular rnn . function we're computing we're then going to adaptively learn how much and on which dimensions to use that candidate update and how much that we just gonna shortcut it and just stick with what we had from the previous time step . and while that stuff in the previous time step will have been to some extent computed by this regular and updated the previous time step . but of course that was also a mixture so to some extent it will have been directly inherited from the time step before that . and so we kind of adaptively allowing things from far past time steps just to be passed straight through with no further multiplications into the current time step . so a lot of the key to is it that we have this plus here . the stuff that is on this side of the plus we're just saying just move along the stuff you had before onto the next time step which has the effect that we're directly having stuff from the past be present to affect further on decisions . so that's most of what we have in a gru and a gru is then just a little bit more complex than that because if we do this it's sort of all additive you kinda kick stuff around forever . you're deciding which to pay attention to once you've paid attention to it it's around forever . and that's because you're sort and so the final step is to say well actually maybe we want to sort of prune away some of the past stuff adaptively so it doesn't hang around forever . and so to do that we're adding this second gate the reset gate . and so the reset gate gives you a vector of again numbers between zero and one which is calculated like a kind but it's sort of saying well to some extent what we want to do is be able to delete some of the stuff that was in ht- 1 when it's no longer relevant . and so we doing this sort of hadamard product the element wise product of the reset gate and the previous hidden state . and so we can forget parts of the hidden state . and the parts that we're forgetting is embedded in this kind of candidate update . the part that's being just have direct updates is still just exactly as it was before . so to have one attempt to be more visual at that . so if we have a basic vanilla tanh-rnn one way that you could think about that is we have a hidden state unit is doing as a program is saying you read the whole of that register h you do your rnn update and you write the whole thing back . 
so you've got this one memory register . you read it all do a standard recurrent update and write it all back . and you're just sort of repeating that over and over again at each time step . so in contrast to that when you have a gru unit that is then this adaptive flexibility . so first of all with the reset gate you can learn a subset of the hidden state that you want to read and make use of . and the rest of it will then get thrown away . so you have an ability to forget stuff . and then once you've sort of read your subset you'll then going to do on it your standard rnn computation of how to update things . but then secondly you're gonna select the writable subset . so this is saying some of the hidden state we're just gonna carry on from the past . we're only now going to edit part of the register . and saying part of the register i guess is a lying and simplifying a bit because really you've got this vector of real numbers and some said the part of the register is 70% updating this dimension and 20% updating this dimension that values could be one or zero but normally they won't be . so i choose the writable subset and then it's that part of it that i'm then updating with my new candidate update which is then written back adding on to it . and so both of those concepts in the gating the one gate is selecting what to read for your candidate update . and the other gate is saying which parts of the hidden state to overwrite . does that sort of make sense how that's a useful about having a recurrent model . yeah so how you select the readable subset is based on this reset gate . so the reset gate decides which parts of the hidden state to read to update the hidden state . so the reset gate calculates which parts to read based on the current input and the previous hidden state . so it's gonna say okay i wanna pay a lot of attention to dimensions 7 and 52 . 
and so those are the ones and a little to others . and so those are the ones that will be being read here and used in the calculation of the new candidate update which is then sort of mixed together with carrying on what you had before . so the question was explain this again . let me go back to this slide first cuz this has most of that so here what we want to do is we're carrying along a hidden state over time . and at each point in time we're going to say well based on the new input and the previous hidden state we want to try and calculate a new hidden state but we don't fully want to sometimes it will be useful just to carry over information from further back . that's how we're going to get longer term memory into our current neural network . cuz if we kind of keep on doing multiplications at each time step along a basic rnn we lose any notion of long-term memory . and essentially we can't remember things for more than seven to ten time steps . so that is sort of the top level equation to say well what we gonna calculate . we want to calculate a mixture of a candidate update and keeping what we had there before and how do we do that . well what we're going to learn is this ut vector the update gate and the elements of that vector are gonna be between zero and one . and if they're close to one it's gonna say overwrite the current hidden state with what we calculated this time step . it's gonna say keep this element vector just what it used to be . and so how we calculate the update gate is using our regular kind of recurrent unit where it looks at the current input and it looks at the recent history and it calculates a value with the only difference that we use here sigmoid so that's between 0 and 1 rather than tanh that puts that at between minus 1 and 1 . and so the kind of hope here intuitively is suppose we have a unit that is sort of sensitive to what verb we're on then what we wanna say is well we're going through this sentence and we've seen a verb . we wanted that unit well sorry these dimension of the vector . let's say their five dimensions of the vector that sort of record what kind of verb it's just seen . we want those dimensions of the vector to just stay recording what verb was seen until such time as in the input a band new verb appears . and it's at precisely that point we wanna say okay now is the time to update . forget about what used to be stored in those five dimensions . 
now you should store a representation of the new verb . and so that's exactly what the update gate could do here . it could be looking at the input and say okay i found a new verb . be being given a value of 1 and that means that they'll be storing a value calculated from this candidate update and ignoring what they used to store in the past . but if the update gate finds it's looking at a preposition or at a term in our it'll say no not interested in those . so it'll make the update value close to 0 and that means that dimensions 47 to 52 will continue to store the verb that you last saw i haven't quite finish . so that was that part of it so yes . and when we do update the candidate update is just exactly the same as it always was in our current new network that you're calculating this function of the important put it through a tanh together from minus 1 to 1 . then the final idea here is that well if you just have this if you're doing a candidate update you're always using the new input word in exactly the same way . whereas really for my example what i was saying was if you have detected a new verb in the input you should be storing that new verb in dimensions 47 to 52 and you should just be ignoring what you used to have there . and so it's sort of seems like at least in some circumstances what you'd like to do is throw away your current hidden state so you could replace it with some new hidden state . and so that's what this second gate the reset gate does . so the reset gate can also look at the current import in the previous hidden state and it choses a value between zero and one . and if the reset gate choses a value close to zero you're essentially just throwing away the previous hidden state and calculating something based on your new input . and the suggestion there for language analogy is well if it's something like you're recording the last seen verb in dimensions 47 to 52 . when you see a new verb well the right thing to do is to throw away what you have in your history from 47 to 52 and just calculate something new based on the input but that's not always for example in english english is famous for having a lot of verb particle combinations which cause enormous difficulty to non-native speakers . like make up make out take up . all of these combinations of a verb and a preposition have a special meaning that you just have to know . it isn't really you can't tell from the words most of the time . so if you are wanting to work out what the meaning of make out is so you've seen make and you put in that into dimensions 47 to 52 . 
but if dimensions 47 to 52 are really storing main predicate meaning if you see the word out coming next you don't wanna throw away make because it's a big difference in meaning whether it's make out or take out will give out . what you wanna do is you wanna combine both of them together to try and calculate the predicate's meaning . so in that case you want your reset gate to have a value near one so you're still keeping it and you're keeping the new import and calculating another value . okay that was my attempt to explain grus and now the question . so the question is okay but why this gated recurrent unit not suffer from the vanishing gradient problem . and really the secret is if you allowed me to simplify slightly and this is actually a version of a network that has been used . it's essentially not more details but this aspect of it actually corresponds to the very original form of an lstm that was proposed . suppose i just delete this this- ut here so this just was 1 . so what we have here is ht- 1 so kind of like the reset gate the update gate is only being used on this side . it's saying should you pay any attention to the new candidate but you're always plussing it with ht-1 . if you'll imagine that slightly simplified form well if you think about your gradients then what we've got here is when we're kind of working at h this has been used to calculate ht . ht-1 is being used to calculate ht so ht equals a plus ht-1 so there's a completely linear relationship with a coefficient of one between ht and okay and so therefore when you do your calculus and you back prop that right you have something with slope 1 . that ht is just directly reflecting ht-1 . and that's the perfect case for gradients to flow beautifully . nothing is lost it's just going straight back down the line . and so that's why it can carry information for a very long time . so once we put in this update gate what we're having is the providing ut is close to zero this is gonna be approximately one and so the gradients are just gonna flow straight back to the line in an arbitrary distance and you can have long distance dependencies . crucially it's not like you're multiplying by a matrix every time which causes all with vanishing gradients . it's just almost one there straight linear sequence . now of course if at some point ut is close to 1 so this is close to zero well then almost nothing is flowing in from ht-1 . 
but that's then saying there is no long term dependency . so nothing flows a long way back . so the question is isn't ht tilted ut both dependent on ht-1 . just like the ut you're calculating it here in terms of ht-1 . so in some sense the answer is yeah you are right but it's sort of turns out not matter right . so the thing i think is if i put words in to your mouth the thing that you're thinking about is well this ut look right down at the bottom here vector multiply from ht-1 . and well then where the ht-1 come from it came from ht-2 and there was some more matrix vector multiplies here so there is a pathway going through the gates where you're keep on doing matrix vector but it turns out that sort of doesn't really matter because of the fact that there is this direct pathway where you're getting this straight linear flow of gradient information going back in time . yes i don't think i'll get any further in this class if i'm not careful . so the question was why when you is before ut and one one is ut . that was bad boo boo mistake cuz obviously we should be trying to be consistent . this is sort of in some sense whether you're thinking of it as the forget gate or a remember gate and you can kind of have it either way round . and that doesn't effect how the math and the learning works . i'm happy to talk about this because i do actually think it's useful to understand this stuff cuz in some sense these kind of gated units have been the biggest and most useful idea for making practical systems in the last couple of years . it depends on a lot of particularities it sort of seems like somewhere around 100 . sorry the question was how long does a gru actually end up remembering for and i kind of think order of magnitude the kind number you want in your head is 100 steps . so they don't remember forever i think that's something people also get wrong . if we go back to the other one that i hope to get to eventually the name is kind of a mouthful . i think it was actually very deliberately named where it was called long short term memory . right there was no idea in people's heads that this was meant to be the model of long term memory in the human brain . long term memory is fundamentally different and needs to be modeled in other ways and maybe later in the class we'll say a little a bit about the kind of ideas people thinking about this . 
what this was about was saying okay well people have a short term memory and it lasts for a while . whereas the problem was our current neural networks are losing all of there memory in ten time steps . so if we could get that pushed out another order of magnitude during 100 time steps that would be really useful to give us a more human like sense of short term memory . so the question is do grus train faster than lstms . i don't think that's true so richard says less computation the computational cost is faster but i sort of feel that sometimes lstms have a slight edge on speed . no huge difference let's say that's the answer . any other was there another question that people want to ask . you can ask them again in a minute and i go on . okay so then finally i wanted to sort of say a little bit about lstms . so lstms are more complex because there are more equations down the right side . and there's more gates but they're barely different when it comes down to it . and to some extent they look more different than they are because of certain arbitrary choices of notation that was made when lstms were introduced . so when lstms were introduced hochreiter & schmidhuber we have this privileged notion of memory in the lstm which we're going to call the cell . and so people use c for the cell of the lstm . but the crucial thing to notice is that the cell of the lstm is behaving like the hidden state of the gru so really the h of the gru is equivalent to the c of the lstm . whereas the h of the lstm is something different that's related to sort what's exposed to the world . so the center of the lstm this equation for updating the cell . is do a first approximation exactly the same as this most crucial equation for updating the hidden state of the gru . now if you stare a bit they're not quite the same the way they are different is very small . so in the lstm you have two gates a forget gate and then an input gate so both of those for each of the dimension have a value between zero and one . 
so you can simultaneously keep everything from the past and keep everything from your new calculated value and sum them together which is a little bit different . to the gru where you're sort of doing this tradeoff as to how much to take directly copy across the path versus how much to use your candidate update . so it split those into two functions so you get the sum of them both . but other than that where's my mouse . the candidate update is exactly the same as what's being listed in terms of c tilde and h tilde but the candidate update is exactly well sorry it's not quite i guess it's the reset gate the candidate update is virtually the same as and then for the gates the gates are sort of the same that they're using these sort of r and n style calculations to get a value between zero for one for each dimension . so the differences are that we added one more gate because we kinda having forget and input gates here and the other difference is to have the ability to sort of that the grus sort of has this reset gate where it's saying i might ignore part of the past when calculating my candidate update . the lstm is doing it a little bit differently . so the lstm in the candidate update it's always using the current input . but for this other half here it's not using ct minus 1 it's using ht minus 1 . so the lstm has this extra ht which is derived from ct . and the way that it's derived from ct is that there's an extra tanh here but then you're scaling with this output gate . so the output gate is sort of equivalent but effectively it's one one time step earlier cuz on the lstm side on the preceding time step you also calculate an ht by ignoring some stuff with the output gate whereas in the gru for the current time step you're multiplying with the reset gate times your previous hidden state . it's the question about was the ft . no as presented here it's a don't forget gate . again you could do the 1 minus trick if you wanted to and call this 1 minus f1 but yeah as presented here if the value is close to 1 it means don't forget yeah absolutely . an update gate because if if the value of it is close to 1 you're updating with the candidate update . and if the value is close to zero you're keeping the previous reset . so okay so the question was sometimes you're using ct-1 and sometimes you're using ht-1 . and the question is in what sense is ct less exposed in the lstm . right so there was something i glossed over in my lstm presentation and i'm being called on it . 
is look actually for the lstm it's ht-1 that's being used everywhere for all three gates . so really when i sort of said that what we're doing here calculating ht that's sort of similar to the reset gate in the gru . i kind of glossed over that a little . it's sort of true in terms of thinking of the calculation of the candidate update cuz this ht- 1 will then go into the candidate update . but's a bit more than that cuz actually stuff that you throw away with your output gate at one time step is then also gonna be thrown away in the calculation of every gate at the next time step . yeah and so then the second question is in what sense is the cell less exposed . and that's sort of the answer to that . the sense in which the cell is less exposed is the only place that the cell is directly used is to sort of linearly add on the cell at the previous time step plus its candidate update . for all the other computations you're sort of partially hiding the cell using this output gate . hm okay so the question is gee why do you need this tanh here couldn't you just drop that one . i'm not sure i have such a good answer to that question . well this ct is kind of like a linear layer and therefore it's kind of insured if you should add a non linearity after it . and that gives you a bit more power . well we could try it both ways and see if it makes a difference or maybe shane already has i'm not sure [laugh] . make them a softball one that i can answer . so i had a few more pictures that went through the parts of the lstm with one more picture . i'm starting to think i should maybe not dwell on this in much detail . cuz we've sort of talked about the fact that there are the gates for all the things . we're working out the candidate update just like an rnn . the only bit that i just wanna say one more time is i think it's fair to say that the whole secret of these things is that you're doing this addition when in the addition it's sort of a weighted addition . 
but in the addition one choice is you're just copying stuff from the previous time step . and to the extent that you're copying stuff from the previous time step you have a gradient of 1 which you're just pushing . so you can push error directly back across that and you can keep on doing that for any number of time steps . so it's that plus having that plus with the previous time step rather than having it all multiplied by matrix . that is the central idea that makes lstms be able to have long short-term memory . and i mean that has proven to be an incredibly powerful idea and so in general it doesn't sound that profound but that idea has been sort of driving a lot of the developments of what's been happening in deep learning in the last couple of years . so we don't really talk about in this class about vision systems . you can do that next quarter in 231n . but one of the leading ideas and has been used recently in better systems for doing kind of vision systems with deep learning has been the idea of residual networks commonly shortened as resnets . and to a first approximation so resnets is saying gee we want to be able to build 100 layer deep neural networks and be able to train those successfully . the way resnets are doing that is exactly the same idea here with the plus sign . it's saying as you go up each layer we're going to calculate some non-linear function using a regular neural net layer . but will offer the alternative which is that you can just shunt stuff up from the layer before add those two together and repeat over again and go up 100 layers . and so this plus sign you may have learned in third grade but turns out plus signs have been a really useful part of modern deep learning . okay yeah here is my little picture which i'll just show . i think you'll have to sort of then slow it down to understand that this is sort of going backwards from time 128 as to how long and it sort of looks like this if i play it . and so if we then try and drag it back i think then i can play it more slowly . all right so that almost instantaneously the rnn has less information because of the matrix multiply . but as you go back that by the time you've gone back so at ten times steps the rnn is essentially lost the information . whereas the lstm even be going back it starts loose information but you know you sort of gain back this sort of more like time step 30 or lost all of its information which is sort of the intuition i suggested before . 
but something like 100 time steps you can get out of a lstm . almost up for a halftime break and the research highlight but before that couple other things i wanted to say here's just a little bit of practical advice . they're gonna be wanting to train recurrent neural networks with lstms on a largest scale . so here is some of the tips that you should know yes . so if you wanna build a big recurrent new network definitely use either gru or an lstm . so for any of these recurrent networks initialization is really really important . that if your net recurrent your network should work if your network isn't working often times it's because the initial initialization is bad . so what are the kind of initialization ideas that often tend to be important . it's turned to be really useful for the recurrent matrices that's the one where you're multiplying by the previous hidden state of previous cell state . it's really useful to make that one orthogonal . so there's chance to use your good there aren't actually that many parameters in a recurrent neural net . and giving an orthogonal initialization has proved to be a better way to kinda get them learning something useful . even with sort of these ideas with grus and lstms you're gonna kinda keep multiplying things in a recurrent neural network . so normally you wanna have if you start off with two large values that can destroy things try making the numbers smaller . here's a little trick so a lot of the times we initialize things near zero randomly . an exception to that is when you're setting the bias of a forget gate it normally works out much better if you set the bias gate for the forget gate to a decent size positive number like one or two or a random number close to one or two . that's sort of effectively saying you should start off paying a lot of attention to the distant past . that's sort of biasing it to keep long term memory . and that sort of encourages you to get a good model . and if the long term past stuff isn't useful it can shrink that down . 
off mainly forgetting stuff it'll just forget stuff and never change to any other behavior . in general these algorithms work much better with modern adaptive learning rate algorithms . we've already been using adam in the assignments . rmsprop work a lot better than basic sgd . you do wanna clip the norms of the gradients . you can use a number like five that'll work fine . and so we've used dropout in the assignments but we haven't actually ever talked about it much in lectures . for rnns of any sort it's trivial to do dropout vertically . it doesn't work and i either do drop out horizontally along the recurrent connections . because if you have reasonable percentage of drop out and you run it horizontally then within the few time steps almost every dimension will be dropped in one of them and so you have no information flow . there have been more recent work that's talked about ways that you can successfully do horizontal dropout in recurrent networks in including orthongal's phd student called base in drop out that works well for that . but quite commonly it's still the case that people just drop out vertically and don't drop out at all horizontally . patient if you're running if you're learning recurrent nets over large data sets it often takes quite a while and you don't wanna give up . sometimes if you just train them long enough start to learn stuff . this is one of the reasons why we really want to get you guys started using gpus because the fact of the matter if you're actually trying to do things on decent size data sets you just don't wanna be trying to train in lstm or gru without using a gpu . one other last tip that we should mention some time is ensembling . if you'd like your numbers to be 2% higher very effective strategy which again makes it good to have a gpu is don't train just one model train ten models and you average their predictions and that that normally gives you quite significant gains . so here are some results from mt systems trained . the red ones is a single model . the purple ones are training 8 models and in this case it's actually just majority voting them together . 
but you can also sort of you can see it's just giving very nice gains in performance using the measure for mt performance which i'll explain after the break . but we're now gonna have michael up to talk about the research highlight . and i'll quickly explain it until the video is in there- hi everyone . i'm gonna be presenting the paper lip reading sentences in the wild . so our task is basically taking a video which we preprocessed into a sequence of lip-centered images with or without audio . and we're trying to predict like the words that are being said in the video . maybe it doesn't said security had been stepped up in britain . so anyway and for the rest of this i'll talk about what architecture they use which is they deem the watch listen attend and spell model . strategies that might also be helpful for your final projects . there's also the dataset and the results was actually surpassing like a professional lip reader . so the architecture basically breaks down into three components . we have a watch component which takes in the visual and the listening component which takes in the audio and these feed information to the attend and spell module which outputs the prediction one character at a time . and they also use this with like just the watch module or just the listen module . to go into slightly more detail for the watch module like the face centered images and feed that into a cnn which then the output of much size over the time steps . we output a single state vector s of v as well as the set of output vectors l of v and the listen module is very similar . we take the pre-processed speech and we again site over using the lstm and we have another state vector and another set of output vectors and then in the decoding step . so we have an lstm as a really steps of during the decoding and the initial hidden state is initialized as the concatenation of the two hidden states from the two previous modules as well as we have like a dual attention mechanism which takes in the output vectors from each of their respective modules and we take those together and we make our prediction using a softmax over a multi-layer procepteron . and so one strategy that uses called curriculum learning . so ordinarily when you're training this sequence to sequence models one full sentence at a time . tip by what they do on curriculum learning is you start with the word length like segment and then you can slowly increase the length of your training sequences and what happens is you're actually like the idea is you're trying to learn like slowly build up the learning for the model and what happens is it ends up converging faster as well as decreasing overfitting . 
another thing that they use so ordinarily during training you'll be using the ground truth input like character sequence but during the test time you wouldn't be using that you'd just be using your previous prediction after every time step . so what you do in scheduled sampling is kind of like bridge the difference in scenarios between training and testing is that you actually just for a random small probability like sample from the previous input instead of the ground truth input for that time step during training . so the dataset was taken from the authors collected it from the bbc news and they have like dataset that's much out there with over 17000 vocabulary words and the other the quite a bit like processing to like some other things on the lips and do like the alignment of the audio and the visuals . so just to talk about the results i guess the most eye popping result is that they gave the test set to actually like a company that does like professional lip reading and they're only able to get about like one in four words correct or as this model was able to get one in two roughly based on word error rate . and they also did some other experience as well with looking at if you combine the lips version with the audio you get like a slightly better model which shows that using both modalities improves the model as well as looking at what happens if you add noise to the model . yeah so obviously a lot of details there . but again that's kind of an example of what's been happening with deep learning where you're taking this basic model architecture things like lstm and saying here's another problem let's try it on that as well and it turns out to work fantastically well . i'll see how high i can get in teaching everything else about it on machine translation . so it's something i did just want to explain is so back here and in general when we've been showing machine translation results . we've been divvying these graphs that up is good and what it's been measuring with these numbers are things called blue scores . so i wanted to give you some idea of how and why we evaluate machine translation . so the central thing to know about machine translation is if you take a paragraph or text and give it to ten you'll get back ten different translations . there's no correct answer as to how to translate a sentence into another language . and in practice most of the time all translations are imperfect and it's kind of deciding what you wanna pay most attention to is that do you want to maximally preserve the metaphor that the person used in the source language or do you wanna more directly convey the meaning it conveys because that metaphor won't really be familiar to people in the target language . do you want to choose sort of short direct words because it's written in a short direct style . or do you more want to sort of you choose a longer word that's a more exact translation . there's all of these decisions and things and in some sense a translator is optimizing over if we do it in machine learning terms but the reality is it's sort of not very clear . you have lots of syntactic choices as whether you make it a passive or an active and word order and so on . so we just can't have it like a lot things of saying here's the accuracy that was what you were meant to use . so one way to do mt evaluation is to do it manually . 
you get human beings to look at translations and to say how good they are . and to this day basically that's regarded as the gold standard of machine translation evaluation because we don't have a better so one way of doing that is things like likert scales where you're getting humans to judge translations to adequacy which is how well they convey the meaning of the source and fluency which is for how natural the output sentence sounds in the target language . commonly a way that's more easily measurable that people prefer is actually if you're comparing systems for goodness is that you directly ask human beings to do pairwise judgments of which is better translation a or translation b . i mean it turns out that even that is incredibly hard for humans to do as someone who has sat around doing this task of human evaluation . i mean all the time it's kind of okay this one made a bad word choice here and this one got the wrong verb form there which of these do i regard as a worse error . so it's a difficult thing but we use the data we can from human beings . okay that's still the best thing that we can do . basically it's slow and expensive to get human beings to judge translation quality . well another obvious idea is to say translation into some task we can just see which is more easily a valuable . we could just see which mt system lets us do the final task better . so we'd like to do question answering over foreign language documents . we'll just to get our question answers correct score and they'll be much easier to measure . and that's something that you can do but it turns out that that often isn't very successful . cuz commonly your accuracy on the downstream task is very little affected by many of the fine points of translation . an extreme example of that is sort of like cross-lingual information retrieval . when you're just wanting to retrieve relevant documents to a query in another language . that providing you can kind of produce some of the main content words in the translation it really doesn't matter how you screw up the details of syntax and verb inflection . it's not really gonna affect your score . okay so what people have wanted to have is a direct metric that is fast and cheap to apply . and for a long time i think no one thought there was such a thing . 
and so then starting in the very early 2000s people at ibm suggested this first idea of hey here's a cheap way in which we can measure word translation quality . and so they called it the bleu metric . and so here was the idea of how they do that . what they said is let us produce reference translations . we know that there are many many possible ways that something can be translated . but let's get a human being to produce a reference translation . so what we are going to do is then we're going to have a reference translation by a human and we're going to have a machine translation . and to a first approximation we're translation is good to the extent that you can find word n-grams . so sequences of words like three words in a row two words in a row which also appear in the reference translation anywhere . so what are the elements of this . so by having multi-word sequences that's meant to be trying to judge whether you have some understanding of the sort of right syntax and arguments . because you're much more likely if it's not just you've got a bag of keywords . you actually understand something of the syntax of the sentence . the fact that you can match it anywhere is meant to be dealing with the fact that quite flexible word order . so it's not adequate to insist that the phrases appear in the same word order . of course in general in english a lot of the time you can say last night i went to my friend's place or i went to my friend's place last night . and it seems like you should get credit for last night regardless of whether you put it at the beginning or the end of the sentence . the bleu measure is a precision score . so it's looking at whether n-grams that are in the machine translation also appear in the reference translation . there are a couple of fine points then . 
you are only allowed to count for a certain n and n-gram once . so if in your translation the airport appears three times but there's only one you're only allowed to count one of them as correct not all three of them . and then there's this other trick that we have this thing called a brevity penalty . because if it's purely saying is what appears in the machine there are games you could play every passage with the word the . because if it's english the word the is pretty sure to appear somewhere in the reference translation and get precision one . so if you're making what your translation is shorter than the human translations you'll lose . okay so more formally so you're doing this with n-grams up to a certain size . commonly it's four so you use single words pairs of words triples and four words . you work out this kind of precision of each . and then you're working out a kind of a weighted geometric mean of those precisions . and you multiplying that by brevity penalty . and the brevity penalty penalizes you if your translation is shorter than the reference translation . there are some details here but maybe i'll just skip them and go ahead . so there's one other idea then which is well what about this big problem that well there are a lot of different ways to translate things . and there's no guarantee that your translation could be great and it might just not match the human's translation . and so the answer to that that the original ibm paper suggested was what we should do is collect a bunch of reference translations . and the suggested number that's been widely used was four . and so then most likely if you're giving a good translation it'll appear in one of the reference translations . and then you'll get a matching n-gram . now of course that's the sort of a statistical argument . 
cuz you might have a really good translation and none of the four translators chose it . and the truth is then in that case you just lose . and indeed what's happened in more recent work is quite a lot of the time actually the bleu measure is only run with one reference translation . and that's seems a little bit cheap . and it's certainly the case that if you're running with one reference translation you're either just lucky or unlucky as to whether you guessed to translate the way the translator translates . but you can make a sort of a statistical argument which by and large is valid . that if you're coming up with good translations providing there's no correlation somehow between one system and the translator . that you'd still expect on balance that you'll get a higher score if you're consistently giving better translations . though this problem of correlation does actually start to rear its head right . that if the reference translator always translated the things as us and one system translates with us and the other one translates with united states . kind of one person will get lucky and the other one will get unlucky in a kind of a correlated way . so even though it was very simple when bleu was initially introduced it seemed to be miraculously good that it just corresponded really well with human judgments of translation quality . rarely do you see an empirical data set that's as linear as that . like many things that are surrogate metrics there are a lot of surrogate metrics that work really well if no one is trying to optimize them but don't work so well once people are trying to optimize them . so what happen then was everyone evaluated their systems on bleu scores and so therefore all researchers worked on how to make their systems have better bleu scores . and then what happened is this correlation graph went way down . and so the truth is now that current and this relates to the sort of when i was saying the google results were exaggerated . the truth is that current mt systems produce bleu scores that are very similar to human translations for many language pairs which reflects the fact that different human beings are quite creative and vary in how they translate sensors . but in truth the quality of machine translation is still well below the quality of human translation . okay few minutes left to say a bit more about mt . 
i think i can't get through all this material but okay so one of the big problems you have if you've tried to build something any kind of generation system where you're generating words is you have a problem that there are a lot of words . so from the hidden state what we're doing is multiplying by this which is the size of the vocabulary times the size of the hidden state doing this softmax . and that's giving us the probability of different words . and so the problem is if you wanna you spend a huge amount of time just doing these softmaxes over and over again . and so for instance you saw that in the kind of pictures of the google system that over half of their computational power was just going into calculating these softmax so that's being a real problem . so something people have worked on quite a lot is how can we string the cost of that computation . well one thing we can do is say ha let's use a smaller vocabulary . let's only use a 50000 word vocabulary for our mt system and some of the early mt work did precisely that . but the problem is that if you do that you start with lively sentences . and instead what you get is unk unk unk because all of the interesting words in the sentence fall outside of your 50000 word vocabulary . and those kind of sentences are not very good ones to show that human beings because they don't like them very much . so it seems like we need to somehow do better than that . so there's been work on well how can we more effectively do the softmaxes without having to do as much computation . and so there have been some ideas on that . one idea is to sort of have a hierarchical softmax where we do the standard computer scientist trick of putting a tree structure to improve our amount of computation . so if you can sort of divide the vocabulary into sort of tree pieces and divide down branches of the tree we can do less computation . remember we did noise contrast to destination for words of that was a way of avoiding computation . those are possible ways to do things . once you start taking branches down the tree you then can't do the kind of nice just bang bang bang type of computations down to gpu . so there's been on work on coming up with alternatives to that and i wanted to mention one example of this . 
and an idea of this is well maybe we can actually sort of just work with small so when we're training our models we could train using subsets of the vocabulary because there's a lot of rare words but they're rare . so if you pick any slice of the training data most rare words won't be in it . commonly if you look at your your word types occur only once . that means if you cut your data set into 20 pieces 19 of those 20 will not contain that word . and then we also wanna be smart on testing . so we wanna be able to at test time as well generate sort of a smaller set of words for our soft max and so we can be fast at both train and test time . well so at training time we want to have a small vocabulary . and so we can do that by partitioning the vocab for partitioning the training data each slice of the training data we'll have a much lower vocabulary . and then we could partition randomly or we could even smarter and we can cut it into pieces if we put all the basketball articles in one file and all the foot walled articles in another pile will shrink the vocabulary further . and so they look at ways of doing that so in practice that they can get down and order a magnitude or more in the size of the vocab that they need for each slice of the data that's great . okay so what do we do at test time . well what we wanna do it at test time as well when we're actually translating we want to use as much smaller vocabulary . well here's an idea of how you could do that . firstly we say they're are just common so we pick the k most frequent words and say we're always gonna have them in our softmax . but then for the rest of it what we're actually gonna do is sort of have a lexicon on the side where we're gonna know about likely translations for each source word . so that we'll have stored ways that would be reasonable to translate she loves cats into french . and so when we're translating a sentence we'll look out for each word in the source sentence what are likely translations of it and throw those into our candidates for the softmax . and so then we've got a sort and when translating a particular soft sentence we'll only run our softmax over those words . and then again we can save well over an order of magnitude computations . so k prime is about 10 or 20 and k is sort of a reasonable size vocab . 
we can again sort of cut at least in the order of magnitude the size of our soft mixers and act as if we had large vocabulary . there are other ways to do that too which are on this slide . and what i was then going to go on and we'll decide whether it does or doesn't happen based on the syllabus . i mean you could sort of say well that's still insufficient because i sort of said that you have to deal with a large vocabulary . and you've sort of told us how to deal with a large vocabulary more efficiently . but you've still got problems because in any new piece of text you give it you're going to have things like new names turn up new numbers turn up and you're going to want to deal with those as well . and so it seems like somehow we want to be able to just deal with new stuff at test time at translation time . which effectively means that an infinite vocabulary . and so there's also been a bunch of work on newer machine translation and dealing with that . but unfortunately this class time is not long enough to tell you about it right now so i'll stop here for today . and don't forget outside you can collect your midterm on the way out . navdip did his phd at toronto with geoff hinton the godfather of deep learning . he's done some really exciting work on end-to-end speech recognition . really his name is on most of the exciting breakthrough papers of the last couple of years when it comes to speech . so very excited to have him here . he's now at nvidia and i'm guessing continuing to work on speech recognition . so today i thought i'd give a high-level overview of methods that we're looking at for so here's the plan for the lecture today . i'll start by taking a brief look at traditional speech recognition systems . then i'll give a little motivation and a description of what i mean by an end-to-end model . then i'll talk about two different models for an end-to-end speech recognition systems one by the name of connectionist temporal classification . 
and another recent one based on listen attend and spell which is something i believe you guys are familiar with it at this point . then i'll talk about some of the work we've been doing on making improved versions of these end-to-end models influence speech recognition and some efforts at improving decoding which is an important part of these models . okay so starting with the basic definition of what is in the era of ok google i guess i don't really need to describe it but here it goes . you have a person or an audio source saying something textual . and you have a bunch of microphones which are receiving the audio signals . and what is received the microphone of course depends on how it is oriented with respect to the person . and you get these signals from the devices one or many devices . and you pass it to an automatic speech recognition system whose job is now to infer the original source transcript that the person spoke or that the device played . well firstly it's a very natural interface for human communication you don't need a mouse or a keyboard so it's obviously a good way to interact with machines . you don't even really need to learn a new techniques because we most people have learned how to speak by certain time and of course it's a very natural interface for talking with simple devices such as cars or handout phones . or even more complicated and intelligent devices such as your call center people or chatbots and eventually our robotic overlords . so i think we need a good speech recognition system . okay so how is this done classically . i'll be focusing mostly on stuff we've been doing lately which are all neural inspired models but i thought it would be a nice start by talking about how these things have been built classically and see how they've been replaced with single neural net over the time . okay so the classic model for speech recognition is to build something called a generative model i don't know how many people are familiar with generative models here . perfect so the classic way of building a speech recognition system is just to build a generative model . you build the generative model of language . on the left you say you produce a certain sequence of words from language models . and then for each word you have a pronunciation model which says hey this is how this particular word is spoken . typically it's written out as the sequence of phonemes which are basic units of sound but for our vocabulary we'll just say it's a sequence of tokens where tokens have been a cluster of things that have defined by a linguistic expert . 
so you have the pronunciation models which now convert the sequence of text into a sequence of pronunciation tokens . and then the models feed into an acoustic model a given token sound like . using a gaussian mixture model or they were in the past and these gaussian mixture models with a very specific sort of architectures associated with them . you'd have three state left to right gaussian mixture models that would output frames of data . and these models were now used to describe the data themselves . so here the data would be x which is the sequence of frames of audio features x1 to xt . typically these features are something that signal processing experts have defined things like features that look at frequency components of the audio wave forms that are captured . things called spectrograms and bell filter banks spectrograms that are sort of very similar to human beings . so the classical pipeline proceeds as described . now each of these different components in this pipeline uses a different statistical model . in the past language models they served very well . so here obviously in this class language models for you you have essentially tables describing probabilities of sequences of tokens . the pronunciation models were just simple lookup tables with probabilities associated with pronunciations a couple of pronunciations and if you have an accent in exchange further . so these pronunciation tables would just be very large tables of different pronunciations . acoustic models as i said would be gaussian mixture models and the speech processing was predefined . so the way once you have this model built you can do recognition by just doing inference on the data you receive . so you get some waveform you compute the features for it and you get x you look into your model . and then using some fancy search procedure you figure out okay what's the sequence of y's that would give rise to this sequence of x with the highest probability . so in a nutshell that's basically all the way classical speech recognition systems happen and little pieces are refined . okay so now welcome to the neural network invasion . 
over time people started noticing that each of these components could be done better if we used a neural network . so if you take an n-gram language model and you built a neural language model and feed that into a speech recognition system to restore things that were produced by a first path speech recognition system then results are improved a lot . also they looked into pronunciation models and figured out hey how do we do pronunciation for new sequence of characters that we've never seen before . so most pronunciation tables will not cover everything that you could hear they found that they could use a neural network . and then learn to predict token sequences from character sequences and that improve pronunciation models . so very more production related speech recognition systems such as the google one will build pronunciation models just from rnns . people used to use gaussian mixture models and they found that if they could use dnn or deep neural networks or lstm-based models then you could actually get much better scores for weather frames for real or wrong . interestingly enough even the speech processing that was built with analytical thought process in mind about the production of speech . even those who are found to be replaceable with convolutional neural networks on raw speech signals . so each of the pieces over time networks just do better . there's neural networks in every component but the errors in each one are different so they may not play well together . so that's the basic motivation for trying to go to a process where you train the entire model as one big model itself . and so the stuff i'll be talking about from here is basically an attempt or different attempts to do the same thing . and we call these end-to-end models because they try and encompass more and more of the pipeline that i described before . and the first of these models is called connectionist temporal classification and is in wide use these days in baidu and even at google the production systems . however it requires a lot of training . and recently the trend in the area has been to try and build an end-to-end model that does not require hand customization . and sequence-to-sequence models are very useful for that and help talk about listen attend and spell which is one of these models okay so the basic motivation is we want to do end-to-end speech recognition . it's a sequence of frames x1 to xt . and we're also given during training the corresponding output text y y1 to yl . 
and each of these y's is one of whatever 27 28 some number of tokens letters a b c d e f . not sounds we're trying to going straight towards a model that goes audio straight to text so we didn't wanna use any pre-defined notions of what it means to be a different phoneme . instead these models will start with x and they have a goal to try and model y . so y is just the transcript and x is the audio possibly processed of frequency based processing . perform speech recognition by just learning a very very powerful model p of y given x . so the first model that doing these things is the one at the top . you start with y it's a language model we look into pronunciation models . and you look into acoustic models and you get some scores . these models for end-to-end actually just collapse them into one big model and reverse the flow of the arrows so they're discriminative . you start with data which is x and the features and your goal is to directly predict the target sequences y themselves . obviously this requires a very powerful probabilistic model because you're doing a very difficult inversion task . and i'd say the only reason this is possible now is because we have these very strong probabilistic models that can do that . okay so the first of these models is connectionist temporal classification . this is a probabilistic model p(y|x) where again x is a sequence of frames of data x1 x2 xt . y itself is the output tokens of length l y1 to yl . we require because of the way the model's constructed that t be greater than l . and this model has a very specific structure that makes it suited for speech and i'll describe that in a second . so again x is the spectrogram y is the corresponding output transcript in this case this is the spectrogram . okay so the way this model works is as follows . you get the spectrogram at the bottom x . 
you feed it into a recurrent neural network . you'll notice that their arrows are pointed both directions . this is just my way of drawing out a bidirectional rnn . i'm assuming everybody knows what a bidirectional rnn is . as a result this arrow pointing at anytime step depends on the entire input data . so it can compute a fairly complicated function of the entire data x . now this model at the top has softmaxes at every time frame corresponding to the input . and the softmax is on a vocabulary which is the size of the vocabulary you're interested in . say in this case you had lowercase letters a to z and some punctuation symbols . so the vocabulary for connectionist for ctc would be all that and an extra token called a blank token . and i'll get into the reason for why the blank token exists in a second . each frame of the prediction here is basically producing a log probability for a different token class at that time step . in this case a score s(kt) is the log the probability of category k not the letter k but a category k at time step t given the data x . so you'd have let?셲 say you took x4 here . the probability of if you look at the softmax the first let?셲 say the first index corresponds to the probability of character a . the second symbol corresponds to the probability of character b c and so forth . and the last symbol in this softmax will correspond to the blank symbol itself . so when you look at the softmax at any frame you can get a probability of the class itself . okay so what ctc does is if you look at just the softmaxes that are produced by the recurring neural network over the entire time step you're interested in finding the probability of the transcript through these individual softmaxes over time . what it does is it says i can represent all these paths . 
i can take a path through the entire space of softmaxes and look at just the symbols that correspond to each of the time steps . so if you take the third symbol that's a c in the first time step . if you take the first the third symbol again at the second time step c and then you go through a blank symbol . it's essential that every symbol go through a blank symbol that's a constraint that the model has . and now you go through a blank symbol and then you produce the next character a and then you produce a again for another frame . and then you have produce a blank and you can transition to a t and then you have to produce a blank again . so when you go through these paths with the constraint that you can only transition between either the same phoneme from one step to the next one . or take it from not a phoneme but a label either from the same label to itself or from that label to a blank symbol . you end up with different ways of representing an output sequence . so here we had the output sequence path being representing in these frames as cc blank aa blank t blank . there's many other paths to also correspond to the character sequence cat . so for example if you wanted to produce cat from the sequence of tokens here you could also have produced it from the way the second line here maps it out where you would say cc blank blank and then produce an a then you produce a blank and then you produce a t and then you produce a blank . so all this sounds complicated but really all it is is saying there's some paths you can take through this sequence of softmax and it's got a certain constraint that you have to follow namely you can only transition between yourself and the same symbol again or yourself and a blank . given these constraints it turns out even though there's an exponential number of paths by which you can produce the same output symbol you can actually do it correctly because there exist the dynamic programming algorithm . i'll spare you the details of that dynamic programming algorithm today but it's not as complicated as it sounds i'd refer you to the paper if you're interested in finding out what that is about . anyhow the nice thing about this model is you are able to sort of take in inputs and produce output tokens and learn this mapping because the probabilities can be computed accurately and not only can the probabilities for an output sequence be computed accurately you can get a gradient which is the learning signal that and once you get the gradient from that learning signal you can back-propagate that into the recurrent neural network and learn the parameters of the model . feel free to ask any questions at any time . so the question is are we using processed raw audio or can we can we actually do raw audio or maybe even what do we do in practice . so we found some years ago that we could actually use raw audio however it didn't actually beat the best way of processing the raw audio minimally which would be just computing a spectrogram and then adding the sort of bias that human hearing apparatus has . it turns out the human hearing apparatus doesn't have a linear resolution on frequencies . 
we're able to separate frequencies that are fairly close by in lower ranges and then it becomes logarithmic and you have to be really far apart in frequency space to tell them apart . so if you impose that bias you get a log mel spectrogram instead people have tried to improve upon the log mel spectrogram but the attempts have been not very good when it comes to single channel . there's the case where you might have multiple devices where you have multiple microphones that are recording things and there you can actually subtleties such as one microphone being closer to a person than the other and then it turns out that you can actually use raw wave forms to produce better so i haven't talked about that much here but what you feed in it's really not that important for the sake of calling it end to end . let's just say it's a little convolutional model that works on frames of raw wave forms . so you just take the raw wave form and you split it up into little frames and that works just as well . unfortunately it doesn't work better yet as we had originally hoped unless you have multiple microphones in which case it does . so here's some results for ctc how it functions on a given audio . to his friends and if you look at the model i've aligned or lifted from the paper so we have aligned a raw wave form at the bottom and the corresponding predictions at the top . the symbol h at a certain point it gets a very high probability it goes straight from 0 to 1 so it's confident it's heard there's a faint line here which corresponds to the blank symbol and you'll see that when you want to emit symbols the blank symbol probability starts to dip down to zero . so they swap in terms of probability space because you're not confident you want to produce an h symbol . over time you can see this h now dies down to zero and of course as the audio proceeds you start getting high probabilities for the other characters corresponding to the sounds . to give you some examples on what this is how this looks like when you just take about 81 hours of data and just train to the text corresponding to 81 hour of audio . you are born you listen for 10 days and you start producing text like this . so the target is to illustrate the point that is if you listen eight hours a day which most kids don't . to illustrate the point a prominent middle east analyst in washington recounts a call from one campaign . two alstrait the point a prominent midille east analyst im washington recouncacacall from one campaign . here's another one i'll let you read that yourself we can bootik it's kind of cute and sometimes it gets it quite good . so it's pretty interesting that it produces text that very much is like the output text that you desire . of course it turns out these sound very good . if you read out the transcript it sounds like what you've heard . 
what's missing is correct spelling and also a notion of grammar . so if you had some way of figuring out of ranking these different paths that you produce from your model and re-rank them by just the language model you should get much better . it turns out in this case the original base model had a word error rate of 30% . that means of the words that it was producing 30% were wrong which seems like a very big number but even a small spelling mistake will cause that error . now if you use a language model to sort of re-rank different hypothesis you can get that word error rate done to 8.7% which is just using 81 hours of data . subsequent to this work google looked into using ctc where they actually have a language model as part of the model itself during training and that's kinda the production models you use now when you call in with ok google and that fixed a lot of these issues . and of course they used thousands of hours of data instead of 81 hours of data . there's no such thing as big data or big enough . so if you look at their paper and look at some interesting results when you change the targets instead of using characters you can actually use words . you can have a different see how the recognition system performs . so here the top what the panel is where there are 7000 words in the vocabulary and the bottom panel is where there's 90000 words in the vocabulary . the actual text to be produced is to become a dietary nutritionist . what classes should i take for a two year program in a community college . if you'll look carefully in the panel not sure if it's entirely visible all the way back but these things are color-coded in terms of the different words that were produced in this window and corresponds to the probability . so here there's the blank symbol which again goes up and down depending on weather symbols are being produced or not . it has this word to which is so it's got a high probability . also note that it also is confused a little bit about the word do at the same time . and then it gets to the next word and it produces the word become and then dietary . turns out dietary is not in the vocabulary size of 7000 so it just produces diet which is in the vocabulary and you'll see this row here corresponding to diet being yellow . it doesn't have dietary but just produces terry as a response . 
but overall it gets most of the words that it's expecting correct . if you increase the vocabulary size to be large enough so that dietary is in the vocabulary you find that it actually also produces dietary as an output in there although it also produces diet . so a language model would fix that kind of an issue . so that's what i have to say about ctc . i'm afraid i have to switch here and . i promise to manage with four switches so now switching gears in terms of the models . but if you were paying attention very carefully from a modeling perspective you'd find that the model makes predictions just based on the data . and once it's done with making those predictions for each frame there's no way of adjusting that prediction . it has to the best it can with those predictions . an alternative model is the sequence-to-sequence models which you guys have been reading about from looking at your lectures . so i won't talk too much about the basics and jump straight in . we have a model here which basically does next step prediction . you're given some data x and you've produced some symbols y1 to yi and your model just going to predict the probability of the next symbol of yi+1 and your goal was basically to learn a very good model for p . and if you can do that then you have a model for p of any arbitrary y given x . so this model that does speech recognition with the sequence to sequence framework changes x . in translation this would be a source language . in speech the x itself is this huge sequence of audio . that is now encoded with a recurrent neural network . what it needs to function is the ability to look at different parts on temporal space because the input is really really long . i think if you look at translation results you'll find that translation gets worse as the source sentence becomes larger . 
it's because it's really hard for the model to look precisely at the right place . turns out that problem is aggravated a lot more when you have audio streams . typically a frame is like you have a hundred frames for a second . something that's ten seconds long that's about a thousand . token thousand input times are long as opposed to . in translation you might have like 40 token words that you're gonna translate . so it's a very aggravating problem and you need to do attention if you wanna make this model work at all . whereas with translation you can get by without attention . so how exactly does attention work here . you're trying to produce the first character c and you have this way of producing an attention vector . i'll go into that shortly how that's done but it's fairly standard . this attention vector essentially looks at different parts of the time steps of the input . here it's saying i want to produce the next token . and to produce that token i should really look at the features that were over here and the features that were over here . so once it looks at the features corresponding to those time steps it's able to produce a character of c and then it feeds in the character c to itself and then produces the next character which is a after changing the attention . the attention now looks further down from where it was looking at the first time step and then you feed in the character a into your model . and again you recompute attention and it automatically just moves forward once its learned . so if you keep doing this then you the moving forward attention just learned by the model itself . so here it's producing the output sequence cancel cancel cancel . the question was are we no longer doing predict the previous token or the break . 
which is a sequence to sequence model you feed in the entire data as an input conditioning and there is no notion of consuming a little bit of the input and then producing output . instead the entire input is looked at every time step . and so you don't really need to add breaks anywhere . you just produce one token then you produce the next one condition on the last token you produced . so going back it's essentially doing next step prediction . so it's just doing next step prediction in this model you have a neural net which is the decoder in a sequence-to-sequence model that looks at the entire input which is the encoder . it feeds in the path symbols that you produce because it's a recurrent neural network . you can just keep feeding symbols and the length issue does not arise . so you fed in the path symbols as the recurrent neural network and then you're just predicting the next token itself as the output . i need to switch a third time . this is the second last one i promise . so how does this attention model work . so firstly you have an encoder on the left-hand side . it seems to have a special structure . i'll go into that in a few slides . and just remember that for every time step of the input it's producing some vector representation which encodes the input and that's represented as ht at time step t . at time step t and you were generating the next character at every time step with the decoder . so what you do is you take the state vector of the decoder . so bottom layer of the recurring neural network that is the decoder . and you now compare the state vector against each of the hidden time steps of the encoder . 
and so semantically what that means is you kind of have this query in mind which is this state s . and you have places ht that you're looking at as possible places where the information is present . so you take this query and you compare it to every ht . you could have done something very simple like take a dot product in which case the vectors don't have to be the same sort of size . or you could have done something much more sophisticated which is you take the hidden state that you want to compare the query against concatenate them into a vector . and then put them in a neural network which produces a scalar value and turns out that's what we do . so you basically have this function here function f which takes in a concatenation of the hidden state at a time step t . with the state of the recurrent neural network which is the decoder state and then produces a single number e of t . now you do that for every time step of the encoder and so you have a trend in time in the encoder space . and that's kind of like a similarity between your query and so you get this trend eft and of course these are just scalers . yeah you want to keep these magnitudes under control . so you can pass them through a softmax which normalizes across the timesteps . and so you get something that sums to one . and that what's called the attention vector that turns out showing you what's basically the trends of these attention vector as the query changed overtime . so every timestep you got an attention vector which shows you where you look at for that timestep . then you move to the next timestep you recompute your new attention vector and you do that over and over again . so now that you an attention vector what you can do is now use these probabilities over timestep to blend the hidden states together . and get one context value which is this representation that is of interest to you in actually doing the prediction for that time step . so here you would take all the hidden states and the corresponding attention value . and just multiply them and add them together and that gives you a content vector . 
and this content vector is really the content that will guide the prediction that you make . so you take this content vector you can catenate that with the state of your r and n . and you pass it through a neural net and you get a prediction at that time step . and this prediction of course is the probability of the next token given all the past tokens you produced and all the input that int was fed into the encoder . this is exciting for me i don't have to switch after this . okay so now what's this funny business with the encoder . you're used to seeing a recurring neural network which basically proceeds at the same time step as the input . so you got an input you process with through some hidden stage . you passes through one occurrent step and you move on . we found that for when we did that for audio sequences that we're really long such as realistic speech in wall street journal which is one of the speech corpora . things just wouldn't go off the ground that makes sense because you have a lot of timesteps to look over . so typically you'll get something like 7 seconds which would be 700 frames and you're doing a softmax over 700 timesteps . it's hard for you to sort of initially learn where to propagate the signal down to so it never really catches on very fast . so this hierarchical encoder is a replacement for a recurring neural network . so that just instead of one frame of processing at every time step you collapse neighboring frames as you feed into the next layer . what this does is that every timestep it reduces the number of timesteps that you have to process and also makes the processing faster . so if you do this a few times by the time you get to the top layer of the encoder . your number of timesteps has been reduced significantly and your attention model is able to work a lot better . and i specifically want to out the outputs are very multimodal . and what do i mean by that . 
so you have an input the truth is call a a a roadside assistance . the model produces call a a a roadside assistance as the first output . but it also produces call so the same audio can be used to produce very different kinds of transcripts . which is really the power of the model and says how this model can actually and actually solve this task with just one model instead of requiring many . so interestingly if you look down you'll see the reason why this model is able to produce call a a a and call triple a . because the training set has a lot of call xxx . and so the model learns a very specific pattern in a table to sort of transfer that to a different domain . another aspect of the model is causality and what do i mean by that . here you have an output which is st . mary's animal clinic which is the transcript . so if you look at the attention over time when you produce the first token s . it just look said this little blob at the top and then when you look at t it looks moves forward at the pretty center of the character . and the models per multimodels so instead of just producing st marries . it can also produce saint mary which is a truly different transcript from the same audio . and now if you look at where the attention and then start moving here it's the same word saint . so it actually dwells at the same timestep in attention space . so that's a notion where whatever symbol you've produced really affects how the neural network behaves at the next few timesteps . and that's really a very strong okay the question is the fact that the model produces two different transcript a result of the fact that there's ambiguity in the pronunciation model itself . so i think that is essentially what the model tries to capture . the fact that the same word can be pronounced multiple ways or that the same pronunciation can be written out multiple ways are sort of two different but related problems . 
one is different pronunciation producing the same token which does not require multimodality in a model as long as one source of sound what's happening here is more interesting in that the same sound can be during training clearly it must have heard both sides it's heard saint written out as s-t and another time it must have heard saint written out as an s-a-i-n-t . so you need it in the training data but what's nice is the model's really powerful enough to realize that the same sound can actually do this and it can actually produce very different transcripts . so when we did this about a year and a half ago these are old results we found that our model was when you didn't use a language model it around 14% whereas the best system that we had in google at the time was 8 . at this point i would say i was an intern at google many years ago and then the word error rate was 16% and that was a result of 45 years of work . where people had customized all the speech recognition components very carefully for all these years . and this model by one just one single model that just goes straight from audio to text gets a lower word error rate than what we were getting in 2011 . so that i think something to write home about which is pretty exciting . turns out it still benefits from having a language model so if you feed in a language model the 14% word error rate comes down to 10.3 . so obviously there's no substitute for billions and billions of written text sentences in trying to disambiguate speech recognition better . but just the basic model by itself does very well . so now what are the limitations of this model . one of the big limitations preventing its use in an online system is that if you notice the output is produced conditioned on the entire input . so you get this next step prediction of xt plus 1 given all the input x and all the sorry yt plus 1 given all the input x and all the tokens you've produced so far which is y1 to yt . so it's really just doing next step prediction but the next step prediction is conditioned on the entire input . so if you were gonna try and put it in a real speech recognition system you'd have to first wait for the entire audio to be received before you can start outputting the symbol . because by definition the mathematical model is the next token in this condition on the entire input and the past tokens . another problem is that the attention model itself is a computational bottleneck for every time stamp you have to look at the entire input sequence . long as the length of the input which makes it a lot slower and harder to learn as well . further as the input receives and becomes longer the word error rate goes down . this is really an old slide this doesn't happen much anymore . 
but i'll talk about the methods we came up with to prevent this a little later . so i'm gonna now switch gears to another model which it's called the online sequence to sequence model . this model was designed to try and overcome the limitations of sequence to sequence models where we don't want to wait till the entire input has arrived before we start producing the output . and also wanna try and avoid attention model itself over the entire sequence because that seems to be an overkill as well . so you want to produce outputs as inputs arrive and it has to solve this problem which is am i ready to produce an output now that i've received this much input . so the model has changed a little bit not only does it have to produce a symbol it has to know when it has enough information to produce the next symbol . so this model is called the a neural transducer and in essence it's a very simple idea . you take the input as it comes in and every so often at regular intervals you run a sequence to sequence model on what you received in the last block . and so you have this situation here where you basically have the encoder . and now instead of the encoder attention looking at the entire input it just looks at this little block and this decoder which we call the transducer here will now produce the output symbols . now notice that since we've locked up the inputs we have this situation where you may have received some input but you can't produce an output . and so now we're sort of we need this blank symbol back again in this model . because it really can be the situation where you got a long pause you haven't heard any words so you really shouldn't be producing any symbols . so we reintroduce this symbol called the end-of-block symbol here in this model to sort of encapsulate the situation that you shouldn't be producing any outputs . one nice thing about this model now is that it maintains causality . so if you remember the ctc model it also had this notion of not producing any outputs . but when you produce these symbols you did not feed back what and so it didn't have these notions where the same input can produce multiple outputs . and it didn't have the notion of causality where depending on what you produce so far you would really just change the computation they're on . so here in the neural transducer it preserves disadvantage of and it also of course now introduces an alignment problem just like these slides have an alignment problem too . so in essence what you want to know is you have to produce some symbols as outputs . 
but you don't know which chunk should these symbols be aligned to . and you have to solve that problem during learning . i want describe this very carefully but i'll make a go off it . y1 to s that have to be produced and now if you have an input that is b blocks . you can now output these s symbols along with b end-of-block markers anywhere to describe the actual alignment in the way the symbols are produced . and of course there's a combinatorial number of ways in which you can align the original input to the actual block symbols . so the probability distribution turns out to be the probability of y1 to ys given x is modeled as the sum over all the different ways in which you can align y1 to s to the original b blocks . and the b the extra b in length comes from the fact that there's b blocks and an h block ends with an end-of-block symbol . so now it's similar to ctc you have some output sequence . you can produce them from a bunch of different ways . and all of those ways have some probability and if you have to learn in spite of that model . unlike ctc of course this model is not independent at every time step . once you make the predictions you feedback your previous tokens that changes the entire probability distribution at the next timestamp . what this means is there no decomposition between different parts of the input given the data . so you can?셳 really do a dynamic programming algorithm that just simplifies this computation . so we came up with a simple way of doing this approximation of the sum which was let's just find the best possible alignment given your current model . so you basically try and do some kind of a beam search . and you find the best paths as the output and then you use that during training . okay so sorry one more point that i should make . that's the same process we used during inference . 
the model itself is you want to produce these symbols y1 to ys you can do it in a variety of ways . during inference you find the best one and you go with that one as being the actual transcript . during learning if you take a gradient of that combinatorial sum it comes down to this particular form which boils down to saying . give me a sample from all the probability of aligning the outputs given the input probability of that sample . if that sounds like greek i wouldn't worry too much about it but i'll say it one more time . it's basically this happens in cases where models are a sum of combinatorial of terms . if you want to optimize such a model you basically have to take the gradient of the log prob and the gradient of the log prob turns out to be a sum of the log probs over the posteriors of the samples . and that's the case in this model as well . of course this is really hard to optimize and so we replaced this entire really complicated procedure for optimization by giving an output symbol find the best alignment and just make that alignment better it's kind of like a terby sort of trick . so i'm just gonna skip this part . turns out if you want to find the best path there's also a combinatorial number of ways and so what you can do is kind of do a beam search where you keep a bunch of candidates around and then you extend them and as you extend them you now re-rank them and throw away the best one . however turns out if you do beam search it doesn't really work . and so what we discovered was a dynamic programming which is approximate that works very well . and the way this works is you consider the best candidates that are produced at the end of a block from producing either j-1 tokens or j-2 tokens j-1 tokens or j tokens at the end of block b -1 . so you know that if i wanted to produce j-2 tokens at the end of the previous block what's the best probability . and now that corresponds to this dot here . so from that dot you can now extend either by one symbol or by two symbols or different paths that reach the source . and so now when you're considering the different ways of entering a source you just find the best one and you keep that around . it's kind of an approximate procedure because this ability to extend a symbol is not markovian . and so if we take this max as the max of the previous step extended by one that maybe wrong because the correct that would've been better . 
however it seems to work very well to find an alignment that trains the online sequence-to-sequence model properly . so some results on this model if you change the window size that's how the block is constructed . you find that using different block sizes when there's an attention model mixed model work very well . so in these blocks we can have attention instead of just running a sequence-to-sequence . so it's not affected by the window size of the blocks . and those are these lines at the bottom . it also turns out that you don't really need attention . if the window size is small which side steps this problem of doing an attention over the entire input sequence . and that's where we're really trying to get that is to try and get the model that could do sequence-to-sequence output symbols when it needs to but really not have to do all these computation with respect to the length . so that was basically the online sequence-to-sequence model . i wanna touch a little bit about how you can make the sequence-to-sequence models better themselves . one of the things that people are doing these days borrowing from vision is to use convolutional neural networks . so in vision related tasks convolutional neural networks have been very powerful . some of the best models for object detection and object recognition use convolutional models . they are also very effective in speech so we tried to do this architecture and speech for the encoder side of things . so you take the traditional model for the pyramid and instead of doing the pyramid by just stacking two things together you can actually put a fancy architecture when you do the stacking . so don't just stack two times step and feed it to the next layer but instead stack them as feature maps and put a convolutional neuronetwork on top . i think you guys have not been exposed o convolutional neuronetworks yet but let's just say it's a very specific kind of model that looks at some subset of the input instead of the entire input . and so the subset that it looks at has to be matched to the structure . so if you are in a task such as vision an image patch is natural structure or substructure to look at instead of the entire image . 
for speech also if you look at the frequency bands and the time stamps of the features that corresponds to a natural substructure to look at . so convulational model will just look at the substructure . so what we did in this work was to say now we're gonna change this computation which is a pyramid and add a lot of depth to it by adding this convolutional architectures in every step . so in the past it was just a simple linear projection of two time steps together but now it's a very deep convolutional model which of course for deep learning experts is great because the believe the deeper the number of nonlinearities the better it is and this model actually has a lot of that way . when we did that we found very good results . if you take a baseline on a task called wall street journal it goes from something like 14.76 word error rate down to 10.5 just by using this very specific trick on how to do these convolutions . so deeper continues to be a good model . okay now switching to what is the output space that's a very appropriate one for speech . so in translation what happens is there's multiple ways people have discovered on how to encode the output sequence . you might produce character sequences you might produce words and character sequences or you might produce a subset of characters and use that as the output vocabulary . in speech that seems not natural because what you want to do is you want to produce output tokens that corresponds to some notion of the sound that was being produced in the input . so what you would like to do is change the vocabulary . so it's not just characters but maybe bigrams or trigrams of characters that corresponds to some audio token . so basically these are i guess the slide is talking about the different ways to do it . as i said you can either represent the word and characters but for speech you want to use n-grams . however there's a problem here should we decide the end graphs before hand and then just use those during training . that kind of defies the end to end model where we want to actually learn the entire process as one big model . so we decided okay what we could do is build this bigrams trigrams and whatever number of n-grams of characters and put them in a softmax . and now the problem arises if you have a word like hello it can be decomposed in multiple ways . might spell as h-e-l-l-o or it might spell as he-l-l-o if he happens to be in a target set that you've chosen . 
so it's really indefine undefine problem or it's a problem where now you have to deal with multiple ways of out putting the same sequence . so how should we make this choice . one way of making this choice is you get a word such as cat sits . you just look in your token space . if you have ca in your tokens you just say i'm gonna choose ca as the input . the you do t and then si and then t and s . so this is just very greedy in terms of how you produce the tokens . another way is to look at the compression the sequence of tokens that here it's basically about reuse it's like encoding . and you would just sort of use a minimum description line . so at happens to be a lot more frequent than ca so you would rather choose at as a token . in this case the decomposition for cat sits would be c at si and t s . that would be another way of course it's not clear for the audio which is the best way . so our approach was try to learn this automatically . so you have some output y* which is the correct output sequence and you try out all the possible decompositions of the same output . so you basically look at all possible ways of producing the token . and when you do the learning you take the gradient of all possibilities of producing the output sequence and propagate that error signal down . just to know when this class end . if you look at how this model performs turns out it helps to use larger n-gram pieces . so if you take a character based model which was just ctc with no language model it would have 27% word error rate . if you take the sequence to sequence rate of the last model with character output it produces 14.7 word error rate . 
if you used a 2 gram it does better with a 3 gram it does it even better . the 4 gram does better on training but worst generalization presumably because our dataset was really limited . it's just 81 hours of data and once you start using larger and larger tokens . you can imagine it doesn't enough evidence for similarly for 5 grams . to show you an example the actual test is shamrock's pretax profit from the sale was $125 million a spokeswoman said . the character model produces shamrock as c-h-a-m-r-o-c-k . the 4 gram model will take the sh sound straight up as sh which is nice . and then does a lot of these things with single characters . but you can see common and bigrams and trigrams being used as a result of this . if you look at whether or not the model is actually using it . numerically you find out if you had trained the model by just one kind of algorithm where you just took the maximum extension . it used the n-grams more because it trained to use these longer n-grams . however if you look at the results that come out where you actually learned to use the n-grams . it still does a better job does a good job of learning the n-grams and it gets a lower word error rate . so that's quite promising in terms of achieving what we wanted to do . so now i'm switching gears here and going into some of the final shortcomings of sequence to sequence models when they're applied into speech recognition . if you look at the transcripts that are produced in terms of the probabilities of every token you find an interesting pattern . here at the top is the actual sequence . so south africa the solution by francis candold and leoguen low . it's actually this is not the right solution this is the highest probability one . 
below each token you have the alternate tokens that had a probability which was some thresholds lower than the probability of the best token . so if there's no tokens below a token that means there's no ambiguity . it's really sure that that's the token that's got the right answer . if there's many that means it's a little confused at this part of the token when it's producing the next token . so you find a very interesting trend that there's a lot of ambiguity at the start at the first characters . but as soon as you produce the first few characters there's very little ambiguity as to what the next characters are . unless it's things like names so francis here there's some confusion on how to sound it out and candold has some probability issues as well . so this might seem surprising but it's natural . if you're doing language modelling on a character level . once you have the first few characters of a word you pretty much know what the word is . and so what you really wanna do is produce these be much more discriminative at the starts of words instead of because that's where you'll make an error . if you make an error at the start of a word you're never gonna recover from it . and so if we want to fix this problem we need to sort of address this issue . the repercussion of this problem is that if you're over confident about the wrong word not even a language model can help you . because you've basically decided early on what the word is going to be . and you need very precise language model probabilities to kind of get you out of the rut . so we've found that there's this little technique called entropy regularization . which prevents your softmax from ever being too confident that really just solves this problem . so every time you predict the next character you make sure you're not becoming probability of one for one symbol . instead you say if you're getting to confident and penalize it that you're force to spread the probability of distribution to the other characters . 
once you do that this problem really just goes away . and the baseline model that we had just improved massively . so if you remember we had ctc on an intent task on wall street journal which had some error like 27.3 . then we had there's a baseline sequence to sequence model that wasn't ours . but yoshua bengio's group that had an 18.6 word error rate . our baseline for some reason was 12.9 where error rate then once we applied this technique of entropy regularization that error rate went down to 10.5 . there's different ways by which you can do this regularization . and other is you say the probability distribution must look like the unigram probability distributions of the alpha tokens . and that seems to work better than just doing fully entropy regularization . another big problem that arises during decoding is this lack of generative penalty . i think there was a slide in your translation lecture which talked about this in a different setting . but i'll talk about it in the context of audio . when you have a very long input sequence and you're decoding it one character at a time what you're doing is you're comparing your hypothesis against all alternative hypothesis . so every time you produce a new token you pay a cost for that extra token . if your input's very long then you're gonna obviously associate it with a long input have to produce a lot of tokens because probably the transcript that you're producing is very long . so let's say you have to produce 100 transcripts and you're paying an average cost of one . that means you're gonna pay a cost of 100 for even a correct transcript . now in your beam search you'll probably get some examples which say the end-of-token symbol has a probability less than minus 100 . i think this is a very subtle point but the upshot of it is your model thinks it's okay to terminate an output without even looking at the rest of the input when it's not . and the reason this happens is the model has no notion of explaining the entire input . 
and because it doesn't have to explain the entire input it just terminates early . and very often you'll produce very short output sequences when you should be producing very large output sequences . so to give an example if the output transcript is chase is nigeria's registrar and the society is an independent organization hired to count votes if you look at the language model cost it's minus 108 . if you look at the model cost it's this is just the model from the last model which is minus 34 . and you look at the other alternatives you get chase is nigeria's registrar which has a low cost of minus 31 . so it's just happy it just produce a short token instead of this really long one . if you look at chase is nigeria's or chase's nature is register that also has a small probability . in fact the best probability is just to produce nothing which is -12.5 . so this is tying up in the issue where discriminative models don't explain the entire data and what we found worked quite penalty which says i'm gonna try and make sure the sum of probabilities . so look at all the frames of the input has someone looked at them or not . nobody's looked at them or at least with some threshold tell and when you do that all these other hypothesis that terminate early are now paying a lot of cost for every frame that did not explain and so they fall down and our re-rank the out . so we need do this little trick our model now really performs quite well . this model for sequence to sequence is now able to get six point seven word error rate on the wall street journal . which is the lowest for end to end models including ctc with a language model . it's very very promising in that sequence to sequence can achieve such low numbers with such low amounts of data . i should point out that this you train a model with one objective but during test time you really fiddled with the loss that you claimed was the best one . so technically there's something wrong with the model that needs fixing . so finally something very relevant to an nlp class is what do you do about language models here . no matter how much audio you have and that text can be really useful in correcting up errors that this model will have . so i could train on 2000 hours of data that would just be about 3 the language model you learn from that is not gonna be very powerful . 
so the speech organizer is going to make a mistake no matter what . since i'm an nvidia now i cannot take claim for that not working . how can we do better language model blending . in these models cuz they're really end to end models and you just are basically training the entire tasks you're doing an acoustic model . you're doing a language model all in one model and now suddenly you're saying hey can i please revert back and find ways to add my language model in here . well one obvious way is to add the law of probabilities every time you do a next stop prediction . you make a prediction for the next time stamp and then you blend in a language model prediction with it and then hopefully that fixes a lot of the errors . there's some cool work from yoshua benjio's group which tries a bunch of other tricks to do this which they call shallow and deep fusion models . it's basically an extension of the simple idea that i've just described but it kind of does a little bit more in how the blending is done . instead of just blending the actual feature the actual log probs it uses a model that does a linear projection and it learns that together . you basically have two streams of predictions and you combine them . however one of the things that's lacking about this model is that the model in this case it's for translation . the translation model predictions don't actually affect the internals of the language model and visa versa . what you would like to do is have a model that where diffusion actually means changing of all the features that are computed rather than just summing the logic i apologise i forgot what the slide was . so when you're producing very long sequences with next step prediction what it's doing is just looking at the next token and i highlighted why that can be a problem . for example when you producing words if you produced two characters the next one is almost necessarily just decided right away . so when you have a lost function that's just looking at the next step it doesn't have a very long horizon . it might just go into a wrong path that it can never correct out of . so people have been looking at how to fix this problem . one of the ways is scheduled sampling . 
and i think you guys have looked at this or at least talked about this in the last lecture . but what it does is instead of taking the model and feeding in the ground truth at every time step you feed in the predictions of the model itself . so what you're learning is during test time i'm going to screw up and feed in something wrong . at training time let me do the same so that i'm resilient to that kind of mistake . what that does is it's actually changing your model so it respects long range structure much better . there are other methods also which would be based on reinforcement learning that optimize the word error rate directly . i wouldn't talk about this at all but other than to say that it's another way of letting your model roll forward and generate a bunch of different candidates and than compute an error based on the candidates that you've computed . a very cool paper is this one called sequence-to-sequence as beam search optimization . and this kind of also runs your model forward but with some interesting tricks where it runs the model forward until you've made some mistakes and then it stops and does it again . so finally what can we do with this model that we couldn't have done before . there's some things that you can do one of which is multi-speaker multi-channel setup that people don't really do yet . so flashback to some years ago the motivating problem for speech recognition was this thing called the cocktail party problem . you wanted to be in a room walking around and listening to a bunch of things happening and get your recognizer to produce all the entire output . somewhere along the way people forgot about that particular problem . and they've been focused on just single microphone set-ups where you're stationary with respect to another speaker and you kind of produce only one output as a transcript . i tell the reason this happened is because you have a generative model traditionally which has some transcription mind and then it generative describes the data . and that's not very natural way to sort of do the inverse problem where you can mix in a bunch of people in many different ways . the inverse problem is just follow track one particular individual and that's much easier to do than try and sort of invert a generative model where you have a bunch of sources . so a model such as this such as sequence model should work very well multi-channel set up . and then there's this really cool which talks about direct translation and transcription at the same time . 
so it takes in audio in french and it produces english text as an output with just one sequence-to-sequence model which just blends the last model and a translation model together which is quite exciting . if you look at their paper it's got this really exciting attention plot . so if you take the neural machine translation attention model they're translating how much is the breakfast to com bien coute le petit dejeuner whatever . if you look at the attention model it's looking at the right words . how it pairs with combien much pairs with the two of them as well . if you look at the corresponding attention model from the wave form and the text it also focuses on the same sort of parts . even through it was trained in a different way with a different modality as input . so it's really cool that is able to learn to focus on the audio even though it?셲 translating at the same time . most of this work was done at brain at least the ones i was involved in . yeah google brain gets a very fantastic bunch of students going by and i'd be lucky to work with them . and the google brain team is a phenomenal place to work at . class i think next quarter on just networks for computer vision . where they've really changed the entire field . in nlp they've had some impact but not as much which is why we don't have the entire lecture on just cnns . but they are an interesting model family . they are paralyzable they're very good on gpus and so we'll sort of look into them in detail today understand hopefully by the end why they're so useful . and fast to implement on gpus but really also give you at least some intuition to be honest there's much less intuition behind some of these very advanced cnn architectures compared to even some of the recurrent networks and lstms that we had . so we'll actually start today with a mini tutorial of azure and gpus we wanna encourage you all to really get started on that as soon as possible . also thanks everybody for filling out the survey i think this one is one of the important take away messages which is overall what do you think of the pace . we're very happy to see that the majority are quite happy with the pace . 
it's kind of impossible with such a large class to not be too fast and not too slow for 100% of everybody since people have vastly different backgrounds . very sorry for the little less than a third i think for whom it's too fast i hope today will be not quite as fast . and hopefully in office hours and so on we can make up for some of that . so we'll talk about a couple of different cnn variants . we'll have a fun research highlight on character-aware neural language models . and then we'll actually also look a little bit into tips and tricks that are slightly more practical and you'll observe that these practical details and tricks actually making this particular cnn architecture work are super important and without it you really lose 10% or so of accuracy . at some of the evaluations that are going on in the field and then i will compare a couple of different models which will lead us to a very new model called the quasi-recurrent neural network for treaty which just came out a couple of months ago . with that i?셪l do one organization slide before we go onto azure . so project advice office hours i would really encourage everybody who?셲 doing a project to now come to project advice office hours every week . i?셶e asked groups that i?셫 mentoring personally to also as a server requirement . not all the groups were able to come every week . i am keeping track of whether you're there . so also for everybody who basically is still undecided whether they should move on with their project you'll see kind of where pa4 folks should be at in the next week or so where you have to have run some baselines on your data set by now if you're doing your final project . if you don't even have your dataset ready yet you can't even run a simple let's say bag of vectors kinda baseline it's starting to be really worrisome so definitely make sure you start running your experiments . some simple things baselines could be just any just could be your regressions . you download some code somewhere but you need to make sure you have your data set ready . and for pa4 folks we actually enforce that with a little additional deadline just to make sure you're really all . going to be able to run it cuz this is not one of those things that you can cram really hard and you work 10x and so you make 10x to progress because your experiments will take a day to run and so you run for one day . turns out at the end you had a bug and then the deadline was there and you have nothing . and we really want to make sure it doesn't happen this year even though we're a bigger class . 
also in terms of positive motivation there's actually going to be a really awesome poster session that we're putting together we have corporate sponsors that give us some money and they will allow us to basically give out price have prices for you . we'll make it public so hopefully a lot of folks will show up and check out your research . it's a lot of excitement both from various companies vcs if you have a really awesome poster who knows at the end you may have some funding for your start up . and we'll have food also very nice catering so should be really fun poster session so hopefully you can be very excited about that and your projects . will there be enough food for everybody . we?셪l spend thousands and thousands of dollars on food we hope there will be enough food for everybody . any other organizational questions around the poster areas project . all right then take it away on the gpu side . this is just intended to be a short public service announcement basically about how to get started with azure and why you should get started with azure . by now every team should have received an email to at least one of your team members probably to your stanford one of your stanford emails and you'll have this message which is basically an invitation to join our cs224n subscription . and using following the instructions of this email you should sign up for basically gpu access . so far only 161 people have signed up or teams have signed up out of the 311 and essentially we want this number to increase because everyone should be using gpus for reasons that we'll cover very shortly . and if you have any issues signing up then please report the problems that you have to piazza post 1830 which has the form also screenshotted there and we'll help you essentially through any of the problems that you have with their subscriptions cool . so then more important question that we're gonna go over is why should you really care about the gpus . well first yesterday we actually announced the milestone for the final project and the homework . very quick and easy just a paragraph of what you've done . but we expect you to have used i always experimented with running some code on a gpu like that and this will be worth essentially 2% of your final grade just if you do it or not . but really down there the better reason of why you should be using gpu's is gpu's will train your models much much faster over a much much larger data set . and specifically microsoft has offered us i think 311 mb6 instances on their 0 cloud . these use tesla gpu's m60 if you're interested in the model . 
the specifications are they have a huge number of cuda cores a huge amount of graphics memory and they cost a huge amount of money each . you also get a nice cpu as well as a lot of system memory to go along with your instance . and the key takeaway here is that these ar not your average hardware that you have in your local machine . there's gonna be way more power in terms of the cpu in terms of the gpu and in terms of well even for the gaming whatever hardware you have at home . and the speed-ups will be 10 to 20 maybe even 100 depending on the libraries that you're running . so in conclusion please do get started on azure as soon as possible fill out the form if you run into subscription issues come to office hours or file support tickets if you have technical problems such as not being able to etc . and then also see our step-by-step guide to just get started with the process . for homework 4 the full assignment handout will go over essentially all the details . but decent models will take a long time to train . they'll take one hour plus per epoch even on a strong gpu such as the previously described ones . you'll be spending a week basically just to train a baseline model . and for the final project if your project is sufficiently challenging that needs well you have enough data or your problem is sufficiently challenging you really do want to use a gpu if you want to receive a good score in this class . and by decent model he also means decent implementations so if your implementations isn't super well-optimized so again not something you can cram on in the last couple of days of the class . all right any questions about azure or what if we're not in the same group between homework three and four . so recurrent neural networks were pretty awesome and are pretty awesome actually and a lot of times the default model but they have some issues . namely they can't really they can really only capture a phrase so what do we mean by this . if i want to have just a representation of my birth in this whole sentence well recurrent network will and so that phrase vector up there isn't going to just capture my birth it will also capture the country of . and so sometimes when you have simple classification problems you might actually just want to identify that there's a certain word or phrase in that over all document and just try to give the fact that that phrase exist in your overall document to somewhere higher up in the final classifier that actually needs to classify this . but here you will always go from left to right or even if you have a bidirectional one you go from right to left . but then you have the same problem but on the other side . 
namely the intermediate the words in the center of a longer document might get lost . and of course if you're using lstms are better at doing that they're better able to say don't turn on the forget gate keep some things around keep certain units on when you see something . but it requires a lot of the model to be able to do that perfectly . and so in many of the cases you will see your classifiers only at the very end once it has read the whole sentence and that is not the issue cuz now again the grading has to flow through this . and despite all the [inaudible] and lstm it's even hard for them to keep very complex kinds of relationships alive over many many different time steps . now the main idea here is instead of computing a single representation of vector at every time step that captures basically the context on the left so far what if we could just compute a phrase vector for every single phrase that we have in this sentence . so if we have here the phrase a country in the very first step of this kinds of convolutional networks if vector for the country just this two words in isolation . just country of my birth so basically compute a vector for all the by grams in the sentence and then another one maybe for all the trigrams country of my birth the country . and then for all the fourgrams the country of my birth . so hoping that if this was for instance sentiment classification that one of these said not very good for instance . and then if we captured that vector and we kind of will try to eventually . handle and push that vector all the way to a softmax through some other forms that i'll describe soon . but that is basically the idea of the very first layer of a convolutional network for nlp . and this will basically compute these phrase vectors regardless of whether that is a grammatical phrase . so we know from parsing for instance certain phrases like a country of is not really a proper noun phrase it's sort of an odd ungrammatical chunk but this motto really doesn't care linguistic or cognitive possibility in any kind of way for language . and so people don't read sentences that way but you might be able to eventually compute several of these representations in parallel . and that's going to be a big advantage . so once we compute all these vectors we'll group them after but we'll get to that in a second . so you might ask what is convolution anyway . and so here is a very simple definition for any convolutional operator . 
case of a 1d discrete convolution of a filter over another function at a specific point in time . you'll basically have a filter size here m and you'll basically multiply just a filter at different locations of this input . and so in computer vision that will help us to extract very meaningful features such as edges from an image and eventually more complex features . and for 2d example which you'll observe a lot in computer vision we have this really great animation here from the stanford unsupervised feature learning and deep learning wiki page . so imagine you had an image that you see here in green . and that image let's say is only binary . the first row of this image is 1 1 1 0 0 and the second row of pixels of this binary image is 0 1 1 1 0 and so on and you have a filter . and this filter here has number that you'll see in the small red font here and i?셪l turn the animation off for a second so we can look at it without moving . now the filter here is basically 1 0 1 0 1 0 1 0 1 . and now every time step of the convolution we?셱e going to multiply the numbers of the filter with the numbers off the image . we multiply again the red numbers from the filter with the images with the image values and that will result basically multiply all of them then we sum them up . so very simple in our product if we were to vectorize these three by three blocks into and nine dimensional vector and we just have a simple inner product between those two vectors or we just multiply them here and then sum them up . so one times one plus one times zero plus one times one and so on will sum to four . and we'll basically move this filter one time step one pixel at a time across the image . so let's look again this looks like basically multiply all the numbers and then sum them up . and then we'll move one down and again move from left to right . what would be the equivalent of a pixel in lp and yes you're exactly right it will be a word vector . before i jump there are there any more questions about the general definition of convolution yeah . how do we decide on the convolution . so how do we decide what matrix it is . 
of filter here these red numbers are actually going to be learned . so you have an input and then you do back propagation through a network we'll get to eventually it'll have the same kind of cross entropy error that we have for all the other ones . it'll have a softmax and we're going to basically back propagate through this entire architecture and then we'll actually update the weights here in this particular example in red . after they started with some random initialization . and then we'll update them and they'll change . and what's kind of interesting in computer vision which i won't go into too many details in this class but in computer vision they learn eventually to detect certain edges in the first layer . in the second layer they'll learn to detect certain combinations of edges like corners and the third layer they will learn to basically detect and have a very high activation these guys here . a very high activation when you see more complex patterns like stripes and things like that . and as you go higher up through convolution networks and computer vision you can actually very nicely visualize what's going on and you identify like the fifth layer some neurons actually fire when they see a combination of eyes and a nose and a mouth . sadly for nlp we don't have any of that . it's one of the reason's they're not quite as popular in nlp . sure so you have here m you filter . so in the 1d case that'll just be f and g are just a single number and now you're going to move over f . so imagine this was just one dimension . and so you move from minus m to m as in for the nth time step you're going to multiply the filter g[m] here basically go one times step and you sum up this product between the two numbers at each time step . so you go from minus m which if this is minus and this is minus so you start head of n n time steps away and then you keep multiplying the numbers and then you have your convolution and we'll go over the exact examples for nlp in much more detail . how do we figure out the window size . we'll actually have a bunch of window sizes so maybe this is a good side way to talk about the actual so this is going to be the first and most simple variant of a convolutional network for nlp . you can [inaudible] go to town and towards the end they'll show you some examples of how we can embellish this architecture a lot more . this one is based on a really seminal paper by collobert and weston from 2011 . 
and then the very particular model in its various queuing details came from kim from just three years ago . a convolutional neural network for sentence classification . all right so as with every model out there whenever you wanna write down your equations no worries we're not gonna go into a lot more derivatives today . actually no derivatives cuz all the math is really similar to math we've done before . but it's really still important to identify very clearly your notation . as the question was correctly asked we'll actually start with word vectors . so we'll have at every time step and that will be here for us now a k dimensional vector . and then we'll represent the entire sentence through a concatenation . so we'll use this plus and circle symbol for concatenating and so we'll describe the entire sentence which we'll have for our definition here n-many words to be x from one to n . and that will be the concatenation of the first to the nth word vector . great question are word vectors concatenated length-wise . for now we'll assume they're all concatenated as a long row . this additional notation here so we don't just go from one to n but we might actually want to extract specific words in the range from time step i to time step i plus j or in general some other number of time steps . so if i have for instance x two to four then i'll take the second the third and the fourth word vector and i just have a long vector with just those three word vectors concatenated together . i'll let that sink in cuz it's all very simple but we just need to make sure we keep track of the notation . so in general our convolutional filter here will be a vector w of parameters that we're going to learn with our standard stochastic gradient descent-type optimization methods . and we'll define this convolutional filter here in terms of its window size and of course the word vector size . so h times k so this is just a vector it's not a matrix . there's no times in between the two . a convolutional filter that at each time step looks at three different word vectors and tries to combine some kind of feature representation . 
what we'll then do is basically have a three times number of dimensions of each word vector filter . so i have a very simple example here . let's say we have two dimensional word vectors of course just for illustration they'll usually be 50 dimensional or so . let's say we have two we look at three different words in concatenation at each time step we'll basically have a six dimensional w here . so now how do we actually compute anything and why is it a neural network . let's look at again we have our convolutional filter goes looks at h words at each time step . our word vectors are also concatenated into a single vector . and now in order to compute a feature at one time step for this what we're going to do is basically just have an inner product of this w vector of parameters times the i-th time step plus our window size . so in this case here we're going to in the c one for instance we'll have w times x one two one two three . so we have here three so one plus three minus one goes to three . so we basically just have the concatenation of those word vectors in our product . simple sort of multiplication and sum of all the element wise . then usually we'll have our standard bias term and we'll add a non-linearity at the end . so as you do this the question is don't the words in the middle appear more often . so here actually show this example and i have actually an animation so you are jumping a little bit ahead . at the very end here and the answer will just come we'll actually call this a narrow convolution and where you can actually have wide convolutions which we'll get to later but yes you're right the center but really the filters can adapt to that because you learn sort of how much you want to care about any particular input in the filter . okay so let's define this more carefully so we can think through the whole process so the question is rephrase it a little bit what happens when we have different length sentences . and there will actually be in two slides a very clever answer to that . which is at some point we'll add a pooling operator which will just look at the maximum value across everything . we'll get to that in a second . 
and it turns out the length of the sentence doesn't matter that much once we do some clever pooling . how's the size of the filtering affecting the learning . one the longer your filter is the more computation you have to do and the longer context you can capture . so for instance if you just had a one d filter it would just multiply and matrix with every word vector and it actually would you wouldn't gain much because it would just transform all the word vectors and you may as well store transformed word vectors . as you go to longer filters you'll actually be able to capture more phrases but now you'll also more likely to over-fit your model . so that will actually be the size of your filter will be hyperparameter and there are some tricks . namely you have multiple filters for multiple lengths which we'll get to in a second too that will allow you to get rid of that . alright so let's say again here we have our sentence now we have all these possible windows or length h starting at the first word vector going to this and so on . and now what the means is since we do this computation here at every time step we'll have basically what we call a feature map . and we will capitalize this here as having a vector of lots of these different c values . and again each c value was just taking that same w and having inter-products with a bunch of the different windows at each time stamp . now this c vector is going to be a pretty long n-h+1 dimensional vector . and it's actually going to be of different length depending on how many words we have . because in the end if we want to plug it into a softmise classifier we would want to have a fixed dimensional vector . but intuitively here we'll just again multiply each of these numbers and our w here with the concatenation and remove along . and if you now think carefully you'll actually realize well i kind of cheated because really that's what we really should've done also on the left side . so on the left side we will actually also zero pad the sentence . so we do exactly the same in the beginning at the end of the sentence . all right now because we have a variable length vector at this point and we want to have eventually a fixed dimensional feature vector that represents that whole sentence what we'll now do is introduce a new type of building block that we haven't really looked at that much before namely a pooling operator or pooling layer . and in particular what we'll use here is a so-called max-over-time or max pooling layer . 
and it's a very simple idea namely that we're going to capture the most important activation . so as you have different elements figured computed for every window you have the hope that the inner product would be particularly large for that filter if it sees a certain kind of phrase all right . so namely if you have let's say your word vectors are relatively normalized if you do an inner product you would want to have a very large cosine similarity between the filter and the certain pattern that you're looking for . and that one filter would only be good at picking up that pattern . so for instance you might hope all your positive words are in one part of the vector space and now you have a two dimensional sorry a two word vector sorry . a filter size of length two that looks at bigrams and you want to ideally have that filter be very good and have a very large inner product with all the words that are positive . and that would then be captured by having one of these numbers be very large . and so what this intuitively allows you to do is as you move over it and if you just have one word pair one biagram that it has a very large activation for that particular filter w you will basically get that to your c hat here . and it can ignore all the rest of the sentence . it's just going to be able to pick out one particular bigram very very accurately or a type of bigram . and because word vectors cluster and where similar kinds of words have similar kinds of meaning you might hope that all the positive words will activate a similar kind of filter . now the problem with this is of course that that is just a single number right . c hat is just a maximum number here of all the elements in this vector . if we use a relu nonlinearity here this will just be a single number . now of course we want to be able to do more than just find one particular type of bigram or trigram we want to have many more features that we can extract . and that's why we're going to have multiple filters w . a single feature w we'll convolve multiple of them . and as we train this model eventually we hope that some of the w filters will fire and be very active and have very large inter-products with particular types of bigrams or so it's also very useful to have some filters that only pick out bigrams and you can actually get quite far with that . but then maybe you have someone some examples where you say for sentiment again very simply example it's not very good or risk missing a much originality and now you want to have diagrams in filters of length k times 3 . and so we can have multiple different window sizes and at the end each time we convolve that filter and we do all these inner products at each time step . 
we'll basically max pool to get a single number for that filter for that sentence . if we have different filters of different lengths how do we make sure they learn different feature . of same length or different lengths yeah . of same length how do we make sure they learn different features . well they all start at different random initializations so that helps to break up some symmetry . and then actually we don't have to do anything in particular to make sure that happens it actually just happens . so as we do sgd from the random initializations different filters will move and start to pick up different patterns in order to maximize our overall objective function . which we'll get to it'll just be logistic regression . they would probably still you update so in the beginning well if they're exactly the same basically as you pool right you will eventually pick during backpropagation the max value here . the max value will come eventually from a specific filter . and if they have the exact same one you would never do it . but two if you did they would have the exact same value . and then your computer will have to choose randomly one to be the max . whatever it'll pick one and then it'll backpropagate through that particular filter . and then they're also going to be different in the iteration of your optimization algorithm . so in theory nothing would prevent us from using min too . though we in many cases use rectified linear units which will be max 0x . and so max pooling makes a lot more sense cuz min will often just be 0 . and so we've rallies together it makes the most sense to use the max pooling layer also . it's actually not totally crazy there are different papers that explore different pooling schemes and there's no sort of beautiful mathematical reason of why one should work better but intuitively what you're trying to do here is you try to really just fire when you see a specific type of engram . 
and when you see that particular type of engram cuz that filter fired very strongly for it then you wanna say this happened . and you want to give that signal to the next higher layer . and so that is particularly easy if you choose a specific single value versus averaging where you kind of conglomerate everything again . and the strong signal that you may get from one particular unigram or bigram or trigram might get washed out in the average . great question so once we have a bunch of different c hats from each of the filters how do we combine them . and the answer will be we'll just concatenate all them . we'll get to that in a second . yeah so the main idea is once you do max pooling one of the values will be the maximum and then all of the other ones will basically have 0 gradients cuz they don't change the layer above and then you just flow your gradients through the maximum value that triggered that filter . so the question is doesn't that make our initialization very important and lead to lots of downstream problems . and the answer is yes so likewise if you for instance initialize all your filter weights such as your rectified linear units all return zero then you're not gonna learn anything . so you have to initialize your weights such that in the beginning most of your units are active and something will actually happen . and then the main trick to or the way the reason why it doesn?셳 hurt randomizations you have lots filters . and each filter can start to pick up different kinds of signals during the optimization . but in general yes these models are highly non-convex and if you initialize them incorrectly they won?셳 learn anything . but we have relatively stable initialization schemes at this point that just work in most cases . great questions all right i like it . all right so we basically now have we're almost at the final model . but there's another idea here and that combines what we've learned about word vectors but extends it a little bit . and namely instead of representing the sentence only as a single concatenation of all the word vectors we'll actually start with two copies of that . and then we're going to backpropagate into not into the other . 
remember we had this lecture where i talked about the television and the telly and as you back-propagate into word vectors they start to move away from their glove or word2vec initialization . so again just quick recap word vectors are really great . we can train them on a very large unsupervised scope so they capture semantic similarities . now if you start backpropagating your specific task into the word vectors they will start to move around . when you see that word vector in your supervised classification problem in that dataset . push certain vectors that you see in your training data sets somewhere else the vectors that you don't see in your training data set stay where they are and now might get misclassified if they only appear in the test set . so by having these two channels we'll basically try to have some of the goodness of really trainings the first copy of the word vectors to be really good on that task . but the second set of word vectors to stay where they are have the good nice general semantic similarities in vector space goodness that we have from unlarge and supervised word vectors . and in this case here both of these channels are actually going to be added to each of the cis before we max-pool so we will pool over both of those channels . now the final model and this is the simplest one i'll get to you in a second . is basic just concatenating all this c hats so remember each c hats was one max pool filter . and we have this case here that say m many filters . and so our final feature vector for that sentence has just an r n-dimensional vector where we have m many different filters that we convolved over the sentence . z directly into softmax and train this with our standard logistic regression cross entropy error . by having two copies of the work vectors are we essentially doubling the size . well we're certainly doubling the memory requirements of that model . and we just kinda assume you could think of it as doubling the size of the word vectors and then the important part is that only the second half of the word vectors you're going to back propagate into for that task . that's right we can use the same convolutional weights or you can also use different convolutional you can have multiple and this model will have many of them actually . it could have 100 bigram filters 100 trigram filters maybe 24 filters and maybe even some unigram filters . so you can have a lot of different hyper parameters on these kinds of models . 
for a given sentence does the convolutional matrix stay the same . so this matrix is the only matrix that we have . this is just our standard soft matrix and then before we had these w filters these vectors . and yes each w is the same as you convolve it over all the windows of one sentence . a bunch of concatenated word vectors and then you max pool find the largest value from all the n-grams . and that's a cnn layer and a pooling layer . here instead of concatenating them this just kind of simplified this so imagine here you have n same notation . we have n many words in that sentence and each word has k as a k dimensional feature vector or word vector associated with it . so these could be our glove or other word to vector initializations and now this particular model here shows us two applications of a bigram filter and one of a trigram filter . so here this bigram filter looks at the concatenation of these two vectors and then max pool them into a single number . and as you go through this you'll basically get lots of different applications . and you basically for each of the features you'll get one long set of features and then you'll get a single number after max pooling over all these activations from so you see here for instance so the bigram filter is this channel and then we'll basically max pool . over that again notice how here they use indeed the same filter on the second word vector channel the one we might back propagate into . but they will all basically end up in here . so just again inner products plus bias and non linearity and then we'll max pool all those numbers into a single number up there . and now a different namely this guy up there . the trigram also convolves over that sentence and basically combines a bunch of different numbers here and then gets max pooled over a single number there . so do we always max pool over particularly just a set of features that are all coming from the same filter . and the answer is in this model we do and it's the simplest model that actually works surprisingly well . but there are going to be right after our quick research highlight a lot of modifications and tweaks that we'll do . 
there are no more questions let's do the research highlight and that model should be on . today i thought i would share with you a very interesting paper called character-aware neural language models . so on a high level as the title implies the main goal of this paper is to come up with a powerful and robust language model that effectively so to frame this in a broader context most prior neural language models do not really include the notion that words that are structurally very similar should have very similar representations in our model . additionally many prior neural language models suffered from a rare-word problem . where the issue is that if we don't really see a word that often or at all in our dataset then it becomes very hard to come up with an accurate representation for that word . and this can be very problematic in languages that have long tail frequency distributions or in domains where vocabulary is constantly changing . so to address some of these problems the authors propose the following model where essentially we will read in our inputs at the character level but then we will make our predictions still at the word level . so let's dive a little bit deeper into the model and see exactly what's happening here . so the first thing we do is that we take our input and we break it apart into a set of characters . where for each character we associate it with an embedding that we learned during training . we then take the convolutional network and take its filters and convolve them over them the embeddings to produce a feature map . and finally we apply max pooling over time which intuitively is selecting out the dominant n-grams or substrings that were detected by the filters . we then take the output of the convolutional network and pipe it into a highway network . which we're going to use to essentially model the interactions between various n-grams . and you can think of this layer as being very similar to an lstm memory cell where the idea is that we want to transform part of our input but also keep around and memorize some of the original information . we then take the output of the highway network and pipe it into a single timeset of lstm which is being trained to produce sequence given the current inputs . and the only thing different to note here is that we're using hierarchical softmax to make predictions due to the very large output vocabulary . so let's analyze some of the results . so as we can see here from the table on the right the model is able to obtain comparable performance with state of the art methods on the data set while utilizing fewer parameters in the process . what's also really remarkable is i was able to outperform its word level and working level counterparts across a variety of other rich languages such as arabic russian chinese and french . 
while using again fewer parameters in the process because now we don't have to have an embedding for every single word in our vocabulary but now only for every single character that we use . we can also look at some of the qualitative results to see what is it the results is exactly learning . so in this table we have done is that we have extracted the intermediate representations of words at various levels of the network and then computed their nearest neighbors . and what we find is that after applying the cnn we are grouping together words with strong sub-word similarity and that after applying the highway network we are also now grouping together words that have strong semantic similarities . so now the word richard is close to no other first names . we can also look and see how it handles noisy words . so in this case the model is able to effectively handle the word look which it has never seen before . but it is now able to assign it to reasonable nearest neighbors . and on the plot on the right what we see is that if we take the in-grammar presentations learned by the model and plot them with pca . we see that it is able to isolate the ideas of suffixes prefixes and hyphenated words . which shows that at its core the model really is learning something intuitive . so in conclusion i wanna sort of highlight a few key takeaway points . the first is that this paper shows that it is possible to use inputs other than word embeddings to obtain superlative performance on language modeling . while using fewer parameters in the process . second it shows that it demonstrates the effectiveness of cnns in the domain to language modeling . and shows that in this case the cnns and highway network are able to extract which types of semantic and the character level inputs . and finally what's most important is that this paper is combining the ideas of language modelings cnns ltms hierarchical softmax embeddings all into one model . which shows that basically we can treat the concepts that we've learned over the course of the quarter as building blocks . and learn to compose them in very interesting ways to produce more powerful or more nuanced models . and that is a very useful insight to have as you approach some of your own projects not only in the class but also beyond . 
and with that i would like to conclude and thank you for your attention . usually when you have a larger downstream task like question answering or machine translation they can give you 2 to 5% boost in accuracy . sadly when you run any kind of model over characters you think you have a sentence or document with 500 words . of 500 times maybe 5 or 10 characters so now you have a 5000 dimensional time sequence . and so when you train your model with character levels think extra hard about how long it will take you to run your experiments . sort of accuracy versus time tradeoff in many cases . all right so i mentioned the super simple model where we really just do a couple of inner products over a bunch of these filters find the max and then pipe all of that into the softmax . now that by itself doesn't work to get you into state-of-the-art performance so there are a bunch of tricks that and i'm going to go through a couple of them since they apply to a lot of different kinds of models that you might wanna try as well . the first one is one that i think we've already covered dropout but we did right . but it's a really neat trick and you can apply it lots of different contexts . and its actually differently applied for convolutional networks and recurrent neural networks . so it's good to look at another application here for this particular convolution neural network . so just to recap the idea was to essentially randomly mask or dropout or set to 0 some of the feature weights that you have in your final feature vector . and in our case that was z remember z was just a concatenation of the max built filters . and another way of saying that is that we're going to create a mask vector r . of basically random bernoulli distributed variables with probability that i would probability p set to 1 . and probably 1 minus p set to 0 . and so what this ends up doing is to essentially delete certain features at training time . so as you go through all your filters and you actually had a great biagram . and another good biagram it might accidentally or randomly delete one of the two biagrams . 
and what that essentially helps us to do is to have the final classifier not overfit to say it's only positive for instance if i see these exact two biagrams together . so another way of saying that is that it will prevent co-adaptation of these different kinds of features . and it's a very very useful thing . basically every state-of-the-art model out there that you'll observe hopefully somewhere in its experimental section it will tell you how much it dropped out of weights and what exactly the scheme of dropout was . cuz you can dropout for instance through recurrent neural network you can dropout the same set of features at every time step or different sets of features at every time step . and it all makes a big difference actually . so this is a great paper by geoff hinton and a bunch of collaborators from 2012 . now if you carefully think through what happens here well at training time we're basically let's say it's 0.5 probability p is 0.5 . so half of all the features are randomly getting deleted at training time . well then the model is going to get used to seeing a much smaller in norm feature vector z or had a more product here or time z . and so basically at test time when there's no dropout of course at test time we don't want to delete any features . we want to use all the information that we have from the sentence our feature vector z are going to be too large . and so what we'll do is in this care here we'll actually scale the final vector by the bernoulli probability p . so our ws here the softmax weights are just going to be multiplied and essentially halved . and that way we'll end up in the same order of magnitude as we did at training time . so some people liken dropout to assembling models . and intuitively here you could have let's say deterministically you were dropping out the first half of all your filters . and you only train one model on the first half of the filters and the second half of the filters . and then in the end you average the two . that's kind of similar but in a very noisy variant of what you end up doing with dropout . 
and so in many cases this can give you like 2%-4% improved accuracy . and when we look a the numbers you'll notice that it's those 2%-4% that gets you that paper published and people looking at your method . whereas if it's 4% below it's getting closer and closer to a very simple back or forwards model with discrete counts . is it possible to dropout so you could actually dropout some of the weight features as well . and yes there is actually another variant of dropout . there's the filter weight dropout and there is the activation dropout . so in this case here we have activation dropout . and they have different advantages and disadvantages . i think it's fair to say that especially for nlp the jury is still out on which one should you always use . i think the default you just filter out and the original dropout is just to set to 0 randomly the activations and not the filter reads . so basically this will have a certain norm . and at training time the norm of this is essentially say halved if you have a probability of p to multiply the features with zero . and so what that means is that overall this matrix vector product will have a certain size and a certain certainty also once you apply the softmax . and if you don't wanna basically be overly confident in anything you wanna scale your w because at test time you will not drop out anything . you will have the full z vector not half of all the values of the z vector . and so at test time you wanna use as much information you can get from z . and because of that you now have a larger norm for z . and hence you're going to scale back w so that the multiplication of the two ends up in roughly the same place . very good question so what's the softmax here . so basically z was our vector for some kind of sentence . 
and i use the example sentiment because that is one of the many tasks that you could do with this . so generally sentence classification or document classification are the sort of most common task that you would use this model for . we'll go over a bunch of examples in three slides or so and a bunch of data sets . awesome so now there's one last regularization trick that this paper by kim used in 2014 . it's actually not one that i've seen anywhere else . and so i don't think we'll have to spend too much time on it but they essentially also constrain the l2 norm of the wave vectors of each of the classes . so we have here remember this is our softmax weight matrix w and c dot was the row for the cth class . and they basically have this additional scheme here where whenever the norm of one of the rows for one of these classes is above a certain threshold s . which is another hyperparameter they'll select it will rescale it to be exactly s . so basically they'll force the model to never be too certain and have very large weights for any particular class . now it's a little weird cuz in general we have l2 regularization on all the parameters anyway . it's actually the only paper that i can remember in recent years that does that so i wouldn't overfit too much on trying that . now it's important to set back and think carefully . i described this model and i described it very carefully but when you think about it you now have a lot of different kinds of tweaks and hyperparameters . and you have to be very conscious in all your projects and every application in industry and research and everywhere of what your hyperparameters really are . and which ones actually matter to your final performance how much they matter . and in an ideal world you'll actually run an ablation where maybe you have these two word vectors . the ones you back propagate into and then the ones you don't back propagate into . sadly in very few examples people actually properly ablate and properly show you all the experiments they ran . and so let's go over the options and the final hyperparameters that kim chose for neural network model . 
the one amazing thing is they actually had the same set of hyperparameters for a lot of the different experiments . a lot of these are sentiment analysis subjectivity classification as most of the experiments here . but they had the same set of hyperparameters which is not very common . sometimes you also say all right here are all my options . and now for every one of my datasets i will run cross-validation over all the potential hyperparameters . is exponential so it would be too many . so then the right thing to often do is actually to set boundaries for all your hyperparameters . and then randomly just sample in between those boundaries . so for instance let's say you might have 100 potential feature maps for each filter for each window size . now you say all right maybe i'll have between 20 and 200 . and you just say for each of my cross-validation experiments i will randomly sub sample a number between 20 and 200 . then i'll run experiments with this number of filters on my developments split and i'll see how well i do . and you say i have maybe 100 experiments of this kind . you'll quickly notice that again why you need to start your project early . cuz your performance will also depend highly on your hyperparameters . and if you don't have time to cross-validate you may lose out on some of the accuracy . and especially as you get closer to potentially state-of-the-art results which i think some of the groups will . that last couple percent that you can tweak and squeeze out of your model with proper hyperparameter search can make the difference between having a paper submission or having a lot of people be very excited about your model . do you do that sampling for one hyperparameter at a time or not . in the end in the limit it doesn't matter which scheme you use . 
but in practice you set the ranges for all your hyperparameters and then you sample all of them for each of your runs . and it's very very counterintuitive that that would work better than even a grid search . where you say all right instead of having 100 feature maps and randomly sample between 20 and 200 . i'm gonna say i'm gonna try it for 20 50 75 100 150 or 200 or something like that . and then i just multiply all these six or options with all the other options that i have . it quickly blows up to a very very large number . let's say each of these you try five different options that's five to the number of many hyperparameters that you have . which if you have ten parameters or so that's 5 to the 10 that's impossible to run a proper grid search on all these hyperparameters . it turns out computationally and through a variety of different experiments i think the papers by yoshua bengio and some of his students a couple years ago . that random hyperparameter search works surprisingly well and sometimes even better than a relatively fine grid search . until you run out of money on your gpu . or until the paper deadline comes around or the class project deadline comes around in the perfect setting you have the final model that you think has all the ingredients that you'd want . and then you can let it run until you run out of resources . either or time or money or gpu time or you annoy all your co-phd students such as i did a couple years ago . some point to have what is it preemptable jobs so that i could run use the entire cluster . but then when somebody else wants to use the machine it'll just put my job into the cache onto memory or even save it to disk and anybody else can kinda sort of come in . but yeah ideally you?셪l run with all the computational resources you have . and of course this is depending on how much you care about that last bit of accuracy . for some of the papers it can really matter for some applications if your work in medicine . you try to classify breast cancer or something really serious of course you want to squeeze out as much performance as you possibly can . 
so there are actually some people who tried various interesting bayesian models of gaussian processes to try to into hyperparameter space . so you basically run a meta optimization . where instead of optimizing over the actual parameters w of your model you've run an optimization over the hyperparameters of those models . can do that it turns out the jury is sort of out but a lot of people now say just do a random hyperparameter search . it's very surprising but that is i think what the current type of hypothesis is for being the best way to do it . so the question is how do we make sure the same set of hyper parameters they never do and some people this gets we could talk a lot about this this is kind of fun . this is like the secret sauce in some ways of the learning but some people also say i'm going to run the same model with the same hyper parameters five times and then i'm going to average and ensemble those five models because they're all end up in a different local optimum and that assembling can also often help . so at the end of every project that you're in if you have 100 models that you've trained you could always take the top 5 models that you've had over the course of your project and ensemble that and you'd probably squeeze out another 1 or 2% . but again probably don't have to go that far for your class projects . it's only if it really matters and medical applications or and in many cases what you'll observe people write this is my best single model and this is my best ensemble model . and then in the best ensemble model you can claim state of the art and the best single model might be sometimes also the best single model but sometimes you also have a more diverse setup model so all right last question about assembling and crazy model header . and shouldn't we just have a better single model . there are various ml researchers who say i don't like ensembling at all . we should just work harder on better single models . and dropout is actually one such idea that you can do there other optimization ideas that try to incorporate that yeah you're right . in some ways what that means is we still don't have the perfect optimization algorithms that properly explore the energy landscape of our various models . all right so let's go over the extra hyperparameters that they used here . so basically we want to find all these hyperparameters on the dev set . super important you get minus 10% if i see you run on the final test set all your hyperparameter optimization because that means you're now overfitting to your final test set . one of the number one 101 machine learning rules . 
never run all your hyperparameter across validation on your final test set . that's the one thing you want to run maybe once or twice it can ruin your entire career if you do that and you publish it it's never worth it . all right so on the development set on your development test set your or sometimes called dev set we or kim here tried various different nonlinearities and in the end chose the rectify linear unit . so that's actually very common have to run and try that . you just use rally as the default . he had try grams four grams and five grams for the various filter sizes . somewhat surprising note by grams that surprised me . he had 100 different feature maps for each of these sizes . so 100 tri-gram filters 100 4-gram filters and 100 5-gram filters . so the more you have the more likely each of them can capture different kinds of things . so if you have for instance just a simple sentiment classifier you can have 100 shots at trying to capture various types of negation for instance . then drop out just simple in the middle half of the time all the features are set to zero . he chose for this funky regularization trick s equals three . batch size will often also change your performance significantly . so you don't usually want to have most nlp models . so here you had a mini batch size of 50 . during mini batch sgd training and to use the word vectors pre-trained on a large corpus and you had 300 dimensional word vectors . so that was a lot of hyper parameter search you can think that was going on in the background here yeah . wouldn't a higher mini batch size guarantee that we have less noise while training . the answer is yes but you actually want the noise . 
you have a very nonconvex objective function here . and what dropout does in sgd is to actually introduce noise so that you're more likely to explore the energy landscape instead of just being very certain about being stuck in one of the many local optimi that you have . so a lot of optimization tricks and training tricks of neural networks in the last couple years can be described as adding noise into the optimization process . super important during training how do you select your best model . so one option is you just let it run and at the very end you take that last output . so these are your iterations and this might be your accuracy or your f1 score your wu score your blue score whatever you're using for your model . and now you'll often observe something like this as you train over time . and now if you take just the very last one maybe here . maybe that wasn't as good as this random spot that as it did stochastic gradient descent it found just a randomly really good spot on your dev set and so what kim does and what actually a lot of people do is during training you keep checking the performance here . again this is your development accuracy and then you pick the one with the highest accuracy and you set those weights to be the weights for that particular experiment . and another side trick just because they're fun you can also sample multiple times and then ensemble those weights or average those weights . and that sometimes also works better all right . all right so that was a lot of good details for how you tune junior models . so here now some of the results so here we basically have only sadly in this whole paper four implementations and four of the options that are really carefully outlined and all the other hyperparameters we don't know how important they were or the variance of them . so you can average the weights which is very counter-intuitive but also often works . if you average the predictions now you have to keep around and say you have an ensemble of the four top models you have to keep around your model size times four which is very slow and not that great so it's less commonly done especially not in practice . so an appellation study is essentially a study where you start with and then you say how much did each of the components actually matter to my final accuracy that i describe in my abstract . so let's say we have a cool deep learning model with five-layer lstms and some attention mechanisms reversing the input in machine translation for instance and so on . and now you say all right overall this model got me a roster of 30 or 25 . now [blank audio] as a practitioner even as a researcher i want to know well you mentioned five tricks which of the five were actually the ones that got you to 25 . 
you want to know because you might not want to use that entire model but you want to use that one trick . so in this case here for instance he has this regularization trick . how much did the dropout actually help versus this trick . which one should i now use in my downstream task . in this paper here and i don't want to single him out this is really sadly very common in the field . nobody give you a nice plot for every single hyper parameter that says for this hyper parameter let's say dropout p between zero and one this was my accuracy . yeah so the appellation would say i will take out one particular modeling decision . so let's say in his case for instance . the application here is do we need to multichannel two word vectors . one you back probe into one you don't or do we only have a static channel as in we keep he called static here just keeping the word vectors fixed in the beginning versus having only a single channel where we back propagate into the word vectors versus having both of the word vector sets . and this is the ablation he does here . the ablation over should we have two sets of work vectors or not . and as you can see here well it actually sometimes it buys you 0.7 and here 0.9 or so but sometimes it also hurts . so here having the two channels actually hurt by 0.4 . these are relatively small data sets all of them so the variance is actually relatively high . and the first the simplest one is you actually just have random word vectors and you back-propagate into them and you just learn the word so no pre-training of word vectors whatsoever . that's actually fine if you have a gigantic training dataset . so if you do machine translation on five gigabytes of i hope we discouraged everybody from trying that . but if you do machine translation on a very very large corpus it turns out you can just have random word vectors . and you have so much data on that task that as you back propagate into them and update them with sgd they will become very good as well . 
so arun correctly points out and mention this like this is actually very small datasets and so there's very little statistical significance between most of these results here . i forgot all the various thresholds of statistics assuming against for the various papers . i think this one might be significant . but certainly mpqa for instance is a very small data set . statistically significant great question so the question is as you do this let's say you had your full data set . you say this is my training data . this is my development split and this is my final test split and we had properly randomized them in some way . now if i choose based on my development split and this development accuracy here then this model is only trained on this . and only in sort of some meta kind of way used that dev split . now what you could also do and what you should always do if you have and actual convex problem which sadly we don't have in this class very much what you would do is you find your best hyper parameter setting and then you'll actually retrain with that whole thing all the way until convergence . which if you have a convex problem it's great . you know you have a global optimum and you probably have a better global optimal because you used you entire trained data set . now it turns out in many cases because there can be a lot of variance in your training for these very non-convex neural network models . it helps a little bit to just ignore that final that deaf part of your data set and just use it to only choose the highest point . but yeah it's largely because it's a non-convex problem great question . all right we have four more minutes . so one of the problems with this comparison here was actually that the dropout for instance gave it two to four percent accuracy improvement . and overall and you'll see this in a lot of deep learning papers they make claims about this is the better model . sadly when you look at it there are some models here that they're comparing to that came out after or before dropout was invented . so we can be very certain that some models from pre-2014 didn't use any of the kinds of tricks like dropout and hence the comparisons actually kind of flop . 
and sadly you'll observe this in most papers . almost very very few people in the community will go re-run with the newest and fanciest optimization tricks like dropout or add in better optimizers and so on and reimplement all the baseline models of previous authors and then have a proper comparison run the same amount of cross validation on the second best model and other people's models and then have a really proper scientific study to say this is the actual better model versus this model came out later had the benefit of a lot of optimization tricks . so you'll see that a lot and it's in some ways understandable because it takes a long time to reproduce ten other people's results and then start tuning them . these with a grain of salt because the optimization as we see here makes a big difference . so two to four percent when you look at even some of my old papers four precent is the difference between whether this model is the better one or not . still it is kind of a very cool architecture this convolutional network . the fact that it can do so well overall . it's relatively simple and the nice thing is with these filters each of the filters is essentially independent right . we run max pooling over each so each filter can be run on one core of your gpu . and so despite having 300 different filters you can run all of those 300 in parallel maximum peril . and then you have very quickly it can compute that one feature back there and pipe it into the softmax . so that is actually a huge advantage of these kinds of models . now we don't have that much time left so i'm not going to go into too many details but you can really go to town and put together lots of convolutions on top of pooling layers in a variety of different ways . we spend a lot of time trying to gain intuitions of why this lstm node gate has this effect . i don't think we have a good chance of going here in the third cnn layer and having some intuition of why it's this kind of layer versus another . they're really they really get quite unwieldy . you can have various kinds of convolution those are in some sense hyper parameters . you can ignore basically zero pad the outside or you can just not run anything that would require an outside multiplication and only run convolutions when you have for the insides . basically this is the narrow you can eventually run the convolution also not over the times steps but in later layers over the feature maps . so they're a lot of different options . 
and at some point there's no more intuition of why should you do this in the third layer of these texts cnn . one of the most exciting applications was actually to take such a cnn have various pooling operations and in the end take that as input to a recurrent neural network for machine translation . so this was one of the first deep learning machine translation models from 2013 that actually combined these fast parallelizable cnns with a recurrent neural network to do the machine translation that we've seen . so we've essentially described just model entirely in a lecture before but now we're replacing the encoder part instead of having an lstm here we have a cnn here and we give that as an input to all the time steps at the decoder part of the model . now probably i'll end on this slide and we'll maybe talk about this quasi recurring neural networks that combines the best of both recurrent and convolutional models . for another lecture but basically you now know some of the most important and most widely used models for deep learning for nlp . we have the bag of vectors surprisingly works quite well when you combine it with a couple of relu layers . and can actually even in some benchmarks beat this convolutional network that we just described . so very good base line to run for a variety of different projects that we're discussing . we've discussed the window model already where we have basically a very clean model to classify words in their context . now we know the convolutional neural networks and we had a lot of variants of recurrent neural networks . so hopefully you have most of the tools on thursday . chris will talk about recursive neural networks or tree structured recursive neural networks that will be much more grammatically and also have some downsides . so back with lecture 14 so in today's lecture what i'm gonna do is introduce really the last sort of major architecture for neural networks that we're gonna teach you in this class . beyond what we've seen with both recurrent neural networks and convolutional neural networks . and today what i'm gonna start to look at is having tree structured recursive neural networks . so this is a topic that is very dear to both me and richard . it's sort of dear to me as a linguist cuz i sort of believe that languages have this basic tree structure as i'll explain in the coming minutes . and it's dear to richard because it's what his thesis was about . we'll talk [laugh] about some of that work later on . 
so there's some new stuff here there's some stuff that in some sense is the same . so we kind of adopted the name because of neural networks to refer to tree structured neural networks . but if you think about it recursive and recurrent are kind of sort of come they're exactly the same through the first five letters . and so in some sense it's kind of a form of recurrent network that is now done over a tree topology as i'll explain . and so more recently people have commonly referred to them as tree rnns sort of emphasizing that more what's different is the geometry of what you're dealing with . and in the course of that i'll talk also a bit about constituency parsing . so i'll start off with some of the motivations and looking at how these kind of models can be used for parsing . we'll have the research highlight and then i'll go on with some other stuff with some more applications and looking at some of the sort of better architectures that you can then build for tree recursive neural networks . before getting under way just quickly reminders so there's some staffing complications coming up so . for richard he's gonna be away on tuesday . so his office hours are gonna be after class on thursday . so come along then to talk to richard . conversely for me i also have a couple of irregularities in my schedule . so for tomorrow but the sort of the morning scpd slot there's a linguistic's faculty meeting . so i'm gonna have to move that to earlier to nine to ten . the afternoon will be the same as usual for our projects officer hours . but then next week i'm going to be away at the end of the week thursday and friday . so i'm gonna have to sort of reschedule that . and i'm gonna move the afternoon slot until the following monday . so look on the calendar and get straight when those times are . 
but we do still really want people the various other final project tas about their projects . make sure you're getting something working using a gpu for our milestone . we're seeing a lot more people any problems stop by one of the ta's office hours or coding sessions and get both or messages and get that sorted out . final project discussions please come and talk to us about your projects . and we've sort of regarded as just really really important for the final project . that you actually have something that you can run by the end of this week . we hope everyone doing their final projects is in the position where they have some data ready to go and they can run some kind of baseline this week . cuz if you aren't in that position it's pretty marginal . as whether that things could possibly come to a good conclusion given the time available . any questions about that talk to your project mentor . okay let me go on so i just thought i'd start with some general remarks about sort of what kind of structures that we put on language when in some of these nlp and deep learning applications . this is just a fun picture for the sake of it . but over at cmu they actually have this lovely artwork which is the bag of words . and you have the bag with words inside it . and down on to the floor here have full and the function words they're outside the bag of words . so one model of language that ends up being used quite a bit in deep learning and sometimes very effective as we've discussed is you have nothing more than a bag of words . the sad truth is that our state of the art isn't really beyond a bag of words . so that people have used clever bag of words models . things like deep averaging networks and get very good results . so there were papers at last year's ikwea . 
so iclr is this new conference where a lot of deep learning work appears--international conference on learning representations . and sothere was a paper last year from people at the toyota technical institute where essentially their result was that for doing paraphrase detection things have roughly the same meaning that they could get better results with a sort of deep averaging network of a sort than people had been able to get with any kind of more complex structured neural model . i think that's a sad result not a good result but that's the current state of the art . on the other hand in linguistics and people attempt to use very elaborated structures of language which capture a huge amount of detail . you can be at either of those extremes or you can hope to be somewhere in the middle . and if you wanna be somewhere in the middle and have something that roughly captures one of the most fundamental properties of language it seems like you don't wanna have word vectors and you want to have something that can minimally capture the main ideas of the semantic interpretation of language . and the idea i want to get at there is it seems the fundamental notion for human languages is that we have this idea of composition where we can put together the meaning of phrases . and so we'd like to know about larger units that are similar in meaning . so if we have two pieces of text perhaps paraphrase for example the snowboarder is leaping over a mogul or a person on a snowboard jumps into the air . it sort of seems the first essential point that a human being would notice and think is part of the solution here means roughly the same as snowboarder that it's a paraphrase . and we can do that because we can take the sequence of words person on a snowboard and say that's a phrase and we can put together a meaning for that phrase even if we haven't heard it before and know that it's got this meaning that's roughly the same as snowboarder . and so essentially that's what human beings do every day when they're listening to their friends right . every day people are saying to you different sentences . they might first off just say how's it going . and you've heard that 10000 times before . further into describing their day they're gonna be saying sentences that you've never heard before and yet we can work out what they say . and we're not working out what they say just as a bag of words of saying mm this is about a party . we're actually getting the details of what they said and who said what to who . and we're doing that by starting off by knowing the meanings of words and then doing a semantic composition where we can put smaller units into bigger units and work out their meaning . and so we'd like to have models that can do the same . 
so at the very top level this is compositionality is to how you can put pieces together into bigger parts and then understand what those bigger parts do . i think it's a problem that's especially prominent when you start thinking about human languages . but i'd like to suggest this is not only a problem that has to do with human languages . in many ways it's sort of a more like a complex picture well that's also something that has a compositional structure . so that you have pieces of stuff that you have parts of a church that go together . you have people that go together into a crowd or in front of this church . so in general it seems that for language understanding in particular but also for other kinds of artificial intelligence that we need to have models that have this kind of capability for semantic compositionality . you can put together smaller work out the meaning of those larger pieces . and what i'd like to suggest is our tree recursive neural networks . one model that you could think about as a good model for understand compositionally . so there's this general idea of sort of taking parts and putting them together into bigger parts and understanding their meaning . there's a notion that's related to that which goes slightly beyond that which is the notion of recursion . so by far the most famous linguist is noam chomsky . and so in some of noam chomsky's recent work with colleagues they've tried to advance this picture at in terms of . i mean it's for 50 years it's been chomsky's position that humans have this special innate born with part of your brain structure ability for human language that sort of sets us apart from other beings . and it's sort of actually seeing that as having sort of specific not everyone else believes that . but they've been trying to sort of put forward these proposals as to what is special in the sort of capabilities of the humans for language . and chomsky and colleagues have wanted to claim that really the defining property is that humans have this ability which you see through language to have recursion . so that's just like in your cs class right when you have things going back and back to the same thing looping over than you have recursive structure . and human languages have this kind of recursive structure it appears . 
the company that you spoke with about the project yesterday that what we have here i think might have no that's great . what we have here is sort of recursive levels of the same kind of structure . so that project is a little noun phrase the company that you spoke about the project yesterday . that's a bigger noun phrase that contains the smaller part noun phrase the project . and then the man from the company that you spoke with about the project is very that's an even bigger noun phrase that contains my other two nouns phrases . so human languages have this nesting of structure . of where you sort of have the same units like noun phrases and clauses that will nest inside each other . and you can nest them deeper and deeper and that gives us the idea that recursion is a natural thing for describing human languages . and that's essentially the sort of basis of chomsky's claim . i mean cognitively that's actually a little bit debatable and there are active debates on psycholinguistics and this kind of issue . i mean it's sort of complex because really as soon as you're thinking sentences are only gonna be some finite length . you can never sort of prove that things are fully recursive because there's sort of a maximum depth as to which things are gonna be embedded . and actually there's slightly more to it to it than that . it's a well known psycho-linguistic observation that actually having things embedding in the middle of the sentence . so i sort of deliberately the project not right at the very right edge but had something come after it . more central embedding that tends to be disfavored in human languages . so although you get a lot of embedding most of the embedding that you get tends to have more of a chaining structure where you have a right branching kind of structure . and to the extent the structure is purely chaining then you don't actually need to have full recursion to describe it because you can think of it more as a kind of an iterative sequence . anyway some cognitive arguments there cognitive science arguments . but nevertheless if you're sort of wanting to give a neat sort of natural description of natural languages . 
it's sort of basically you end up describing them recursively . cuz what you want to say is well there are noun phrases which can expand to various things like a short noun phrase like the man followed by a prepositional phrase which has a preposition from followed by a noun phrase . and inside this noun phrase it's got a relative clause . and inside that it's got other noun phrases like you and the project . so you kind of get these levels inside each other heading down recursively . and you can and indeed people do in things like news live sentences embed them even more deeply than my example . and so thinking about these kind of tree structures which have embedding inside them where we can have noun phrases with noun phrases inside them . about language structure and to think about things like how we disambiguate sentences . we sort of talked before about having ambiguities of attachment and we talked about dependencies before . the other way to these this and think about them is to have these kind of constituency or free structure representations . which in computer science terms correspond to context-free grammar representations . and then we have noun-phrased units and is modifying the verb eats . it's a child of the verb phrase constituent or the with meat here is a similar prepositional phrase with the noun phrase inside it . but it's modifying the noun phrase spaghetti to build a bigger noun phrase . and so as soon as we started using these kind of structures in a context-free grammar kind of structure we have the ability for recursion and human languages seem to indicate that . and we kind of want to refer to these units when we do other tasks . so next tuesday i'm gonna talk about co-reference resolution which is how you refer back in the text or to the environment refer back to entities that have already been established . and that sort of can be thought of as sort of picking out pieces of structure in terms of this kind of constituency compositionality . so john and jane went to a big festival . they enjoyed the trip and the music there . 
so they refers back to something and it seems to refer back to this noun phrase john and jane enjoyed the trip . so the trip's a noun phrase which seem to refer back to going to a big festival . and the music there the there is again referring to this big festival . okay so and finally having these kind of grammatical analyses of sentences is clearly better for some tasks . it's capturing a very powerful prior of what human language structure is like and is then useful for understanding and so in the start of the course we just had word vectors and so we had word vectors for things like germany france monday and tuesday . and we sort of were able to capture word semantic similarity in terms of our word vectors . what we'd like to be able to do is say we have larger constituents . so we have noun phrases like the country of my birth or the place where i was born . and we'd also like to understand the semantic similarity between those phrases . and the idea that we're going to develop to answer how can we do that is that what we're gonna do is say well what we'd like to be able to do is take bigger units of linguistic structure and also work out how to calculate their meaning as vectors and place them in exactly the same vector space . so we're sort of hoping we can take bigger phrases and say let's just stick those into also represent their semantic similarity as a kind of vector similarity . where of course this example's only two dimensional and in practice we'll be using 100 200 1000 dimensions . so the question is why would we want to put them in the same space as the word vectors . i mean that's not a necessary thing . that's just not what i wanna do . i'm gonna have word vectors in one place and phrase vectors in another place . the reason why a lot of the time that seems a good idea is that i mean individual words can capture a lot of meaning . and in particular they can sort of bundle up a bunch of semantics that's often equivalent to things that you can say in other ways with a phrase right . so i guess i had the example right at the beginning where i had a person on a snowboard and snowboarder . it seems like well those should be counted as paraphrases and mean the same thing . 
and i'm only gonna be easily able to capture that kind of similarity if i'm using the same vector space to represent both phrase meaning and word meaning . and so the question is how can we go about doing that . had a big lexicon of words and said let's learn a meaning representation for each one of them . and we were able to do that . i mean that's clearly not possible for phrases like the place where i was born because we just have an infinite number of such phrases as they get longer . and so we can't possibly calculate and store a vector for each phrase . and as we started to see last week is even for words in a lot of cases it seems like it might actually turn out to be sort of unappealing to store a vector for every word especially when they're words that have some morphological complexity like snowboarder which is snow board . and so we started to talk about even words how we might want to compose their meaning out of smaller units as part of some neural network . so again that's what we're gonna want to do for having these bigger phrases in language . we'd like to be able to use the principle of compositionality which is sort of a famous thing from philosophy of language or semantics which is saying you can derive the meaning of a phrase or a sentence by starting with the meaning of its words . and have some kind of then composition function that you can then calculate meanings of bigger units as you combine things . so what we'd like to be able to do is put together my birth as two words and have a meaning for that phrase a meaning for the phrase country and keep on calculating up and get some meaning for the whole phrase in the vector space . okay so if we build models of this type we can potentially hope to do two things . we can potentially use them both as models that will build structure that will actually build sentence structure as they go . and they will also build semantics that they will build a meaning representation for these phrases as they build up . so the general picture of what we're going to want to do is we're gonna start off with a bunch of words and their word vectors which we'll look up in the lexicon . so we're gonna say that's a noun phrase . this is a noun phrase prepositional phrase verb phrase build a sentence . so we'll have a kind of a syntactic and then we'll using that kind of build up the semantic representations . and for that case i just sort of knew what the right phrase structure i wanted for the sentence was . 
and just sort of drew in those nodes and calculated their semantics . well one of the questions is how can we calculate that as we go along . and i'll come back to that in a minute . then before doing that i just wanted to spend a couple of slides just sort of going back over the connections and differences between the model types that we've been looking at recently . so up at the top half we now have our tree recursive neural network . and in the bottom part we then have our recurrent neural network . now in some sense the kind of linear sequence models that you get for recurrent networks are kind of sort of like a sort of a limit case of a tree so that if you sort of stare down from about this angle what you're looking at actually looks like a tree right . if you sort of tip it it's sort of like this tree but it's a tree that's always sort of right branching down to the right . and actually it turns out that quite a lot of english structure is right branching down to the right in most sentences that you sort of get these pieces of constituency where you get left branching like the country in this example . but if you look at the details of these two models the details of the model are kind of different . because in this model we're exclusively sort of building upwards . we're taking smaller pieces of structure and then computing a representation for a larger piece of structure that they can pose into . actually a kind of a funny sort of mixed model when you think about it comparing it to a tree cuz this sort of the word vectors are going up to compute something but then simultaneously we have something going down the tree . and that's sort of the idea that richard was mentioning last time . how the recurrent models are really sort of capturing representations of whole prefixes and you're not getting any representations of smaller units than that . there are a couple of other pluses and minuses to think about . so the problem of tree recursive neural nets is that you have to get a tree structure . and so this is actually a huge problem . i mean if i had if i admit right at the beginning tree recursive neural networks have not swept the world . there's some really good linguistic reasons to like them and we'll say some stuff about them . 
but if you just sort of go out what people are using in neural networks for language you have to look for a while to find people using tree structured models right . there's ten times as much use of the lstms that we've talked about previously . and a big part of that reason is because the user tree recursive model you have to have a tree structure . and for some of the things that we've talked about i think you can sort of immediately get a sense of why that's problematic . cuz putting a tree structure over a sentence is making deterministic categorical choices as to which words are going together to be constituents while other words aren't . and anywhere you're making categorical choices that's a problem for learning a model simply by running back propagation . and so that sort of puts it also means that they're kind of gpu unfriendly cuz there isn't just this sort of simple lock step computation like an lstm gives you . so lstms have this very simple structure cuz it doesn't matter what the sentence is . the structure is always the same right . you just have that same sequence model that chugs along from left to right which makes them very computationally appealing . but of course they have the disadvantage that they're not actually representing any of the structure of the sentence . and if you want to get back to my original picture with the cmu bag of words i think there's just sort of a manifest sense for human languages that if you just want to have this sort of first cut roughly the structure of human languages write . what you gonna have to have is sort of know which sub units of words goes together to have behaviours constituents and the semantic parts out of which bigger sentences are described . okay so conversely we can also think about the relationship between the tree recursive neural networks and convolutional neural networks . so these central difference there is that the tree recursive neural networks calculate representations compositional vectors only for phrases that sort of make sense that are grammatical which a linguist would say is part of the structure of the sentence . whereas what a convolutional neural network does is say okay let's just work out representations that every pair of words every triple words every four words . regardless of whether they make any sense or not . but again there's actually an advantage to that right . that since you're not actually having to make any choices and you just do it for every pair of words and every triple of words . you don't need a parser you have again you're back to this sort of uniform computation without any choices . 
but it's not very linguistically or cognitively plausible i feel . to some extent i actually think recurrent models are more cognitively plausible as an alternative to tree structure models and convolutional neural networks . so the sort of picture is that for the cnn you're sort of making a representation of every pair of words every triple of words every four words . where as the tree recursive neural network is saying well some of those representations don't correspond to a phrase and so we're gonna delete them out . so that for the convolultional neural network every bigram . so you have a representation for there speak and trigram there speak slowly . whereas for the recursive neural network you only have representations for the sort of semantically meaningful speaks slowly going together to give a representation for the whole sentence . okay so how do we go about calculating things in the course of neural network . so the idea is when we wanna build a representation of a larger unit what we're gonna do is take the representation of its children and we're gonna have sort of binary trees for what we're showing here . we gonna stick them through some kind of neural network and we're going to have one is going to be of a vector that's representing what is going to be the meaning of this larger unit if you construct it . but if we'd also like to parse at the same time and work out good structures . we also wanna have some kind of score as to say is this a good constituent . build a parser at the same time . so how we might we do that . if we sort of start doing it at the simplest way possible just using the kind of rudimentary neural networks that we've looked at . this seems like the kind of idea what we could build so we could take the vector representations of the two children which might be just words or might already be phrases that you've built up . we could concatenate them to make them into a bigger vector have a linear layer non-linearity put that throw a tanh and so we've just got the simplest kind of single neural net layer . and that will then give us our and then we want to score that and well one way we could score that is just having a vector here that we could then multiply this representation by a vector and that'll give us a score for phrase . and doing precisely that was the first type of tree recursive neural network that richard explored in his work back in about 2011 . okay but if we have just this . 
then we're in a position where we can use it to parse a sentence with the tree recursive neural network . and so the easiest way to do that and it's kind of similar in a way to what we did with the dependency parsers sort of three weeks ago . is to say what were gonna do is we gonna run a greedy a parser we're going to look at what seems best and make hard decisions on each action and then proceed along . so what we could do is we could start off with the sentence the cat sat on the mat and what we're gonna do in a sense is kind of like what the first part of a convolutional neural network does . we're gonna take each pair of words and we're going to calculate a representation for that pair of words . but we've got one other tool at our disposal now . that we're also calculating a score for these combinations and so we can say that the thing that scores best is putting together a phrase is these two words on the left . so why don't we just hard commit to those and say okay we've got the cat as the constituent . with this semantic representation and so then at that point we can for what to do next we have all of the choices we had before and additionally we have a choice that we can put the cat together with sat . so that's a new choice that we can evaluate and at this point we can say well looks like the best thing to do is to combine the mat together cuz that's got a good score so we do that and we commit to that . then we thought one new thing we could try cuz we could have the mat go together with on . and we can look at that and we could say yeah that's a really good thing to do . on the mat that's a really good phrase to have found . so the neural network will commit to that one and then we'll kind of keep on repeating and we'll decide putting sat together would be a good idea sat on the mat that's a predicate and then we'll combine that together with the cat and we're done . and so we sort of greedily incrementally building up parse structures as we go along and working out the parse structure of the sentence . and so at the end of the day that's a parse tree and we're gonna have a score for our parse tree and that score for the parse tree we're just gonna say we made we've got a score for each individual decision as we put two node's together . and the tree has got a bunch of nodes in it and we're just going to sum those scores of each node decision and that will give us the score of a tree and what we'd like to do is find the very best tree for this bunch of words and we've kind of approximated this by doing this greedy algorithm where we just committed to what looked like the best constituent to build at every particular point in the time . and so the final thing was then sort of set up as a loss function with a sort of max margin objective . where you were sort of trying to adjust the parameters of the model so that you're maximizing the scores of the sentences that you have found . and then you're considering what the sort of structures you were finding what incorrect decisions you'd made versus what the gold structure for the sentence is meant to be in the sort of gold structure in the tree bank that tells you what are the right answers for it and in this kind of a model you'll sort of in theory what you'd like to do is to find the best tree for each sentence according to your model . 
and then changing the parameters of the model . so the model thinks the best tree is the correct tree in the tree bank . and that's then gonna be minimizing your loss . can require an exponential amount and so we're just substituting in this greedy finding of a parse that looks sort of good according to our model and using that in our loss function right here the sort of score of the parse that we found . rather than sort of keeping just one really best parse that you're finding you could have a beam parser and you could explore some different possibilities . and you could sort of have a ten best beam and sort of explore a bunch of parses to the end . and sort of then come up with a better estimate of what is the best parse according to your model . but the central thing to be aware of is to what you can't do is standard result for parsing context free grammars when you have grammars with labels . like noun phrase verb phrase and things like that . then what you can do is dynamic program and so you can then do parsing context free grammars in o(n^3) time and be guaranteed to have found the optimal parse for your sentence according to your grammar . the problem is here every time we're putting together two constituents we're running it through a little neural network and we're getting a vector out here . things together differently we're gonna get different vectors up here and so there's no substitute that if you were actually wanting to guarantee you'd found the best parse of a sentence that you'd have to do the exponential amount of work of exploring every different possible way of putting things together which we don't want to do . but in general you can work pretty effectively by sort of doing fairly greedy exploration according informed by your model to find good structures . okay so this is our overall objective function and we want to be changing the parameters of our model so that it's wanting to choose this parse of the sentence the one that's the same as the gold parse of the sentence . and so the way we do that is again so we'd sorted from the recurrent neural networks we had back propagation through time where we're sorting of chugging back through the time steps of your linear model . you can generalize that to tree structures and actually that was done by a couple of germans in the 1990s . goller and kuchler then came up with this algorithm called back propagation through structure . and in principle it's the same as the back propagation we've seen again and again and again . but it's sort of just slightly more complex because you have to be getting things working over a tree structure . it ends up that there are three differences . 
for working out the updates to the matrix w in your neural network . so just like an rnn the derivatives of the sort of error signals that you get coming into w everywhere you see it inside the tree structure . something that's slightly different essential back propagating down a tree structure . we then have to split the derivatives and send them down both branches of the tree structure to the next level below . and then when you're calculating your error messages you'll have an error message coming in from the node above . you want to be adding to it the additional error from the node itself . and then you wanna be splitting it and passing it down to the two nodes down below you . in these slides there are then some slides that go through that in more detail . summing derivatives of the nodes splitting the derivatives at each node add error messages . and actually from richard last year doing back propagation through structure . i thought i wouldn't actually try and explain in any more details than that in class right now all the details of that . but you can look at these slides on the website and chug through it in more detail than that . and i thought we could skip straight across to kevin who's gonna be doing today's research highlight . i am kevin and i am going to be presenting deep reinforcement learning for dialogue generation which is a paper by some people here at stanford . so the goal of this paper is to train a chat bot that can hold a reasonable conversation . and the authors approached this task in the sequence-to-sequence framework where the input sequence consists of a message or perhaps several messages from a conversation . and the output sequence is a response to the message and that's what the chat bot will say . so they use the exact same encoder-decoder model you saw for machine translation last week . the exact same training objective which is maximum likelihood estimation . so you find the data set of people talking to each other and you train the model by making it assign high probability to the responses people say . 
so once you've trained a model like this it's kind of fun to have the model talk to itself and see what happens . so this is a real conversation from the model in the paper . so the first chat bot says how old are you . and then the second one says i'm 16 and the first one says 16 . know what you're talking about the first chat bot says you don't know what you're stuck in an infinite loop . we can sort of point to some problems that might be causing this issue . the first one is actually the response i'm 16 . although it's kind of a reasonable follow up to the question it's not very helpful . so it'd be maybe better to say something like i'm 16 how old are you . and now you're giving more guidance in what you should say next . the second issue here is this i don't know what you're talking about response which actually is more or less a reasonable reply . and really the main issue here is that we're training our model to produce sentences that have high probability . but that actually doesn't necessarily mean sentences that are good and useful for the conversation . so with the i don't know what you're saying example it is high probability cuz really no matter what you're i don't know what you're saying and it sort of makes sense . so trained with maximum likelihood estimation the model thinks great this is a good response and we want some different objective to train the model . so that got a little bit messed up but the criteria we could think of for training a good response is that it is reasonable so it makes sense . but also that is non-repetitive so we don't get in an infinite loop . and that it's easy to answer so you say something a little bit more helpful than just i'm 16 . and in this paper the authors come up with ways of computationally scoring a response according to these criteria . so they end up with a single scoring function that takes the responses input and returns some number indicating did we do a good job with this response or not . 
and then they train a model to maximize the scoring function so i'm not going to go into detail on how reinforcement learning works . but the main idea is that instead of learning from an example so how a human responded to a message you learn from a reward signal . so we start off with as before encoding the message in a vector . a human-generated or a response that a human said and try to increase this probability according to the model we're gonna just leave the model to its own devices and have it produce a response . and then give it a reward signal which tells it did it do a good job with the response or not which is that scoring function i mentioned earlier . so here it's negative because i don't know isn't a good response . and through reinforcement learning the model will learn to not produce and so now on to some results how well does this work . these first results are quantitative results where the author showed dialogues produced by the system to humans and had them say which of these dialogues were better and here a positive number means the reinforcement learning system did better . so you can see that humans thought the reinforcement learned system was better particularly at making messages that are easy to answer and also for the general quality of a several turn dialog . we can also have our chat bots here you see that it's doing a bit better . so the first chat bot ask who old are you . but now instead of saying i'm 16 only it also says why are you asking . so it's kind of helping the conversation move along . but actually after a couple turns they end up in the same infinite loop . although reinforcement learning is a useful technique it doesn't kind of magically fix everything . so to conclude reinforcement learning is helpful when we monitor a model to do something beyond just mimicking the way humans perform a task . and it's been applied to many areas beyond dialog . so if you're interested there's a lot of new and exiting work in that direction . yeah so we'll have a bit more reinforcement learning in this class including i think next tuesday it might come up . but maybe i should just while we're on that topic advertise there newest stanford cs faculty emma brunskill started work yesterday . 
and in the spring she's gonna be teaching a class on reinforcement learning . so if you wanna get a good dose of reinforcement learning so what i wanted to do now was sort of show you a bit more about how we develop some of the ideas of having this tree-recursive neural networks . i guess i haven't really shown anything in the sort of quantitative results of show big results tables for that simple recursive neural network . but the summary of it was that we could do some sort of useful things with it for learning about paraphrases and getting syntactic structures right . able to publish a paper on it and all of those good things . it seemed like it wasn't really fully adequate for doing all the things that you wanted to do for understanding sentences and semantic composition . and there are a couple of ways in which that was true it appeared . and so for some of the later work and only some of which i'm gonna be able to show you today we were then sort of starting to explore better ways in which we could put sort of more flexibility or better neural units into this sort of same basic model of tree-recursive structure to be able to do a better job . and there are sort of a couple of ways in which it seemed like the model probably wasn't doing what you want . the first one the no interaction between the input words is i kind of think a common issue that happens with quite a lot of models if you have just a single neural layer . we sort of mentioned this also when we were talking about attention that if you just sort of concatenate c1 and c2 . and put them through a single matrix multiply and then a nonlinearity . that you can think of the weight matrix as sort of being just segmented into two smaller matrices . when one w1 same matrix multiplies by c1 and the w2 matrix multiplies by c2 and then you just sort of do the element wise non linearity . which sort of means that the two words really relate to each other . i'll come back to that as the sort of last thing i touch on today . but before we get to that one the other thing that seems kind of dubious here is . for all semantic composition we just have one composition function which has one matrix . so it doesn't matter whether we're putting together an adjective and a noun or a verb and its direct object . or even if we're putting together the rest of a sentence with a period at the end of it . 
in every case we're using exactly the same matrix multiply and saying just multiply by that matrix and that'll put together the meaning of your sentence for you . when you think about it and so an idea that seemed kind of a neat idea was . could we get something more powerful by allowing more flexibility in the composition function for different kinds of syntactic constructions in their composition . and so that led to the idea of syntactically-untied tree recursive neural networks . which actually proved to be a very successful idea for that parsed very well . and essentially what this model did was sort of argue that there's a reasonable separation that can be made between syntax and semantics in the following sense . that there's sort of basic syntactic structure of languages . so you have a noun phrase which can have a smaller noun phrase followed by a prepositional phrase like the man at the lectern . and that the prepositional phrase will be a preposition followed by a noun phrase . that kind of syntactic structure can be pretty well captured by actually a symbolic grammar . so we assumed in this model that we did have a symbolic context free grammar backbone that was adequate for basic syntactic structure . but the problem for sort of traditional nlp in linguistics is although such a backbone is pretty adequate for telling you the possibilities for it's not very good at working out which structures that you should build or what is the meaning of different kinds of units . so the suggestion is it's perfectly fine to use discrete categorical structures for the syntax of a language . but what we want to do is make use of our soft vector representations for describing the meanings of languages . and so therefore we can sort of start with that observation and then build the kind of flexibility of composition that we're wanting . by saying well if we are sort of knowing about something about syntactic structures in a categorical way . if i walk right over here well we can know the categories of the children so maybe this is a preposition and that's a noun phrase . we can use our symbolic grammar to sort of then say okay these will go together into a prepositional phrase . and so since we know these categories here we can then also use those categories to decide a composition function . so we can decide okay we're composing together a preposition and a noun phrase . 
so let's use the composition function that's the right composition function for putting together a preposition and a noun phrase . rather than just always using the same w matrix for any cases of composition . now we can say let's use the w matrix for putting together a preposition and a noun phrase . and that'll give us this bigger unit which will have some category according to our syntactic grammar . and then we're gonna be putting we'll be able to use the right composition matrix to put together those categories . and there's some other good properties a practical sense . doing simple pcfg parsing with the categorical grammar is fast because we can do it just using the symbolic categories and dynamic programming . and then we only have to be doing the sort of deep learning on the structures that we know are the ones that we want to build . so it's actually we are using syntax than trying out every different way of putting pairs of words together . so essentially we're using the syntactic model to work out reasonably plausible structures for the sentence . and then we're building the semantic combination for those structures . and then using the semantics to do the sort of harder decisions as to what does this prepositional phrase modify and things like that . and so he called this result the compositional vector grammar where it's a combination of a probabilistic context-free grammar . plus then using this tree structured recursive neural networks grammar . and in this class we haven't really talked about sort of the whole history of doing parsing for natural language processing and the kind of grammars that people built . but in some sense you can think of this as a generalization of the kind of things that people have been involved in for the last decade for trying to improve the quality of parsers . so the starting point is you can just have a context free grammar parser . and that works very badly for natural language because you kind of can't do a good job of dealing with all deciding things like prepositional phrase attachments . so back in 2003 dan klein and me sort of said well if we did some manual feature engineering and we kind of split categories and we had fine grained categories . and so that we knew that it was not just a prepositional phrase but a prepositional phrase headed by all of our prepositional phrase headed by for . 
we could actually just have a cfg parse quite a lot better and that was true . then following on from that a few years after that slav petrov said well maybe we could actually learn those subcategories automatically and and that did work and simultaneously there was a whole line of work on doing lexicalized parsers . which sort of said well a reasonable way to represent the semantics of a phrase like the person at the lectern . is to say what is the head word of that phrase . it's person and just represent the semantics of the person at the lectern and that was a useful idea to help pausing and making dismbiguation decisions because to some extent that's right . but on the other hand your losing a lot because your saying the meaning of the person at the lectern is just person and you've lost all the other and so effectively for the cvgs we're trying to sort of extend that further and say we'll no rather than just having a sort of a finer grains in syntactic representation substituting in the head word and using it as a semantic representation . we can actually calculate the semantics the meaning for a whole phrase and then use that for doing our disambiguation decisions in semantic parsing and that will be able to be more accurate . so here are some results from parsing . so this is sort of trying to parse for context free grammar structures natural language sentences over a famous corporates of and what we're scoring here is sort of an f1 measure as to whether you're getting particular constituents right . so you're making constituecy claims like that there's a noun phrase from words three to 12 . and then that's either right or wrong . and so you can see how there's been a succession of people gradually getting better at this task . so if you just sort of have a plain cfg your score is about 72% . so how more kind of manually feature engineered whoops sorry . manually feature engineered context free grammar was considerably better about 85% . some of the ideas of having putting in lexical heads were even better 87% . the automatically splitting which sort of mixes syntax and lexical information and but by build and it's sort of not that great because if you already just have one w matrix you kind of can't model a lot of composition . but by having this idea of this syntactically untied rnn where you can learn these different composition functions for different kinds of phrases that that actually worked very nicely and produced a strong well performing parser . i mean there are some better parsing numbers where people have done various kinds of self training in data orientation and actually there's some more recent results since 2013 that i don't show in this slide . but nevertheless this sort of proved a successful way to sort of build a sort of more semantically sensitive parser . 
in some sense the interest of if can get parse structures right for sentences . so the biggest interestingly different thing here is well actually we are computing a semantic some meaning representation for each phrase that gets back to that original idea of understanding meaning similarities . and that's just something that by itself that's sort of context free grammar isn't giving you at all . things you can see out of that . i mean one of the neat things you can see out of this is just sort of you can observe how the soft grammar learns notions of where the information is and so what are head words and phrases . so this is as it starts to put together pairs of words that you can see by the activations in the matrix as to where it's getting information from . and so there's something you have to know to interpret this . so for training this model what richard did was he started off the matrices with sort of identity initialization . but they're sort of kind of two half matrices with identity initialization because this is sort of the part of the matrix that's multiplying the left child . and this is the part of the matrix that's multiplying the right child . so they were initialized with identity initializations we've sort of spoken about before that sort of has a similar effect of allowing this sort of propagation of information at the beginning of training to the kind of thing that an lstm does . and so if you're putting together a noun phrase with a conjunction so this is sort of something like the student and . that most of the information is coming from the student and there's relatively little coming from and . if you're putting together a possessive pronoun and the rest of a noun phrase . most of the information is coming from cat with relatively little coming from his . if you're putting an adjective together with a noun so this is something like red chair . it's learning with your gain kind of quite a lot of information from both sides . this is a whole adjective phrase and a noun so this is something like extremely dark movie . and there's sort of some structure the same dimensions seem to be marking the kind of modifier meaning for just the plain adjective . the more interesting thing are we actually getting sort of a semantics for these phrases and sentences that is capturing semantic similarity in the way i claim right at the beginning . 
and actually that did work reasonably well . so here is one sort of test that we did to try and illustrate that . so basically we're saying okay for any sentence or any phrase we've calculated a meaning of that phrase so that we can sort of place into our vector space . so just like for word vectors we can then say what other sentences are placed nearest together in the space . cuz they should have the same meaning . so if the test sentences all the figures are adjusted for seasonal variations . the two closest of the sentences in the wall street journal corpus . all the numbers are adjusted for seasonal fluctuations all the figures are adjusted to remove usual seasonal patterns . and this is kind of actually nice right . that well in some parts like all the figures are the same all the numbers are very similar . but in other places it seems to have learnt quite interesting things . so are adjusted for seasonal variations are adjusted to remove usual seasonal patterns . so that's actually quite a different piece of word choice and syntactic instruction . the two closest sentences were harsco declined to say what country placed the order . those ones aren't quite so excellent you could say . i mean to be fair i mean something that you have to be aware of is that there are limits to how perfectly you can find other sentences that mean roughly the same thing . cuz this is only being run over corpus of about 40000 sentences so except the sort of fairly formulaic utterances that get repeated quite a bit . often you're gonna have to be choosing sentences that sort of somewhat different . so you know there probably aren't other sentences with knight-ridder not commenting on the offer . are perhaps a little bit too different . 
declined to say what country placed the order . but nevertheless it does seem to have captured something as to what the main semantics is going on . so all of these so this first sentence is a company not wanting to say something about some transaction . and both of these two closest sentences . also a company not wanting to say something about some transactions . so there is a sort of meta-sense in which it does seem to capture the semantic similarity pretty well . and again the two sentences that are closest to that are both other sentences when sales are growing further . that still seemed to work pretty nicely . we still kind of weren't really convinced that we're doing a great job at capturing semantics of phrases . so now we change things so that we had different ws depending on whether we're combining an adjective and a noun and a verb and its object or whatever like that but otherwise we still had the problem that i mentioned if for that hadn't gone away . that when you're doing this matrix vector multiply that what you're doing is you still got kind of half of w as being multiplying itself by c1 and half of w is multiplying itself by c2 and there is no real interaction between c1 and c2 and that just doesn't actually seem what you want for natural language . and so in particular you know what every semanticist has observed and worked to account for in symbolic theories of semantics for the last 40 50 years is what you actually get in natural language is that you have words that act as operators of functions that modify the meaning of the other words . so that if you have something like very good or extremely good or quite good or any of those things . it seems like what you have is you have good that has a meaning . and then very is some kind of operator or function that'll modify the meaning of good to make it sort of more extreme and strong . or weaker depending on whether you're saying very extremely quite etc . so it sort of seems like somehow we'd like to be able to build neural networks that capture those kind of ideas of composition for language . so the last thing i want to mention today is this was sort of then so essentially if you have a vector here for good and then you want to be able to modify it's meaning with an operator like very and how might you do that kind of a natural idea from sort of linear algebra to think about is well what if i made very a matrix . then i can do a matrix vector multiply calculate a new vector and that could be a meaning of very good . and there had been some previous work that for particular phrase combinations had done precisely that . 
so there's been a paper or two that looked at adjective noun combinations or adverb adjective combinations by doing that kind of matrix-vector multiply . but we wanted something more general that could be applied to whole sentences arbitrarily to come up with a meaning and so this came up with a model where sort of we were going to have matrixes and vectors and we were going to combine them together in all ways to try and create sort of meanings for phrases so . if we now had very goodwhat we gonna say is well we are not quite sure when something is gonna be an operator and when it's gonna be operated on . so let's just have it both ways and see what comes out . so each word is going to be represented by both vector and the matrix so very and good are both represented by vector and the matrix and so then to compute representation of the phrase very good what we are then going to do is we're going to multiply the vector by the matrix . so big a with little b and we're going to multiply big b by little a and so we're going to do both vector matrix multiplies and then having done that we're going to concatenate those as we've done before . and then we're gonna multiply it put it through a neural network layer just like before so we have another w matrix to the decide which of these to use and how it goes through a tanh and that gives our parent representation . so at that point we've got a vector representation for the parent . but well we wanna keep on building this up into a representation of whole phrases and we wanna be able to build very good movie . and well at that point we sort of conceptually wanna be using the word vector for movie and would like to say well this is another operator and we'd like to multiply very good by movie . so to do that we're gonna have to also have a matrix coming out for very good . and so we wanted to build one of those and so . we said well in addition to that what we could do is put together the two matrices a and b that we got down here . we can concatenate them to build a bigger matrix and do a matrix multiplier on that and that would then give us a representation a matrix representation of the parent . and so formally now our parent has- will now have both a vector and a matrix . so i'll show you a bit about this model . it could do some quite interesting things and it also had some weaknesses . and i think- in terms of where the weaknesses are if i give the game away right at the beginning a lot of the weaknesses were in this part here for the matrices . cuz we sort of had problems with that . i mean firstly the matrices were kinda problematic because matrices have a lot of parameters in them . 
and so that makes it hard to learn them effectively and that's an idea that perhaps be revisited using some ideas that have come up since then but secondly we didn't have a very good way of composing matrices to build picking new matrices so that part of the model perhaps wasn't so great but here is sort of a picture that sort of shows you some of the things that you would like to be able to do and whether models can do them . so we're looking now at building two word combinations so fairly annoying fairly awesome fairly sad not annoying not awesome not sad unbelievably annoying unbelievably awesome unbelievably sad . and then what we're wanting to do is take those phrases and put a probability distribution over sentiment scores between 1 and 10 . so 10 is extremely good and 1 is extremely bad . and for some pairs of words we actually had some sort of empirical data that was kind of connected to these meanings . the empirical data was kind of a sort of distant supervision not to be trusted very much . but this was sort of saying well suppose we'd seen a review of a movie that said not said and that what rating was being given to that movie and it was sometimes a bad rating occasionally a good rating . and that sort of was shown in that but what's perhaps more interesting to see is that the sum of these combinations that the plain rnn is actually more effective than you might have thought it first it will be . so if it's something like unbelievably awesome that can be captured pretty well in the model captures that unbelievably awesome that even the basic rnn model is good at knowing that means it's a very good movie and very positive sentiment . we can even do some sort of more interesting things . so if you have the phrase unbelievably sad it turns out that that's kind of ambiguous that there many really good movies that are unbelievably sad . so there's a lot of weight over here and then there are some cases where people say a movie is unbelievably sad because it's just terrible . where you get some weight on both ends of the spectrum . and interestingly even the plain rnn and also our matrix vector rnn is able to capture that pretty well so that's kind of nice . but there are some things that the basic rnn just isn't able to capture right where our new model does a lot better . so if you look in these middle row ones like not annoying and not awesome . that by itself the word not just tends to be a marker of negativity right . people who go around saying not a lot negative people . so all else be equal if you sentiment is more likely to be negative . and then the word like annoying that's a negative sentiment word . 
and so for the basic rnn model if you put those together they just have a kind of additive effect it turns out so that's the green line where it says not annoying it predicts that means bad movie low rating whereas the result we'd like to have is that not is an operator and it modifies the meaning of annoying and this means like it's not so bad and it's not annoying . and the interesting thing about natural languages is that they sort of don't work like sort of basic logic where basic logic might tell you that not annoying means this is a good movie because it's not annoying where if real human beings when they say words like not bad or not annoying they mean it's sort of okay . and so the distribution that the matrix vector model comes up with is sort of flat but it's sort of at least basically right that it actually gives the highest probability to the midrange of the distribution which is kinda correct . and you see a similar effect with not awesome that the basic rnn isn't able to capture that it's still giving most of its weight to saying i've seen the word awesome this is a good movie whereas the matrix vector rnn is at least done sort of better and it's tapped down giving weight to meaning that it's a great movie . so it sort of seems like we're kinda doing better at being able to model meaning combinations . okay so the question is is it really wise to be doing it both ways with the matrix and the vector or could you actually use your syntactic structure and know which way to apply these two things and do it only one way . yeah so these results even though all my phrases have the operator on the left and the thing being modified on the right . this is sort of running that symmetric matrix vector model and so i think you're totally right it wasn't something we did in this work . but it seems like you can very reasonably say well wait a minute . why can't you take both of these two models that you've just shown us . if you're using the sort of syntactically untied model you know that if you're doing adjective noun combination you should treat the thing on the left as the operator . and then you can just run it in one direction and i mean in general i think that'd be a very reasonable thing to try . it sort of means that you have to have complete coverage . and so you've decided in every case of when two different categories come together . you have to decide for sure which is the one you're going to treat as the operator and you know that might actually require quite a bit of work to do . in principle you could do it and that would work . yea so we're basically done for today but just to show you sort of one other thing that we were able to do with the matrix vector model which was sort of a nice example of how we're able to use this model to do an nlp task . so this was a task of learning semantic relationships as a kind of relation extraction task . so this was a data set that some people had explored where you had sentences like my apartment has a pretty large kitchen and then what you meant to say was what is the relationship between apartment and kitchen . and there were set of relationships that they could have and one of them was that component whole relationship and so the correct answer here was to say that the kitchen is a component of the apartment and then there were various other relationships that could be a tool what you were using . 
it could be the material various other kinds of relationships . and so we explored using this matrix vector recursive neural network to learn these relationships . and so the way that we were doing that is we were sort of building up semantic vector rnn model and so we build up this semantic compositions until you reach the point where the two noun phrases of interest joined together in the structure of the sentence . so the movie showed wars so this was the sort of the message the content that's being shown on this media . we'd say okay we've built up a semantic representation that so at this point we then use another neural network that's just a straight classifier that says okay classify the relationship here it's an example of message topic . so we built that model and this is a nice example where it seemed like we weren't able to show that again having this extra power gave us extra power . so here are just some results on that . so people had worked on these data set previously . vector machine got about 60% f1 maxent model so it's like a logistic regression model with a lot of features got 77% . svm model with a huge amount of hand built linguistic resources and features . so this is using everything it's using wordnet dependency parses levin verb classes propbank framenet nomlex-plus google n-grams paraphrases textrunner . every feature and knowledge source you could possibly think of to throw into it got 82.2% . so here are our results with our neural network model . so the plain recursive neural network got held a little under 75% so that's actually pretty good when all its doing is learning its own semantics representations that puts things together but not quite . so the matrix vector model actually is clearly doing something useful so it's sort of getting you about 4% better scores . so that shows we have made some progress in semantic representation . of course like everyone else we wanted to have our model better so then we sort of built a model that sort of put in a few semantic features but only sorry a few extra features but only fairly basic ones . wordnet part-of-speech and ner and that was sort of hey i wanna push this just over the line but i think the main message is that you're sort of starting to get decent semantic models of phrase relationships without actually having much more than just these continuous representations of semantics . okay i'll stop there for now and we'll get to more next week . the crucial end phase of the semester so i guess we're now at the start of week nine . 
so first of all if i just do sort of reminders . obviously everyone should keep working on their final projects- assignment 4s . a couple of just notes on that . so on thursday we're just going to talk about dynamic memory networks . while they are only one of several ways that you could go about approaching assignment 4 . they'll certainly be relevant material if you are doing assignment 4 because it's an instance of the kind of architectures of sort of attention based architectures that people can use for tasks like the reading comprehension question/answering like the squad data sets so watch out for that . we've been trying to keep money in people's azure accounts actually a microsoft rep kristine is here right now if you need to pester her . you're certainly contact us on piazza if they're any issues and we've been trying to be proactive at keeping things restocked . i know it's slightly frustrating if you go out of money and it then locks you out and we need to reset it but we're doing our best . okay and it's great that there are now lots of people that are clearly very actively using it and doing stuff . okay then for assignment four so for the assignment four submissions we're doing submissions on codalab which is conveniently tied right into azure as well . and so for assignment four we've set up a leaderboard at least when i looked this morning it only had one submission from chris and the people who set up the leaderboard . and they were only getting 2% on squad so if you can get higher than 2% on squad at least temporarily you could be top of the leaderboard . i hope people can try that out . and so a couple of percy liang's ras have been very actively working at helping us out of doing this using an assignment four . so big thank you to percy and his ras . and so they've also made a couple of videos on how to use codalab that there's short videos . there was an announcement about it on piazza so have a look about those . okay so then moving right along for today's lecture so for today's lecture i'm gonna talk about coreference . so when we did the mid quarter survey one of the things that a whole bunch of people complained about was that we actually weren't doing much linguistics and natural language content in this class . 
so today it's getting a little bit late since it's the start of week nine . i've actually got a try to have some more linguistic content in the first half before going back to deep learning models for the same . i think that sort of comment in the mid-quarter evaluations was completely fair because the reality was in the first half of the class . it really was sort of just about all deep learning models all the time . i mean i'm not sure i've yet worked out the perfect solution to that because the fact of the matter was we kind of felt when organizing the class . that we were sort of on this treadmill where we had to get through more stuff in and so that is what it is but over the last couple of weeks we'll try and have a bit more nlp content as we go along coreference resolution is an instance of a task . and it's really the only one that we're going to look at in any depth here . where we're working on a larger level of a text so that we're not longer just trying to look at an individual sentence and say what's the subject and the object and parsing or is this a company name . where we're trying to make sense of a bigger text and work out what's going on about that . and it's not the entirety of understanding long text but it's sort of one of the most prominent things you need to do as you go along . so when we're doing coreference resolution the first thing that we're doing is working out all the mentions in the piece of text so that pieces of text basically noun phrases that refer to some entity in the world . and then once we've got those we're trying to find the ones that refer to the same real world entity that co-refers . so there's one set them here which co-refer in this example to barack obama and then there's another set which are these ones here and they're the ones that co-refer to hillary clinton . okay so that's our task of coreference resolution and so i thought next we could just go through really get a sense of what goes on in coreference resolution of go through an example text . and this is where i signal to these guys here to flip me over to my other screen . okay so here's our example text done this is from a short story story by shruthi rao called the star . now i have to admit since this isn't a literature class i actually made some little cuts and edits to this story so a larger font size on my slide . so the text is slightly mangled but it's basically part of the story so what have we got here when we do this right . so first of all we have the named entities which is precisely what you were finding in assignment three right . so we have vanaja and akhila and there's akhila and there's prajwal akash lord krishna he is a named entity akash . 
so we've got all of those but then we have a lot of other kinds of phrases that refer in the text . and the second prominent category is that we have pronouns . there's this one thats a pronoun that's kind of a special pronoun herself . then i admit i noticed i missed at least one of the named entities and there are probably other things i've missed so you can tell me what i've missed . okay so those are both prominent categories but that's not all there is there's really a third category . which are then things that are mentions but are done with common nouns so that they're neither pronouns or named entities . so that's something like the local park well there's her son is such an example the same school the preschool play . so there's sort of you get an interesting thing is you get embedded ones . so the preschool play is in reference of a mention of an entity but inside that there's the preschool which is another mention of an entity . okay so there is the naughty child a tree the best tree a brown t-shirt brown trousers the tree trunk a large cardboard cut out . okay then there is a circular opening there are red balls . so they are those ones and then there are some other things that are common noun phrases and it's not quite so clear whether they're actually mentions of anything in the world . is that a mention of anything in the world . and then there's this a tree's foliage which doesn't really seem like it's referring to anything concrete in the world . so there are various complicated cases like that . and in particular there's another one of those more complicated cases at the end which i'll maybe come back to later this the nicest tree . and then the task we want to do is to start to then work out which ones corefer . for a start there's then vanaja here . then what's the next thing that refers to vanaja . so this is her which i guess i forgot to mark when i was marking the pronouns . 
so embedded in the mp her son so again you can get mentions and side mentions there's that her . is there anything else that is coreferent . she okay so there's that she there . and then the next one is herself . right and so here we have these reflexive pronouns . and so reflexive pronouns are kind of special because they always corefer very closely back to each other as in that example she resigned herself . now we have akhila which is akhila . sometimes you just get names being repeated as names . are there other things that corefer with akhila . note that this is kind of part of how it's tricky right . because well here at the beginning we have the names of two women . akhila's name is actually repeated in the second sentence . but somehow we have to understand enough of the text beyond that to understand that all these a her aren't referring to akhila at all they're all referring to vanaja . so we have these two entity chains and then we have some more entity chains . so if we go to the next one the local park that one is just a singleton . nothing else refers i believe to the local parks . there's something for things have just been mentioned once and never repeated . and so then we have prajwal who sort of appears twice here . like there's akhila's son which is sort of a descriptive term and then his name prajwal so that's generally referred to as apposition . so we get two mentions of him right there . 
and then where else is prajwal appearing . well one's his name is right here . are there other places that prajwal appears . there is this they which refers to two people right . so that's a phenomenon maybe i'll start putting akash in as well . so we have this phenomenon here of when you have split antecedents . so you can have a plural they that's referring back to two things that are disassociated with each other they're just discontinuous in different places . when we start looking at co-ref algorithms that nlp people use a bit later one of the embarrassing things that you will notice is that the standard algorithms that we use just can't handle this kind of split antecedents . that you're looking at a mention and you're trying to decide what to make it coreferent to and you just can't get ones like that right . so that's a bit of embarrassing but that's the current state of natural language processing . okay so he goes through all the tree . so what other entities are there that occur multiple times here . if you think about it there's definitely here a tree but if you think about it which of these things count as mentions in the real world and which one do you want to deem this coreferent is actually tricky . if you just half look at it you just think okay anytime i see the word tree in a noun phrase i'm just gonna say all of those coreferent with each other but that doesn't actually seem to be right . cuz when it was here akash was to be a tree that's not talking about any specific tree that's referred in the world that's some intentional description . she resigned herself to make akash the best tree that anybody had ever seen . again that doesn't seem to be referring to any particular tree that's extant at any time that's some kind of descriptive text . on the other hand when it's gone down to she bought him a brown t-shirt and brown trousers to represent the tree this is now it's an abstract tree costume obviously . but that actually seems to be a real tree that's a thing in the will that you can point at . so that's a real thing and then well what about when it says a tree's foliage . 
is that referring to that tree that she's building . but by the time it says the tree that she's making and it's a good clear one . but then the last sentence says so for that one the it is clearly again referring to the tree that she's constructed . the question is what to do about the nicest tree . and to be honest if like for various other nlp tasks for co-reference resolution people have constructed data sets where people have essentially done what i'm trying to do live in front of you as to identify mentions and then say which one's a co-reference . and so what we have here for something like it truly was the nicest tree . the nicest tree is referred to as a predicate nominal . of a noun phrase but it's actually a property that is being predicated of the subject . and so some of the data sets that people use for co-reference they declare predicate nominals like this to be co-referent to the subject and but there's kind of an argument that that's actually just wrong and the predicate nominal is actually kind of a descriptive property of the nicest tree and it's not actually referent to anything . and so then it wouldn't be what you're wanting to do . but i'll leave my purple there for the moment okay . are there any other interesting things i should comment on . so there's yeah so i mean there are obviously lot's of other things that are mentioned that aren't in change right . so something like akash's face is a mention that's a singular mention . there's a circular opening i guess that's kind of an interesting one cuz it seems like that is an entity in the real world . but it's an entity that's a hole in the world as opposed to a thing that's in the world . so there are lots of real world complications as to how things pan out in co-reference but that's sort of an idea of that task and the problems . okay i will go on from there by sending back my person awesome . okay right so what we've seen from that is basically what we're working with is the noun phrases . most of them refer to entities in the world . 
there are many of them that in pairs refer to the same entity of the world and they're the ones we're gonna call co-referent . and then the other interesting thing that's different to what we tried to do with that there are lots of cases in so when you have cfo of prime corp but cfo of prime corp is also a mention . his pay is a mention but the his inside is also a mention . so you got lots of these nested examples . i mean in truth you'd get the same thing happening also in named identity recognition . and some people including a former student of mine jennie finkle actually looked at that . because there are a whole bunch of cases that also in sort of names of things when you have something like palo alto utility company that you have the organization which is palo alto utility company . and inside that you have a location that's palo alto . though in general in ner people just do it flat and you kind of lose those embedded locations . but if you're wanting to follow along co-reference links like john smith and his pay that you're sort of really losing a lot in your ability to interpret texts if you aren't dealing and so normally people do . so co-reference resolution is a really key task . it's used in all sorts of places . essentially anywhere where you want to do a fuller job of text understanding you need to have co-reference . so if you want to sort of understand a story like the story of the star where you definitely need to be able to follow along the co-reference . it helps in lots of other tasks . so if you wanna do machine translation if you're doing machine translation from one of the many languages like turkish that don't distinguish gender in pronouns . and you want to then translate into say english and it does have gender . then what you need to do is follow along the co-reference chains to be work out which ones should be he and which ones should be she . it's been observed and complained about by a number of people recently that current mt systems don't do that . if you take turkish and you translate into english everything comes out male sorry . 
that's a state of nlp on that . so text summarization including things like web snippets right if you're trying to cut out sort of a little snippet to put on web results . and it contains a pronoun in it it would be much cleverer if you could replace it with its reference so its interpretable . and then also tasks like information extraction relation extraction question answering . this doesn't apply to the squad but a lot of the time we have questions like who married claudia ross in 1971 . and you start searching the text for the answer to that question . and you say yeah i found the right place to look . and you're sure you've got the answer if only you could work out what he was co-referent to and that's why you need co-reference resolution . so when we've made all of these attempts to link things together i'll just explain now how we go about evaluating co-reference resolution . so effectively co-referenced scoring and basically any metric that people have used for cluster evaluation the co-reference resolution . and so the one that we're gonna emphasis in today's lecture is one called b cubed which is one of the widely used clustering evaluation metrics . so what you have here i mean i've sort of just duplicated on both sides . so i can show you precision recall is so are the gold answers of what's correct . and then the circles that i've drawn around it is how my system has it thinks its co-reference . and so what you do for the b cubed metric is you sort of align system clusters and the gold clusters . so i've chosen to align this system cluster with the blue color here which i've shown by that black around the circle . and then i say okay well of the things that i put in my system cluster what is the precision of what i put in there . and well it turns out that four out of the five of them are blue and one of them is pink . and so i say my precision is 4 5ths for this cluster . then i do it the other way around and i work out a recall . 
so i say well i aligned the blue things with this system cluster . and well actually this system cluster only contains four out of the six blue things . so my recall for that alignment is then four-sixths or two-thirds . and i'm gonna put those together in an f measure and that's then going to give me the b cubed measure . and so that's sort of the main idea . it's just a little bit more complex than that . i mean first of all obviously i want to do it not only with that cluster . i also want to align this cluster with the oranges and say they're precision recall one cuz they're completely correct . and this cluster and the pinks and then i wanna say precision four sixths and recall four fifths . so there are a couple of other tricks . one is that you're weighting the different precisions and recalls based on the size of the clusters . so it matters more to get high precision on really big clusters . the other bit that i sort of slightly glossed over is i said well you align these system found clusters with a gold cluster . and as you might know from some other class doing a bipartite alignment or things of that sort that's actually an np-hard problem . so it's sort of almost impossible to guarantee that you've found the optimal b-cubed score . so normally what you're actually finding in your system is sort of a lower bound on a possible b-cubed score . but in practice provided your system is reasonably good it's fairly easy and a greedy manner to start aligning together system clusters and gold clusters starting with the ones that you did best . and in practice there's greedy matching software that's used for b-cubed which seems to nearly always work and give you the right answer . and so that hasn't been a huge problem in practice . that's only one measure that's been used for coreference . 
there are a whole bunch of other ones that have been used . most of which relate to clustering algorithms evaluations that people have used elsewhere . okay so before getting to the halfway point i then want to say just a little bit more about what goes on in coreference from sort of a linguistic point of view . and this is actually a little bit interesting and hasn't actually been much dealt with by nlp systems . so what kinds of things do we have . so we have referring expressions so things that directly refer like john smith as named entities or the president as common nouns . we then have things that aren't directly referring but or sort of variables that are contingent on something else . and there ones that are free variables . so his pay that's sort of a free variable but it's reference is dependent on the reference of smith . and then we have these reflexives the bound variables which are sort of closely connected with something nearby . and so in linguistic theory most of the work is dealt with these variables and are going to be coreferent with . whereas in doing practical coreference of a real text there's quite a lot of pronouns . but a lot of the actions was actually dealing with these proper noun and common noun referring expressions . getting these guys right is actually harder than getting these guys right so things to notice . so if we have every dancer twisted her knee her knee does not refer to anything because it's sort of embedded under this quantifier of every dancer . that's perhaps more clearly seen if you look at the second example . there's no her knee being talked about right . so that's a clearly non-referring noun phrase . okay and similarly that refers to any dancer either . so in linguistics people normally distinguish two relations . 
so one of them is coreference which is when two mentions refer to the same entity in the world . and that has nothing to do with the structure of text . and the other one is a relation of anaphora which is a textual relation when a term the anaphor refer gains reference with respect to another term the antecedent . so you're using it for its interpretation . so if you go back to greek roots this word whose interpretation was dependent on something and so anaphora was distinguished from the opposite relationship which was called cataphora where you actually had a dependent term that was dependent on something after it in the text . from the corner of the divan of persian saddle-bags on which he was lying smoking as was his custom innumerable cigarettes . lord henry wotton cold just catch the gleam of the honey-sweet and honey-colored blossoms of a laburnum . this is a laburnum in case you were wondering . cataphora because the referential noun phrase is lord henry wotton . and then both he and his then cataphors on the following lord henry wotton . again it turns out in nearly all of our nlp systems we never try and do this . so we're always coming across mentions and then we're trying to assign them to something before them . so we always treat them as backward looking anaphora . so we'd be actually hoping to say this he doesn't refer to anything before it . and then we'd be later on saying lord henry wotton is coreferent with the he . but that's actually sort of linguistically bad and doesn't make terribly much sense . so a lot of the time things that are anaphoric are coreferencial because the textural dependence is one of identity . so an anaphor is coreferencial with its antecedent . so you have things anaphoric relations that aren't identity relationships and then they're not coreferential . and so here's an example of this . 
we went to see a concert last night . so the tickets here is an anaphor that's dependent on reference to this antecedent . because it's meaning that the tickets for the concert were really expensive . but it's not an identity relationship so those are referred to as bridging anaphors . and there's been a little nlp work on trying to interpret bridging anaphors but extremely little . for most of the coreference systems a concert is a mention of an entity . the tickets are a mention of an entity . and you just don't learn the relationship between them . okay so there are really so you can have anaphoric relationships in the text which may or may not imply (co)reference 90% of the time they do but (co)referential relationships with two things in the text referred to the same thing in the world . but that may just be because they refer to the same thing in the world . there isn't necessarily any textual dependence relationship between them and so something that you might like to think about is maybe those two phenomenas should actually be handled somewhat differently in our models and the truth is for most of the models we build at the moment they're really not handled differently . you could hope if you crossed your fingers really hard that somehow the way our neural network model works will end up treating the pronouns which are normally anaphors sort of differently to the way it's treating you know the various mentions of a cache which were just (co)reference relationships but fingers really hard there's nothing really that model structure that's sort of really distinguishing these two notions . okay so on the second half of getting to say how we build (co)reference systems but before we do that we're onto the research highlight and james is going to talk about that . today's research highlight will be summarizing source code using a neural attention model . this paper was published in acl 2016 and its by authors from university of washington computer science and engineering department . so the main task in a dataset that they define is to generate sentences that describe c# and sql queries and they use a dataset from stackoverflow . essentially what they do is they just query the whole dataset for all the posts that have tags that have c# sql or database or oracle in them . well as you would expect just doing this naively doesn't work very well because there's lots and so one of the cleaning sets that they do beforehand is to remove all the posts where the question and the text doesn't actually have any relevance with the content of the code . for example like people often ask questions about codes such as like how can i make these code run faster and in that sense that won't be a very good summary at all . a second thing that they do in order to a more technical thing that they do is they actually try to parse the code in the sense that like a lot of code contains like literals they have specific variable names things like this which are not very general like general systems to try to summarize code and what they do in this case is they actually replace literals with their types . 
they also replace the table and column names with something more generic and then they also remove in-line comments in order to make the system and two examples of the code are shown on the side where one is c# code for getting the whiff of a text block in some view i think and the second is source code for sql where you're trying to get the second largest element . and specifically like they define two tasks that they're actually going to try to attempt and these are to generate to describe some code sequence that maximizes a scoring function . a second task is the information retrieval task which is to go through find the code snippet that most closely relates to the input question which would in a natural language . the scoring function is shown to the side . it's essentially the product of the next word probabilities and these are proportional to what the output that we get from their model which takes into account the hidden states of the lstm and also some attention on the source code . specifically here's an example of how they generate their text using their model which they call code-nn . you feed in some starting token and then you make some form of prediction based on the attention and based on the lstm to get on the next word n1 and then you keep doing this iteratively again and again and this is how they so to evaluate their system they did first on the text generation side they compared against well they used existing mt metrics such as meteor and bleu and they took some existing translation system and information retrieval baseline system called moses which is a phrase then on a previous model that also know that and they found that their models get higher scores on essentially meteor and bleu and then they also did something that's a user study where they got five people and they had them rank the result . like manually score the results in terms of naturalness which is how well does the sentence actually read in terms of fluency and informativeness . in terms of how much of the content of the code was actually captured by the summary and they found that their model unsurprisingly does better than existing approaches . and then their information retrieval mean reciprocal rank and they compared against some existing previous papers and existing baseline out there . so what's more interesting is the actual example outputs that the model generates . here's an example of a c# code which is to add children to some tree node and treenode is actually part of a treeview and in this case code-nn actually gets pretty close where they recognize that these tree nodes are related to tree views but it doesn't quite get the idea that you're trying to add instead of get them all . on the second example where the code-nn actually got the right result is this query where we're trying to select random rows from a table and code-nn actually get's it exactly . so now for the remaining 40 minutes it's now algorithms to try and do (co)reference resolution and so i guess for about the first 15 minutes i'm gonna sort of say something about sort of the history of ways of doing (co)reference resolution anaphora resolution in general and just the sort of space and traditional methods . and then sort of for the last 25 minutes i'm gonna talk about one particular way of doing it which is actually from a paper that kevin clark did . the most famous thing in the space of (co)reference resolution actually just an algorithm for determining the pronominal anaphora resolution working out . what you know he him she hers its refer to is an algorithm that was proposed by a long time ago by jerry hobbs which these days is normally referred to as the hobbs' algorithm . but actually in jerry hobbs' paper he refers to as the naive algorithm for a reason that i will explain in a minute . and so this algorithm i'm not gonna read through it it's a complex mechanistic procedure for deciding what a pronoun refers to . so you begin at the noun phrase so it's assuming a syntactic pass of the sentence begin at the noun phrase immediately above the pronoun . 
go up the tree to the first np or s . call this x and the path p . traverse all branches below x to the left blah blah blah and this is in all of it . it keeps on going on the next page and it's got go-tos go to step 4 . so the sort of embarrassing thing is that not in the system i'm gonna present at the end part of class . but if you look at the sort of machine learning approaches to co-reference resolution that were done in the 2000s and the first half of the 2010s nearly all of them used this algorithm as a feature . so if you had a regular statistical classifier you can take any kind of little sub routine and sort of put its judgment in as a feature into your logistic regression . and it turned out that what this calculated was sort of a useful enough approximation to getting out most likely anaphoric relationships out of syntactic trees . that most machine learning systems use this and got value out of that as a feature . so here's the kind of idea of how it was meant to work . so "niall ferguson is a prolific well-paid and a snappy dresser . stephen moss hated him." okay so here's a him and you're wanting to work out what that's co-referent to . and so you start at this noun phrase here . and so what it said was that you started from this noun phrase and you went up to the next s or the path that you'd gone up p . and then it says traverse branches below x to the left of p propose as antecedent any np that has an np or s between it and x . so i traverse things to the left and i can find here a noun phrase . but that doesn't have anything else in between that and so therefore it's not a candidate . and so at that point i'm going to be at the highest s in the sentence and i'm gonna traverse the parse trees of the previous sentences in the order of recency . and i traverse each tree left-to-right breadth first . so there's a lot of stuff embedded in this very complex mechanistic procedure . 
but most of it is sort of correct and gets first order linguistics right . so when we see him the first thing to suspect is maybe it refers to something to the left in this same sentence . but that's where the kind of linguistic constraints on pronouns come in right . so that if stephen moss referred to him he couldn't of said him he would of needed to say himself right you have to use a reflexive there . so that's why it says unless there's some intervening np or s in between but precisely if there was a more complex sentence structure it would be a candidate . so if you had something like a more complex noun phrase in which stephen moss was a modifier then co-reference would become possible . so if it was something like stephen moss' brother hated him then it'd be possible for him to refer to stephen moss right . and so that will be captured because then there would an extra np node in between here then it would be okay . so that didn't work and so we went backwards and so then again now instead of using software heuristics . so the heuristics are usually right it said to go backwards to sentences and order of recency . so if the antecedent isn't in the current sentence it's mostly in the immediately preceding sentence so you look there first . and then it says within the sentence go from left to right . there's sort of an obliqueness hierarchy in sentences . and actually within a sentence a subject which at least in english is on the left side is more likely to be the antecedent than something that's an object or an indirect object or object of a preposition that's buried down here . so it's saying the first thing you should try is niall ferguson and so that's then a candidate . it doesn't get disqualified on page two from reasons of gender or anything like that . and so we propose it as an answer and the hobbs algorithm gets it right . and so the hobbs algorithm gets the right answer for pronouns about 80% of the time it's actually pretty good . of course sometimes it gets it wrong it's easy to come up with sentences that won't get it right for . so i just wanted to before going on deviate for a minute and talk about what jerry hobbs was actually interested in . 
so what jerry hobbs was actually interested in was knowledge-based pronominal coreference . and so from the early days of ai there's been sort of observations about how to actually get coreference right in many cases you actually have to understand sentences . and so there was this famous pair of sentences which was proposed by terry winograd who until very recently was on the stanford faculty . though he had kind of dropped out of doing nlpn and moved onto hci . and so terry contrasted these two sentences . the city council refused the women a permit because they feared violence . and the city council refused the women a permit because they advocated violence . this was back in the 60s and 70s when there was more protests around i guess . so anyway so in the first sentence the natural reading is the they is coreferent with the city council . and in the second sentence the natural reasoning with reading is with the they being coreferent with the woman right . and the crucial thing to notice is this isn't something that the hobbs naive algorithm could possibly get right cuz both of these sentences have completely identical structure . and so the answer that was suggested at the time was well what we actually need to do is have knowledge of the world and being able to sort of represent these actions and representing relationships that are likely to occur . and that we just need to know we have to sort of understand about city councils and permits . to understand that if you're refusing a permit that would happen if someone was advocating violence but who would be getting the permit were doing the advocation of violence . and so this is an idea that people have actually tried to resurrect recently . so hector levesque a good old-fashioned ai guy . and i guess he gave an invited talk in 2013 where he sort of suggested gee we should kind of try and get back to these kind of winograd sentences and actually be trying to understand them as interesting co-referenced challenges . and so people have tried to more recently run a winograd schema challenge . so really this is what jerry hobbs was interested in . and so actually why he it was actually one of the first instances of nlp when someone said gee before proposing something really complex i should have a baseline . 
so i've got a good baseline to compare against as to how well something simple works . was the kind of systems he built couldn't possibly beat his baseline because trying to do knowledge-based pronominal coreference was way too hard for what could be done back in the 1970s . but this is what he wrote about it . so he said the naive approach is quite good . computationally speaking it will be a long time before a semantically based algorithm is sophisticated enough to perform as well . and these results set a very high standard for any other approach to aim for . yet there is every reason to pursue a semantically based approach . anyone can think of examples where it fails . in these cases it not only fails it gives no indication that it has failed and offers no hope in finding the real antecedent . so in one sense since 1978 we have progress because we now have algorithms that are significantly better than the hobbs' naive algorithm . so we've passed that bar for at least a decade so that's a good news . on the other hand jerry hobbs could very viably argue that actually the second hasn't been addressed whatsoever cuz we're writing . they might have more machine learning than them but we're writing the same kind of mechanistic algorithms that usually get things right sometimes get things wrong . we don't really have any way okay so how do people do coreference . so there are different ways that people approach the coreference problem . so actually the most common way of doing it is what's referred to as mentioned pair models . and that's what we gonna look at for the end part of this class . so we try and work out all the coreference relationships by just making a sequence of pairwise links . so we're gonna take pairs of mentions and not yes or no . so we're doing binary classification decisions independently . 
binary decisions we sort of induce a kind of clustering of mentions into entities and we just do that in a simple deterministic way . we just join everything together into a lump that's been put together by these binary decisions . and we just say and they all close together by transitivity . there are a couple of other approaches that people have used for coreference resolution . rather than simply doing binary yes/no decisions another choice is to say that you can actually use a ranking algorithm . something that's gone very prominent in certain areas of machine learning that you kind of don't cover in your basic ml class is doing ranking problems . but they have come up in a lot of places . think things like netflix recommending you a movie . google recommending you a web page all of those things are ranking problems . and so you can think of coref as a ranking problem because if you have a pronoun well it should be coreferent with something . and maybe there are seven prior mentions in the document . and then you're doing a ranking task as to which one of those seven it would be . and then there's a third way of which is arguably really the right way . which is what are referred to as entity mention models . and that's just explicitly think about the entities . they're actual real entities and when you see a mention you should be saying that's a mention of a particular entity or maybe this mention introduces a new entity in to your discourse and you've got this set of underlying entities . so in some sense your entities are your clusters or mentions but you're actually giving them first class status as objects in your model rather than them just appearing as a result of some linking process . and so a number of people then tried to work on models that explicitly represent entities and then do some kind of joint inference or have some kind of generative model of how the mentions are created from the entities . but the simplest case and what we're look at mainly are these mention-pair models . and so mention-pair models are normally trained to supervise learning models . 
what you do is you have some data you have mentions . and so there's this prior problem of finding the mentions but we can roughly think of the mentions as our noun phrases . and then here's a he and what we're gonna say is that gonna well if we have gold standard data we'll know that the right answer would be either mr. obama or the president cuz both of them are coreferent . and if you have multiple choices you'd normally just choose the nearest one . and you say the correct answer for this one is the president . and then you have negative examples which are things that are not coreference . so you get positive and negative examples you train a binary classifier and you're done . so if a conventional coref people then used all sorts of features that were indicators of coreference . so for pronouns in english they're things like person number and gender agreement . that has to be mary because of gender rather than jack . there are softer notions like semantic compatibility . so if there's a reference to the mining conglomerate that could be coreferent with the company because that's sorta semantically compatible that's much harder to do . some things that we've already mentioned are hard syntactic constraints . so john bought him a new car him can't be john that'd have to be himself . so that's a feature we can use . but there are lots of softer things which i was mentioning before . that sort of sounds like it was probably jack that wasn't busy at least to me . and that's presumably a recency effect it's not really categorical it has to be . john went to a movie with jack . i think the most natural reading of that is that john was not busy so that's preferring subjects . 
parallelism john went with jack to a movie . joe went with him to a bar . i think the most natural reading of that is that that is jack that joe went with . and that seems to make sense not according to grammatical role preference which would give you john but in terms of the parallelism of the two sentences and interpreting it that way . so there are lot's linguistic features features from and put them into a classifier and try and determine coreference and people built these things where loosely they are big logistic regression classifiers with hundreds of thousands of features that try to capture some of these kind of relationships . but for the last 25 minutes what i want to tell you then is about what people have done with deep learning and coreference . and the answer to that in two words is not much . and so at this point in time there are basically four papers that have tried to use neural networks deep learning to do coreference by two authors . so there's sam wiseman at harvard who's worked on the problem in a couple of papers . and then there's kevin who's worked on the problem in a couple of papers . and so there is some sort of connections and there's some different approaches here . i mean in particular there are a couple of papers both sorta sam's second paper and kevin's first paper which we're both trying to do entity-mention models . and actually try to have explicit representations for entities and doing more global inference in terms of entities . and i think most people who have tried to do coreference a bit really do believe that surely they should be good power from doing things jointly over these entities and that should give some real advantages . in practice it's repeatedly sort of proven hard to get sustained advantages from doing that and so there's sort of been this continuing use of entity pair sorry mention-pair models which are very simple to implement and you keep on thinking to work out how to make them work well . so kevin's most recent paper is actually back to a mention-pair model and that produces great results . and i'd thought i'd actually show that one not only because it's the most recent and best but because it might be kind of a good chance to sort of show a couple of other techniques of doing things in the context of deep learning . okay so here we go so the first couple of bits may be fairly similar right . so we wanna find these coreference clusters . and so we're gonna be doing it simply as a mention-ranking model where you want to assign a score so we want to be saying what does my refer to . 
and we're picking the preceding mentions and then we add on one extra candidate cuz for any mention you have one possibility is this is a new referent in the discourse and it's not co-referent with anything that appears before it . so we then have this new up the end . so you can say this a new referent . and so for each of these mention pairs we're gonna build a model that scores them . and so it's just gonna score a pair of mentions for coreference independently still and give them a score . and then what we're gonna do is say well which one has the best score . okay that's putting i and my together . and so that one we're going to rejoin . and so then we can literally just go through the mentions in the discourse from left to right and run this mention-pair classifier on each successive mention and sort of then assign them . and that will give us our model . and so then the question is how can we go about designing and training a good mention-pair classifier . then yeah cuz at the end of the day our actual set of coreferent things will just be the result of these local decisions . so if we say i as a new thing nader as a new thing he refers to nader my refers to i she refers to my . then the result of that is we've coreferent clusters as a result of these local decisions . and as a result of imputing transitivity . so we're then saying that she is also coreferent to i . okay so i hope the setting in general is clear enough . and so for doing the neural mention-pair model this is being done as a feed-forward network . it's sort of in some sense it's no more complicated than that . but what are the parts that go into it . 
so down at the bottom we have two kinds of things . so firstly for both the mention and the candidate antecedent we have embeddings of words . and so this model didn't use any kind of recurrent neural network or something like that that goes through the mentions . i mean kevin actually experimented with that a little bit and found no particular value in it . and so it actually kind of like the dependency powers of danqis that you did in assignment two if you remember back to that one . and so it picks out particular words and uses their word representations . so it will use the head word of the mention the last word of the mention and things like that and so that gives you some word embedding features . and so the word embedding features are gonna be good for capturing similarities . i mean certainly when it's just the same word right . they both say akash you'll get that . but you hope to also get things like conglomerate and company having similar word representations that you can do things with . but there are some relationships that that's clearly not capturing . if you think of some of those properties that we've already mentioned like recency and grammatical role and things like that they're not being captured . so there are also then a few features that are calculated for each mention that are also put into it . so after that it's really a straightforward architecture . it's a deep feed-forward network of sort of relus at every level that take you up to the top and then at the top you're turning that into a score which is a numeric score of how likely it is to be coreferant . yeah so the tradition compared to traditional systems the number of handcrafted features but there are still just the number that really help . and if you have any kind of dialogue doing tracking of speakers and change of speakers also really helps you and you don't just get for free out of word embeddings . i mentioned the no rnns deep network dropout . so that part is all pretty straightforward . 
so what's a novel more interesting part of the model . you aren't necessarily gonna do well if you just train such a model as a straightforward classifier . you either got this decision right or wrong . you could do that but that's non optimal and the reason why it's non optimal is that some mistakes matter much much more than others . because even though the mention pair classifier is just an independent classifier of a pair of mentions . the reality is that as a result you're then going to end up with these clusterings of mentions that are your entities . and you have the quality of your co-reference is going to be decided by how good are those clusters mentions that your entities that you formed at the end of the day . and so what that means in particular so if you've started along saying bill clinton coreferent with he clinton coreferent with he and hillary is coreferent with clinton her . if you then said let's make these two clinton's coreferent that sort of collapses your . two partial clusterings together into one huge hair ball . and that sort of destroys your ability to kind of do discourse interpretation because you've collapsed two individuals in your discourse into one individual . but there are other ways in which you can make little errors which don't really matter . so it was raining but the car stayed dry because it was under cover . so this first it is what's referred to as the pleonastic it . that it's just not really referential at all . just somehow in english we like to have subject . so rather than just saying it's raining as you would in many other languages you say it is raining . but it's not really referring to anything so it was a mistake to make that co-reference the car but it sort of doesn't really matter . it's not gonna destroy your understanding of the dialogue you've just got sort of one thing wrong . and so the question is couldn't actually sensitive to these ideas of what a major error is and minor error is . 
and the secret of that is to say well that means that we can't work out the loss simply of individual decisions . we've gotta work out what impact those decisions have at the end of the day . and if you're in that situation in which you can't locally work out the loss of individual decisions but you have to wait around and say how does things turn out later in the day cuz i'll have to use that . that's the space in which you need to use reinforcement learning . and so that's one possibility that this paper talks about . so previously people had done and i'll show you that in just a minute which is to say gee some of these decisions are more important than others . so we could come up with heuristics to decide how important different things are . and then we could for different kinds of errors . we can kind of set hyperparameters to weight those kind of errors and adjust those to maximize our performance . and to some extent you can do that . but it seems like the more right and principled way to do it would be to say no this can be done this reinforcement learning problem . and if we too wanna be finding local decisions which lead to the end of say that our reward function for reinforcement learning is do we a get good clustering at the end of the day? . and if we do that we can get rid of having to manually find things to weight and setting the weights of them with hyper parameters . and we can get some gains not huge gains but some gains from doing that . so the thing that are being done in prior work is to say well there are different classes their importance we might want to weight differently . so the mistakes you can make is you can do a false new . you can claim that something is a new cluster when really it should have been made coreference to something . so that's failing to cluster when you should have . something should have been it's own cluster but you've joined it to an existing cluster . and then there's a wrong link where you should have been joining it with some previously established cluster and you chose a different one . 
so these notions still don't really get at sort of making a big scale mistake like the clinton mistake versus small scale mistakes . but they sort of distinguish different decisions . and some of these are worse than others . so in general doing a wrong link is worse than doing a false new . cuz a false new doesn't thave the same knock on effects that a wrong link has . yeah so what prior work had said is okay we have these four kinds of things . and done correctly or you can make these three different kinds of errors . and so what we could do is manually set weights for these different kinds of errors and adjust them to try and maximize our score . and so sam wiseman had proposed doing that in the kind of margin loss scenario . so that we are taking the maximum choice of candidate antecedents for each mentioned . and then here we have that we are looking at the score difference of the model . for the true antecedent versus this candidate . and they're both being scored by a current model . and so again we wanna adjust the scores according to our model to sort of minimize that kind of large margin loss . but we add this one extra factor to our loss function which we say is well scale how much it cost's you to make this mistake based on what kind of an error you're making . which is sort of classifying it as one of these different kinds errors . okay and so that kind of idea wasn't actually original to sam weisman . so really sort of a whole bunch of papers really kind of just about all the coref papers that were done in the 2010s had used this kind of idea . but it doesn't seem perfect firstly you have to do the hyper parameter search . and secondly those error types are a bit correlated with badness but they don't seem to be very directly correlated with badness . 
and so kevin was wanting to try and do things better than that . these are the ways he approached it and what we can say is when we are doing coreference what we're doing is we're and so each action is looking at one mention as we head through and choosing something as it to be coreferent to where one possibility is you are co-referent to new . so you're making this sequence of actions . so you are deciding what to do with i deciding what to do with nader deciding what to do with he . that these are your sequence of actions . and so what we'd like to do is chose the sequence of actions that maximizes getting a good coreference clustering at the end of the day . so how do we decide what's a good coreference clustering . well what we do is we actually just believe our metric . so i showed you the bq metric for coreference and that seemed kind of sensible it was sort of is f measure of getting your links right and precision and recall . so we call that our reward function . so if you kind if you get everything right your bq metric is 100 or a 1 depending on whether you make it a point or make it a percentage . and so we then have no loss and if you make some mistakes we can then work out the reward for different coreference algorithm . the reward will then be a lower award corresponding the b cubed score . and so kevin explored two methods of doing this . one's sort of the reinforce algorithm which is the most common reinforcement learning algorithm that's used in deep learning techniques and elsewhere . so for the reinforce algorithm what you're doing is you're defining a probability distribution over actions . and the way he was doing that was sort of taking the scores from the mentioned pair model and exponentiating those and normalizing them so sort of standard soft max function . and saying that's the probability for taking different actions . and then what you're wanting sequences with their probabilities you want to maximize your expected reward . so the reinforce algorithm maximizes your expected reward . 
so you're taking the expectation over action sequences according to this probability distribution and then working out the reward the b-cubed score having taken that action sequence . the problem is of course that there are sort of an oops the problem is that there are an exponential number of different action sequences that you can take here . and so you can't actually explore all of them . but what you can do is then sample trajectories to estimate that expectation and to approximate the gradient and you can learn according to that . so using the reinforce algorithm for reinforcement learning it basically worked . it's sort of competitive with the heuristic loss functions that people had found . but it still seemed to have a small disadvantage which is that the reinforce algorithm maximizes performance in expectation but that's not what we actually want here . we actually want to sorta maximize the highest scoring action sequence cuz that's where we're actually gonna use in practice . and so kevin explored this other idea which is to sort of say let's actually continue with this idea of incorporating rewards into the max margin objective slack rescaling . but instead of using these sort of handset hyperparameters like before what we will do is actually we sort of losses for rescaling the large margin objective . so the idea there is for our training data we can actually just look at the effect of different decisions . so since each action is independent from every other action we can change one action and see what effect it had for the reward . so this is the correct set of actions when our reward = 100 . and so we can just say for ai there suppose we made that decision differently suppose we had said ai then declared that mention to be new . well we can just say this is what our system returned what's the b-cubed score for that . and the answer is 85 and so our regret = 15 because we could have gotten the right answer and 100 . and then we could say well let's consider a different possibility . we could have put my as coreferent to he what's the b-cubed score then . and the b-cubed score is 66 and so now our regret is larger our regret = 34 . so we can actually empirically over the training data work out what the cost of different mistakes is in terms of b-cubed score . 
so then what we can do is sort of incorporate that so that now the sort of the scaling factor over our max taken as the difference between the best action we could have taken at that point . which may no longer be the perfect action because we might have previously made mistakes versus the actions that we did choose . so that the cost is then sort of the regret for taking a particular action and that replaces the heuristic cost we used previously for actually what is the actual cost of this mistake in the context of a particular sentence . okay so that was the system or the second system that was built . and so then this was evaluated on coref . so most of the recent work on coreference has used there were these conll shared task in 2011 and 2012 the coreference . it was it had english and chinese in it and these are scored with the sort of conll score . the people who did the conll competition i guess they didn't wanna take sides as to which was the best metric for co-reference so they came up with the conll score for co-reference which was actually just the arithmetic mean of three co-reference metrics b-cubed and two other ones . and so these are how things performed . so the kind of heuristic losses that people had used previously actually work quite well . so using the reinforce algorithm is a smidgen better but not really better than using the current heuristic loss functions . but what you could find is that using reward rescaling actually did work significantly sort of actually use the real losses that were incurred in different environments . now these results and differences may not look very impressive . but that's partly because even the heuristic loss is being run on a good neural coreference system . so i should also show you these other results just to give you a sense of things . so this is the sort of progress that's really been made in coreference resolution . the best chinese system was this chen & ng which got 57 on the conll score the best english system got 60 . there had been some work on non-neural systems since then so there's a better chinese system and there's also a bit better english in this system . so the wiseman was sort of the first neural system so that was actually now starting to do a lot better . and then here is kevin's two system and it sort of starting to get some decent gains beyond that going up . 
so the neural systems actually have given a nice new level of gain beyond and so i just wanted to end in the last minute it by saying well why is that . so one of the biggest gains is just the sort of general goodness of embedding . so one of the places where you get the biggest gains is what turns out to be one the hardest cases of coreference in practice is when you have these common noun nominals and you have to realize they're coreferent . and so that's things like the country's leftist rebels and the guerillas the gun the rifle 216 sailors from the uss cole the crew these are the kind of ones that are very hard to get right . if you're just doing conventional system with word features and things like that . but that's precisely the kind of place where having our word vectors actually does give us some purchase that these are still hardest cases to get right . but the other place you were getting gains is from using this reward rescaling algorithm . and the kind of interesting thing that the results actually turned out is compared to the heuristic loss function that using reward rescaling that the reward rescaling system actually made more mistake than heuristic loss function . but was cleverer at deciding where to make its mistakes and so it made mistakes that were less important . so even thought it made more mistakes overall it was able to achieve a better b-cubed score by concentrating on making less important mistakes . and that reflects the fact that is for different mistakes there is a wide variety of different costs . so this is an empirical graph of looking at all cases of false new and then this is the distribution over how much they cost you . even though as you can see there is a clear mode to this graph . so if you were doing a heuristic loss you would say okay customer false new is about 0.28 or something like that . for different situations there's an enormous distribution as to what the real cost of a false new is and that's precisely what could be captured by the reward rescaling . okay that's it for coreference and then back on thursday for doing the dynamic memory networks . to what i personally think is one of the most fun lectures now we can really just assume we have all these basic lego blocks and we can build some awesome stuff . namely dynamic neural networks for question answering . in terms of organization there's not too much going on today . except hopefully all of you are working hard on pa4 and on your projects . 
i'll have my project advice office queuestatus 90 . now why is pa4 so open ended and why do we think it's so exciting as a problem set . basically the question that i wanna ask today and maybe you can also ask yourself during your projects and your problem sets is whether we may be able to cast all nlp tasks actually as a question answering problem . and this question is what led us to invent the dynamic memory network so what do i mean by this . let's go through a couple of examples of what question answering might look like . the first one here is sort of the standard question answering type of problem where we have some inputs we have a question and we need to do some logical reasoning maybe to answer that question . so for instance here the inputs are mary walked to the bathroom sandra went to the garden daniel went back to the garden . sandra took the milk there and now in order to do this we'd have to actually do a little bit of reasoning because if you just try to retrieve the sentence or the last sentence that mentions milk . well then it will tell you sender talked milk "there" and you don't know where sandra is and so now i have to do some effort resolution . if we were to try to build sort of a hand tuned old school machine learning nlp system where we put our human knowledge into the task to answer that question . and if we were to do that then we'd realize all right let's try to find out where sandra is and then we look at this sentence and see that she is in the garden last and then we could answer the question correctly . now by the end of this class you will know a model that you don't have to give any of that kind of information to . it will actually just learn all of this from examples of this kind . now that's a standard q&a problem but you can also look at sentiment where we might have an input here . and again we have the question with the sentiment at the end so it's just the label . essentially the word pertaining to that label namely positive . we had the task of sequence tagging nicknamed entity recognition and part of speech tagging . and we can also ask what are the named entities and then we might want to obtain either a list that could include a lot of os for not the named entity . or just a list of the actual words that are named entities . and again input question-answer triplet that we would need for training that kinda model . 
same with what are the part of speech tags . every word has a part of speech tag so length as the input . and we can even go as far as and this is starting to be question of how useful it is but where we can cast also machine translation as a question answering problem right . in the end most every nlp problem has some input some questions and answer about it and some output some answers . so now with that in mind wouldn't it be amazing if we were able to build a single joint model for general question answering . can basically learn from any input question answer triplet dataset . now towards a single joint model for discompetence of qa there're two major obstacles . so lets talk about what they are . the first one is that we don't even have a single architecture or up until last year when we published a paper that solves all these there was no single architecture that consistently got state-of-the-art so for question answering we have strongly supervised memory networks . we actually used tree structured lstm so similar to the recursive neural networks that chris talked about two lectures ago . but using the ideas of having various gates as you combine words and phrases into phrase vectors of longer length . and for part of speech tagging it used to be a bidirectional lstm conditional random field another type model family that we didn't go into any details in this class . and sort of in the graphical models world . what you do notice is that all the state of the art models have some kind of neural network in them somewhere these days . that's one of the reasons we merged from 224d into 224n . felt like you really need to know these basic building blocks . but there are still different kinds of models that make different kinds of assumptions . and we would call these different kinds of architectures all right . like so the architectures that we've talked about so window-based . convolutional neural network recurrent neural networks lstms and so on . 
now the second one is that fully joined multitask learning is actually incredibly hard . and what i mean by saying that is that we don't just wanna share some parts of the model like word vectors which we're already pretty good at sharing . but we wanna have the exact same decoder or classifier . and we don't wanna just transfer between a single source and one target task but we wanna ideally train both of them jointly . so in computer vision i'll encourage you all to take 231 next quarter i think when it's getting offered on convolutional neural networks for computer vision . computer vision's actually better at able to share more of the layers as you go up the stack . whereas an nlp so far when you try to do multitask learning and share weights what we've been mostly able to do so far is to share the word vectors . and then we train the word vectors and then we initialize some other model with those pre-trained word vectors . nobody yet consistently got improvements though it's an active area of research to share deeper layers of the lstm for instance . so we trained the system on machine translation and then just changed the last layer of the soft max email send machine can analysis or something like that . it hasn't been any paper yet on showing improvements for that . and even worse it's hard to publish negative results and you'll only ever read about successful transfer learning cases because the unsuccessful ones don't get published . but when you actually do research in this area you'll notice that as soon as the tasks are not directly related they actually tend to hurt each other . so if you're trying to train two tasks together in one model say you just have two soft maxes on the same hidden state of your lstms . it turns to actually get worse in many cases too . so those are some of the two major obstacles . now the dynamic memory networks that i'll talk to you today about only tackle the first obstacle . as in their an architecture which still has hyper parameters . that might differ for different tasks but it's at least the same general modules that you have in that architecture . and basically it can tackle all these different tasks that i described to you at least in some capacity . 
and several of them actually at the time of publication . any questions around these obstacles and sharing and the idea of multitask learning . so basically we're thinking of multitask learning through the lens of seeing everything as a question over some kind of input . so let's look at the high level idea for answering these tougher kinds of questions . so imagine you had this story each of the facts is relatively simple and straightforward . john went to the hallway and things like that . but imagine i now asked you after you read it where's sandra . and you might have to actually try to retrieve the episode in which that fact or the answer to that question was actually mentioned . and so in some cases as we saw with the football you may actually have to go multiple times over the input to answer that question correctly . this is kind of what led us to this idea of dynamic memory network . this is the kind of architecture you would see a lot in papers and so we're going to walk through it first on a high level intuitively and then we'll zoom into the different areas to gain a better understanding . now the first thing you will notice is we have different modules . this is what i call sort of model components . and the reason we're basically separating them out is that i think eventually you have to do deep learning research and also general engineering . through or with the help of good software engineering practices . where you have different modules they define interfaces and you might be able to switch out one module with another one but that doesn't mean you have to change all the other modules in your larger architecture . so that's generally a good sort of modeling framework . now what does this dynamic memory network do . so it starts with having simple word vectors like glove or word2vec . and basically we'll have a recurring gru that goes over the input and just computes the hidden state at every word and at every sentence . 
so a standard gru that we have defined very carefully before . now that's just independent of the question . it will just basically compute a hidden state for every word . now it will have also a gru for the question in the question module . in fact sometimes you can share also the waits between the question and the input here these grus . that question vector q is just going to be the last hidden state of the gru after it went for every word of the question . now and this is where we'll use these attention mechanisms that you've learned about before in a very specific way . we will define it soon but we'll have this attention mechanism that is essentially triggered by the question to go over the inputs . so here the question is where's the football . and now basically we would assume that the fact that football was mentioned in this question is stored somewhere inside the hidden state of . that last time step gru and we call that q . and we use q to essentially trigger an attention mechanism over all the potential inputs . and whenever there is a strong attention being paid to a specific sentence we're going to give that sentence as input to yet another gru . and that is in the episodic memory module . now whenever we have a line like this here we basically assume that is a some kind of recurrent neural network sequence model . so basically a question triggers an attention mechanism that goes over all the hidden states of all my inputs and now basically says this fact seems relevant for this question at hand . so for instance where is the football . well the last sentence that mentions football seems relevant . and the hidden state of this gru captures that there is something mentioned about a football and the hidden state of this gru captures that there's something about football . so we restore this and now this gru agglomerates only the facts that are pertinent or relevant to the question at hand . 
so it's essentially a filtering gru that tries to only keep track of what's relevant for the current question . and now we'll define this memory state here as the last hidden state of that gru . now for the next iteration because again here john put down the football . we don't know where john is so we now have stored in this vector m that in order to answer the current question it seems pertinent and these things of course by itself . we don't manually tell it these are objects these are people or colors or anything like that but we basically now store in this vector m that john and football seem relevant to the question at hand namely where's the football . and so as we go over the input again we'll take m and to answer this question and basically pay attention to now every fact that mentions both john or the football . these are realistic numbers you basically pay a lot of attention to john move to the bedroom and john went to the hallway and that is the last sentence that seems relevant . and so the attention scores for a subsequent and previous sentences here are very low . restore this and now m is giving us input and so the zero time step to give to another g or u that then just has our standard soft max classifier at every hidden state to produce an answer . so that was a lot to digest it was probably the most complex model . we've looked at so far but all the components of this model we've already discussed but this is kind of where our research is right now . there are lot of folks who are trying to find new kinds of architectures for different kinds of problems . in this particular case we're trying to find a general architecture that should be possible or should be usable across a whole host of different kinds of tasks . so we'll zoom in in a second into these different modules but are there any questions about the general idea of the model the general architecture . so what are the two different tracks of the episodic memory module . essentially these tracks are mirrored perfectly with the input . so there are always as many time steps in the episodic memory module here as there are time steps in the input . but the model either can decide with a classifier or just goes over the input of fix number of times . and every time it goes over it it tries to basically pay attention to different kinds of sentences in the input . depending on what the question is and what it has so far agglomerated in terms of relevant facts from the previous time step or previous episode in this case . 
so here again we go over it the input the first time . store things and facts about john and the football . and then the second time we'll now pay attention to john facts too . and so intuitively again the first pass i ask where's the football . and at this state here john moves to the bedroom just doesn't seem relevant to ask the question to answer the question . cuz john moving to the bedroom has nothing to do with the football . so you have to in order to do transitive reasoning as in a to b as in b to c and c and d . now if you wanna go from a to d you need to understand that steps to get there . so this kinda transitive reasoning capability you can only get if you have multiple passes over your info . so the question is if we had some adverbs or temporal expressions and asked sort of different kinds of questions like was john there before or various other kinds of questions like that . and the answer is there's surprisingly many things it can learn if it has seen them in the training data . so this kinda model will not be able to come out of completely new kinds of reasoning and types of reasoning but if you show it a lot of temporal reasoning it will be able to answer those kinds of questions . so i don't see why it couldn't for some theoretical reason answer the questions of where was he before that or things like that . so m1 so it's m superscript 1 that's right . so the question is in the first pass we mostly use the question . it turns out we'll actually copy the question twice and in the second pass we'll replace the second copy of q with m1 but sort of a detail . but in general yes we'll only use the information of q for the first pass . once we have the second pass we'll use m1 and q to understand the attention . and again that way we hope that m is agglomerating the facts that are relevant to answer the question . so now let's zoom into the model in detail . 
we basically just have a standard gru but maybe before i define these sort of different modules in the end here we have again this softmax and so whenever you see softmax you can think of cross-entropy error as your training objective . and now because of how we define the modules we constrained this entire architecture end-to-end . you just have to give it an input sequence a question and an answer . and now from this answer here we basically will make errors in the beginning and then we'll have high cross entropy error . and then we basically backpropagate through all these vectors here through everything all the way into the word vectors and we can train this whole system end to end . and that's really where the power of deep learning and these kind of architectures comes in . so the question is should we have some attention from the answer module also sort of skipping the episodic memory module's attention and going directly to the input . and the answer is yes for the tasks that we had we did not even try that cuz we solved them to the state of the art level and slightly above . but what we actually have found is that it makes sense in the second iteration on the harder data set and namely those stanford question answering data sets that you are all gonna work on to actually have a co-attention mechanism where you want to re-interpret the question also in terms of the input . if you have a question may i cut you . then the interpretation is very different if the person asking the question is holding a knife or if somebody is standing in line in a supermarket . and so sometimes the interpretation of your question is actually different depending on the input . and so you want to have co-attention mechanisms . essentially attention is kind of a fun concept right now to do . a lot of people are sprinkling into a lot of different places and a lot of different kinds of models . and i think this would be a very fine way to do it or a place to do it . so we train everything end-to-end cross entropy errors standard stuff . now the input module is a standard gru . and again in this particular case here where we have very simple sentences and facts about each sentences we'll actually make that last hidden state of each sentence explicitly accessible when we answer the question . now one improvement that we've made on the second iteration is instead of just having a unidirectional gru we actually have a bi-directional gru on top here and then each sentence is represented via the concatenation of both the left to right and right to left direction . 
now the question also just a standard gru . we have here word vectors we call them v_t here . and basically we just get q_t and then we'll drop the subscript t for the last hidden state subsequent slides . now where it gets interesting is that episodic memory part and we've already seen some attention mechanisms and this one is slightly different . basically we'll have which i'll define in the next slide of g . g is just a single scalar number . should i pay attention to this sentence . at the ith time step or not . now the superscript here refers to the iteration or the episode the tth time that we went over the entire input . so we start here with t equals 1 and then we go up to t equals 2 . and if we go over the input five times which of course is also kinda slow we'd have here "h superscript 5." but the main idea is essentially that we have a global gate on top of a standard gru . so this global gate will basically say this entire sentence does not matter or does matter a lot . and instead of having every single hidden state have its own gate we just turn off the entire gru if a fact doesn't seem relevant . so if g_i at the ith sentence and the tth episode and pass over the entire input is 0 what we'll do is basically just entirely copy that h vector forward . no computation really necessary no updates to any of the units whatsoever . so intuitively that makes a lot of sense . if we have sentences like sandra went back to the kitchen and we're asking about the football and maybe eventually we'll figure out something about john and so on . going to other places and that's really easy to capture with this single scalar . now the last remaining question is well how do we compute that g and the main idea here is actually fairly simple and straightforward . we essentially compute this vector similarities between the sentence sector . 
that's again the hidden state of at the end of each sentence . the question back there and our memory stayed from the previous iteration and m0 here would just be initialized to be the question . so the very first iterations just have these two twice . but that way we can use the exact same mechanism in the first second and third iteration and higher . now what kinds of similarities are we measuring here . these are all element-wise and these are just how to mark products so multiplicative interactions between the sentence and question and each sentence at the memory state . and then here we just have elementwise subtraction and absolute value . so just basically two very simple similarity metrics between the three vectors that we have . is there a reason why we don't use the inner product for similarity . and in the first version of the paper we even had things like question transpose times w times a sentence for instance so this way we have a and matrix that's multiplicative but can weigh different elements of the vectors and so on . it turned out after we've done some replacement studies again something you should all do for you projects too if we remove that we had the same accuracy . and whenever you can remove something from your model you should so now once we have this feature vector here this essentially it's a vector that has all the similarities between these different sentences . we just plugged that into a standard true layer neural network . standard tanh hidden units were very familiar at this point with these equations . we have a linear they're here and then we basically put a softmax on top of that so all the attention scores sum to 1 . in some ways this could actually be a limiting factor . cuz that means that i can only pay attention to so many things very very strongly each time i go over the data set and we might not always want to do that . maybe we want to pay 100% attention to five different facts . turns out to work reasonably well for some data sets but sometimes you might also instead of having a single softmax have just sigmoids so you can pay a lot more attention and then at the very end here these two lines turn out to also be a gru but one that won't have very many time steps . it's basically a gru that goes from each of the memory states to the next memory state and agglomerates the facts that have been agglomerated over time here . 
turns out that is actually not an important parameter eventually replaced that gru standard rectified linear unit type of two layer neural network and that worked well too but first iteration of the model had a gru between these two states as well . any questions about the episodic memory module and how we agglomerate facts could you raise your hand if you feel like all these modules and how we put them together make sense to you . to the people who haven't raised their hand can you formulate any kind of question around why it doesn't make sense or what confuses you . so the question is on a very high-level we're going over the input multiple times . because every time we go over the input to different kinds of sentences . so basically intuitively here i'll just rephrase your question in the answer which is basically when i go over the inputs the sentences here s_i for the first time maybe s_i here captures . facts about john but my question here is about football . so in the very first iteration and m^{t-1} (m_0) is just initialized to the question too so it's essentially not adding anything either cuz we haven't gone over the inputs an entire time yet . so we really have these two factors . the sentence which basically in your hidden state we hope capture some fact about john having move to the hallway or somewhere but the question is just asking where the football is . so the similarity between this vector and this vector is not going to be very high and then we plug this long feature vector into this two-layer neural network but no matter what the two sentences network will not learn or be able to identify that the sentence seems relevant for the question . and so this gate g here will just be very small but then in the second iteration with basically edit one sentence that connect the john and hallway . so now in our hidden state m that had gone through this gru here so that last hidden state we define here as m now captured from the very down the football seems relevant . so now m has in its hidden state some facts about both john and the football . and now those similarities can be picked up by this attention mechanism . and basically now in the next iteration as we move over the sentence again give a higher attention score to that sentence that mentions john . that's right a sentence is just a gru or some averaging . so there are a bunch of different i will get to this in a second don't worry . there are a bunch of different kinds of things that people have tried and checked if this works and one of them is actually basic coreference . so being able like requiring to answer a question that it is he and then asking who does he refer to then finding the right person . 
and so the model can actually do this very accurately as long as it sees that . and so the way it would do that is just basically noticing how inside here you mention john or you mention he and then it learns to just go back and find the next previous kind of sentence that mentions any kind of name for instance . and it could learn more complex patterns than that too . but again it would have to have these multiple passes to be able to now go back to something that only it didn't make sense in the first time you went from left to right reading it yeah . but you would hope that the sentence vectors capture what is in the sentence . that's right so sorry to rephrase what you said i guess it wasn't quite a question but . what important here yeah as the line goes through the last hidden state of s_2 of that second sentence is the input and so it's one continuous gru . and you would hope that as it mentions john it keeps track of that in one of its hidden states . something happened about john and then as it reads in "he" and updates our hidden states we would hope that it captures something about the two being connected now . that's right so the question is has there been a study of using this exact model the answer is yes . and there are some data sets where we actually did really well on but we also had to modify the model slightly . tricks that will be a little bit outside the scope of this lecture . so there's nothing in theory of why this couldn't work at all for coreference . the main problem is that there's a lot of different kinds of patterns and you need a lot of trained data to show the model . what kind of patterns you might wanna capture for co-reference . and then the main problem in co-reference is that you might want to have an answer for every pair of words . in saying could these two words quote "refer to each other or not" . and [inaudible] so there is some issues but there is no reason of why this model in general couldn't do coref . the main tricky bit in that why you needed extra modifications is to squeeze coref into a question and answering problem which is not very intuitive . for instance let's say you have this whole sentence and they're a bunch of couple of he's and she's in there and now you ask what does he refer to . 
and you wouldn't know which he even mean from the question . so then you have to say what does he refer to . and then we'd have to give sort of a indicator function to which he were actually caring about in the input . so those are the kinds of changes you may have to make to the model to do coreference so the question is what if the input was john moves to the bedroom and the question is where did john move to . and yeah in this case it just needs to pay attention to that one sentence and can immediately agglomerate that . and then you just wanna make sure it doesn't ignore it in the next couple of iterations if you have a fixed number of passes that you have over the input . and then you can just output that bedroom . but maybe your answer is it's never seen the word bedroom before . so if they're completely new words that describe new existing kinds of relationships it would also have some trouble . in this case it probably would still work because it doesn't really care about that many things in between unless now you have certain things like john slept in the bedroom versus john went to the bedroom and now you have different kinds of questions and it needs to know what the actual action and verb is . and then if you don't have that in a trained data then it couldn't do it . but in general this kind of model struggles with these things that i mentioned which is this thing right here in the first version of this model is still a softmax . so if you've never seen a certain answer at training time the word hallway or bedroom maybe they've only went to kitchens and living rooms or something that it would never be able to give you that kind of answer . but they're now ways to extend these kinds of models with the pointer sentinel idea and generally pointers that learn to point to certain parts of your input and that's one way of fixing that problem and you'll implement pointers i think for your pa4 as well . but a lot of the other kinds of ideas are not that unreasonable . so the question is how do we compute the m vector . so the m vector is going to be defined as the last hidden state of this time sequence model which inside has a gru but also has this global gate on top of it . so for simplicity for instance if the fact is very relevant based on this attention score g then h will just be computed as a standard gru . all right so that single scalar is one then we'll just have it gru . now the last hidden state of this gru as it goes over the inputs we'll just define as m . 
but then the second pass will actually take that last hidden state and give it as input together with the previous hidden state and actually never mind yeah . let's just assume that that's your m state . there are lots of modifications that you can make to this model but they usually only change your accuracy . 1 to 2 or 3% so the simplest iteration is just going to be going through these m's independently . and then you can try various options of incorporating this last hidden state m also at the very last time stamp . but you can kind of- all right awesome . so the answer module also adjust the gru but one little trick here which is we don't just have the previous hidden state at and some word vector cuz there are no word vectors there are no inputs for the answer module . and so what we have instead is will concatenate the question vector at every single input state here as well as the previous words output . so if we have this longer sequence of things that we're generating then we have here the previous word that we generated each time . so this is similar to mission completion for instance where we give it as input every time the word we just generated in the ten step before . cool now there's a bunch of related work . many of these papers we've talked about well actually not that many some of them . so sequence to sequence learning is one such model . we didn't really cover neural turing machines in some of these other models here with somewhat over-promising titles like teaching machines to read and comprehend . i don't think any of these models are really comprehending that much or reading in the sense that people read it's similar kinds of models that have memory components . as they go over different kinds of text . the one that's most relevant and was actually developed in peril to this dynamic memory network is the memory network from jason weston and the extension to them in end to end memory networks . and so it makes sense to look a little bit at the differences between these . basically both of these kinds of models have mechanisms to compute some representations for input then scoring attention and responses . the main difference is that for the memory networks the input representations are bag of words . 
and then have some nonlinear or linear embeddings that explicitly encode the position of each word . and then the memory networks interfere on a variety of different kinds of functions both for attention and responses . so essentially each of these four components is a very different kind of network . and it's not just a sequence model and so the dynamic memory network here really takes as its core a neural sequence model . tried compared to lstms turned out lstms got the same performance but have more parameters . so we use just a gru and what's nice about this is that that will naturally capture that we have a temporality in our sequence . so if we asked did this happen before or if we want to do sequence tagging things like that we can immediately do that with this model . so we have basically much broader range of different kind of tasks that we can solve with this model . now before we get to evaluation we'll have a research highlights . happy belated international women's day and so for the recent highlight i'm going to present the paper . learning program embeddings to propagate feedback on student code by chris piech et al . if you remember your first days of programming and for some of you if you've taken one of six-eight here you'll probably remember karel . you'll know how important it is to get feedback from your teachers about what your coding to become a better programmer . but now let's imagine that you're teaching a class that has let's say an online course with a million students . how do you actually make sure that you give feedback to everyone . wouldn't it be nice if you just gave feedback or if you graded let's say about 100 assignments and you could propagate that feedback to the other students in that course . so that's kind of the motivation for creating program embeddings . yeah the idea's that you want to be able to cluster on these programs and together by like similar in some ways . so and you all know that we can do that with flagged sentences . we've seen that a lot in this class . 
and the question is can we also represent programs with vector representations such that these vectors capture something about the functionality of the code even if the code let's say crushes or doesn't really compile . so you know how to encode sentences . usually we train them on some task which requires and bottling the language . and then we can use that neural network to create embeddings and for new imports . so can we do the same thing for computer programs . and here this is what's presented see the program in the middle . it's just like to put a beeper and move and then on the left side you can see that there are two precondition states . so that means just define what precondition postcondition means . so pre-condition is a current state of karel world . for example karel is in the first square and there is no beeper to world so that could be like a state so that's a pre-conditioned and then once you execute a program we get to a post-condition . so as you can imagine for one program we can actually have a lot of different pre-conditions and then once you execute program you have different post-conditions . so here you see two samples of p_1 as one precondition once we execute the program we get to q_1 . p_k here which brings us to q_k . and you can imagine we have lots of these pre-condition post-condition pairings . and so yeah so the first step in this model is that we want to encode the state . we get a vector presentation for that state then we apply the matrix m_a which is actually the embedding that we're trying to learn . so we apply that and then we get and after from that which we are then decoding into what we predict to be the post condition . so it's in some ways similar to the approach that we see at the beginning of the class with word2vec where we are trying to learn this feature so the goal is that m_a captures something about the meaning of this particular program . and you can also see that they follow very similar structures that you've seen in class . we apply a linear function with a nonlinearity around it and also for decoding it's very similar so we use the output from our cell in the mddle and then we again apply a linear function with a nonlinearity afterwards . 
so the last function here we have a prediction loss which measures how well we can predict the post condition given our p prediction and the program . and we also have an autoencoding loss which measures how good our encoder and decoders are . so in a way like if we have get and code us and decode us we should be able to also and the last term is just regularization which you also have seen many times in this class . program we would like to actually combine or we can't necessarily train the previous model on all different kinds of programs . so we want to be able to use these building blocks and then create our representation form or complex program . and here so you've already seen recursive neural nets in this class so we can use recursive neural nets to do that . and the cool thing about programs is they're already in a tree structure so we don't even have to create a tree structure but it's already given to you . so given that tree structure we can reconstruct a recursive neural network which exactly follows the same structure . and then we combine these embeddings that we've learned from the first task together recursively until we reach the root . and the ideas that embedding like the activation at the root that's blue at the top . that representation should contain something about the meaning the logic of the entire program . and we can also train this recursive neural network because when we're combining these embeddings together we also multiply them with by training on an objective . just to summarize what this paper presented is a neural network architecture which allows us to encode programs as a mapping from precondition spaces to a postcondition space using recursive neural networks . and the advantage of using recursive is that we are also using the structure of the program itself . and once we have these learned representations we can use them for lots of different tasks . so going back to our initial motivation we can use them to cluster students by their similarity of programs . so let's say once you've waited around 100 students we can use that feedback and give it to other students as well . we can essentially to feedback prediction part and another application is to actually perform knowledge tracing over multiple code submissions . so an example of that is when you're solving your programming challenge you might actually submit multiple times . it'll be interesting to actually see your trajectories . 
so as a teacher you can oftentimes see how the student's progressing and you can see if the student actually understood what they were programming or if they just kind of randomly guessed the code . so this is actually still ongoing research by something that i've actually been working on with chris . so for example possible interventions that you might want to predict an online course . when should you give a hint to the student should you show a motivational video right now or should you maybe choose a different next exercise . and in order to do that one thing that you could do is given all these program submissions by the student you convert them into the program embeddings using the approach that we just saw . and then you can feed them into another recurring neural network and then predict future student success . yeah so and that's it if you have any questions feel free to find me after class and i would love to chat about it thanks . so now let's look at the various tasks that we can apply to dynamic memory network too . so the first one here is the babi or babi data set it's actually not a great data set in some ways because it is actually a synthetic data set . which is in some ways a big no-no for a lot of nlp people . because that was what the field had done many many years ago . we've moved past that since and actually can use real language . but i still think it was an interesting data set for a while . we've solved it to such a high degree at this point already that it's not that interesting anymore . we've sort of solved it as a community fairly quickly . but it's interesting in that it gives you a lot of necessary but not sufficient conditions for systems to be able to answer certain kinds of questions . so simple things like counting like x goes into the room y goes into the room how many people in the room . so it's very simple and if you give it enough examples it will be able to predict some number between 1 to 10 or something like that or a simple negation like john is john in the bathroom . so if it sees a lot of certain kinds of patterns it can do this . just simple sentences like john may have gone to the hallway or the bathroom . 
and this one is actually a little harder . you basically have a bunch of inputs saying the castle is north of the beach and the beach is east of the desert . now john moves west and north from the beach . is he south of the desert or something like that probably doesn't make sense but you get the idea . so those kinds of reasoning were a little hard but it turns out there's another version of this data set where you can sample 10000 examples instead of 1000 . and then most of the models can also solve positional reasoning it's just a matter of how many examples you've seen . so in many ways this was encouraging . briefly and then we've moved onto real data sets . but that use real kinds of real language . the agent motivation here is fairly simple too we basically showed a lot of examples of somebody eating or drinking something . and then you asked why did they drink something and it's because they were thirsty . and so thirsty is sort of the simple answer . so again very simple kinds of patterns . if you can't even solve that with your deep neural network model you will never get to a real question answering system either that can solve all kinds of more complex types of reasoning . what's more interesting is when we actually applied it to real sentiment . so here the question is essentially always the same you could almost ignore the question vector and just have a zero bias weights to the model . but what was cool is that model actually got the state of the art on sentiment analysis on the stanford sentiment treebank . but again sadly it's not this same exact model . and one such hyperparameter is for instance how many times do you need to go over the input before you wanna predict the final answer . and sadly you get different state of the art results depending on the tasks . 
so for sentiment the best number for fine grain sentiments so very negative negative neutral positive very positive classifications for each sentiments . the best and when you allow the model to go over the input twice . but for various of types of reasoning such as reasoning that requires three different kinds of facts . the hallway is in the house the house is in this area . is john in this area yes or no right . you now need to know and go over multiple facts . or the simple examples like john drop the football there where is john so that those kinds of transitive reasonings you can make . you can create artificially sort of transitive reasoning chains that will require multiple passes and this also shows here that in theory but in practice the model hadn't been able to perfectly pick up all the relevant over the input the first time and needed multiple times . needed to go over it multiple times . and this is assuming you don't set version of the data set that tells you this fact is important and the first time you go over the input . and then this fact is important the second time you go over the input if you do that then you can get away with three passes for these three-fact reasonings . question answer input triplets need to have 5 passes to get very high accuracy here . so why is this not a task-dependent hyperparameter . it is it is actually a task-dependent hyperparameter . and we did here based on the development splits the analysis and found that the best hyperparameter of number of passes for sentiment is 2 . so the question is in practice you would just at training time adjust these hyperparameters and for your trained data set and identify based on your developments what the best type of parameter is and that's exactly right . so at least you don't have to for a variety of different tasks think about all the different kinds of architectures that are out there . but you still have to run some hyperparameter search on what the best type of parameters are for this architecture . so why is there no result here for 5 passes . because compute time is costly and and the probability i guess our estimates of that it would magically go back up or very very low so we just didn't run the experiment . 
all right so now let's look at a couple of examples of this attention mechanism for sentiment analysis . so we now here have a couple of examples that even the dynamic memory network got wrong when we only allowed it one iteration over the inputs . and what you see here is basically a coloring scheme . and the darker the color is the larger the attention weight that g scalar is for that particular word . and so these are the kinds of examples also that you now need to get correct if you want to push the state of the art in sentiment analysis . and they're kind of interesting and fun actually . so in its ragged cheap and unassuming way the movie works . you can see it in the first pass over the input it just kind of pays attention to all the things you would sensibly pay attention to for a sentiment analysis trained neural network which is a bunch of adjectives right . ragged cheap unassuming a little bit of way and so on . now on 2 passes the model actually is not quite certain where to pay attention to . in the very first pass a little bit of cheap unassuming way somewhat oddly the and works . but in the second one it sort of now takes into consideration the context of that whole sentence and really increases the attention to the movie working and being sort of unassuming which is less negative than for instance just cheap . and now correctly classifies it as positive . like see all the words once and process that . why does the model need multiple passes . why couldn't it just do it in one pass . i guess the trouble is that basically and this is in some ways what we think is the reason and the intuitions that we used to build this model . but i can't definitively tell you that that is exactly why it cannot work . it works so it works on 50% or so of the cases it gets it perfectly right . it didn't get it right on 0 or 1 pass and basically the difference here is 0.6 . 
so in a small subset of the sentences it could only get it right on multiple passes . so intuitively here what's happening is you actually agglomerate all the facts . and now with that global viewpoint of the sentence you can now go back . and having this m vector you can now use the m vector to pay attention to every single word out there and you can realize with that that maybe some words are more important than others . so that's intuitively what you can do . if you only go from left to right once then you cannot really incorporate the global information at every time step . you can only take the information if you have a bi-directional one from the left and the right but not sort of the global picture of the sentence . the best way to hope for any chance of enjoying this film is by lowering your expectations . in the beginning here it basically pays a little bit of attention to a lot of different things . but everything pales in comparison to the second pass where it really focuses on the expectations a little bit that they're lowered and realizes this is actually negative now . and when i say realizes i'm anthropomorphizing a little bit here . the color scheme is the same inside each plot but not across different plots . so does the attention converge or does it shift again . we've noticed it's converging in a lot of cases but we also didn't run it for ten passes or something . it might eventually explode or do something crazy i don't know . but in most cases it does well but then we also notice that it actually deteriorates sometimes . so there are some cases clearly where it then also deviates again from what it should have done . i'm sort of over- again anthropomorphizing my model here a little bit but overthinking it . are the weights all summing up to one . for this model they do not no . 
it's just sigmoids at every time step . so do we share weights of how we compose information at each pass . so yeah so we have these different grus that each as we go over the inputs and it's actually a hyperparameter that we evaluate it on . and it works slightly better across several tasks to have separate weights for each time you go over the input . my hunch is that is a balance between how much training data do you have . if you have enough training data it's better to have more parameters there . and if you don't have enough training data you might wanna share each pass weights . the film starts out as competent but unremarkable and gradually grows into something of considerable power . again focuses first as competent on competent and a little bit out of everything and power and then really hones in on the power . and now the last one here i actually like a lot which is my response to the film is best described as lukewarm . so every normal sentiment algorithm would overindex just like this one on the first pass on best . cuz best if you run a simple unigram base type model get very very high certainty that best correlates very much with a positive sentence . but in the second pass it actually lowered it and our hope here is that it realized actually used here as an adverb and just best describing something . but what it's actually describing is lukewarm and then correctly classifies it as a negative . the difference here is instead of triggering the answer module only at the end of the episodic memory module we actually trigger it at every single time step . so at every single timestep you classify . so one thing we don't that's not mentioned here . is that we actually have i think two different models . and we ensemble to get to the state of the art . but really personally who cares about 0.06 improvement on a task like part-of-speech tagging . 
but it's good to see that it can very accurately also predict sequence tasks . that's sort of the main take away message here . now in the interest of time i'll skip the live demo and go to another fun fact . or a fun aspect of this model . which is we had a new researcher join our group . and so he said well there's this cool new vqa visual question answering dataset . and so he basically said i'll replace the input module with one that will give us a sequence over images image blocks . and he was also just new we had implemented at this time everything in torch . we've since moved away from torch and then now came back to it through pytorch but different story . and basically replaced the input module and checked if we can actually run visual question answering with this architecture . input question the input is now a picture an image instead of a sequence of words . so the kinds of questions you might have here is what kind of tree is in the background . in this particular dataset the answers you can get to way above state of the art if you only ever predict a single word instead of a sequence of words . because most of the answers are just single words . so instead of running a fancy gru for always a single time step . you just classify a softmax right away . and you give the inputs that you would have given to the gru directly to a single softmax layer . now how does this input module change . we won't be able to fully appreciate this figure to be honest but we can get some intuition . we've no convolution networks but we don't know this exact has a lot of bells and whistles . 
on top of it essentially intuitively what's happening here is that the convolutional network will give us a feature of vector representation for every block of the image . so in the end that every image that we get as input will chop into 14 by 14 and a grid of 14 by 14 and we have a feature vector for every one of these 14 x 14 grids . so you can actually backpropagate everything jointly as well . but as i mentioned in the beginning of multitask learning computer vision is further ahead in that sense than natural language processing because most people start their convolution work from a pre trained . convolutional network that usually is trained on imagenet . so is there yet a project that answers questions based on both images and text and the answer is there are some small datasets but no dataset that . i personally find it exciting enough tohave started working on it . it's really a dataset problem at this point until we have a very compelling dataset where you collect hundred of thousands ideally questions that you really need to have both an image and some text about the image to answer the questions from . it's tricky cuz a lot of times there's some overlap . if you take the news for instance sometimes news images . i thought about this for awhile and thought about collecting that kinda dataset . but a lot of times news images just show sort of some general thing that is barely related to the content . and so it's non-trivial to find a good data set where that's really necessary . so the question is will you be able to do this kind of model with paintings too . i mean unless they're super abstract and you have to really interpret a lot . but as long as they're realistic paintings where you actually see objects i think that as long as they're into trained data it should do reasonably well . so let's assume we have a feature vector for every single region of the image . what we're going to do is we're just in a snake like fashion lay them out and now have a sequence . and now this sequence is given to again bidirectional gru block and then the final feature vector that every time step is just a concatenation of the forward and the backward or the left to right and right to left hidden states of these by direction you use . so essentially we replaced word vectors with feature vectors from regions of images . 
now what was amazing is that literally on the second experiment we ran we got to state of the art on this dataset . and we didn't really change the episodic memory module code the question code . just the input module changed and we got state of the art on this data set and what's even more fun is looking now at this attention visualizations . again it's unsupervised the model is never given inputs on to where it should pay attention to to answer a certain kind of question so here we basically visualize this so this is kind of the equivalent plot to what i showed you about the sentiment . but the difference here is instead of being very dark instead of being very dark blue it's white . so the whiter it is the higher the attention score here is for that region of the image . and so when we ask what is the main color on the bus it actually literally pays attention to that bus and then gives the answer blue . and so we can ask what type of trees are in the background . pays attention to the background and trees . pays attention to the pink flags and gives the answer two . now in general number questions are actually not that great . so you can see here that while this is sort of the close to the best model . none of the models actually do really great on numbering because the attention mechanisms that all these different kinds of models are using are very continuous . all we're doing is as we're agglomerating facts into this gru we have high attention score and so we're agglomerating that region . but these models don't have a discrete sense like this is a discrete object and this is another discrete object . and so also if you have 50 objects it would never give you the answer 50 because it can't count in such a fine grained way and of course we have the problem that it's a classifier in the end and so if it's never seen 39 objects at training time it cannot produce the number or the answer 39 as a class . so there's actually still problems with this but in this particular data set actually are relatively small . it pays attention to the man-made structure the house or barn or whatever it is in the background here and the answer is no . and so i was still pretty skeptical in the beginning of why it's doing so well . but through these attention visualizations i actually felt much better about this model . 
it's really clearly learning something about this domain . so again sometimes you can overinterpret too much . it's not like the model actually gained an understanding of that there are two photos . and then captures is their face in the same person and both . the majority object that you would answer to a who question on this is just a baby girl so it just says girl . pay attention to the arm and then the surfboard and gives answer surfboard . this is more than just sort of learning facts from just the language itself . it really takes into consideration image and this one here's actually another fun example of that . cuz there are some baselines where people compare on just looking at the image and answering ignoring the actual question . it's like here's an image what's the answer . so it's just their certain patterns when you ask a question about this image what are they gonna ask and it's capturing that sort of baseline . in some ways even is if you just look at the question what is the bar holding in almost half of the cases also gets it right not having to look at the image at all . sometimes we glance over these tables but it's important in both your projects and that's what we'll do when we grade them . actually you've really critically questioned what's going on in those tables . so really what this model has been able to do when this data set first came out we combine the two question and images . read just only 4% or less better than just the question alone . but then this model does around 8% better than this model and around 12 or so than just looking at the question alone . where it actually took the image into consideration namely the question here is what color of the bananas . and if you just look at the question and the default answer that you'd probably be pretty good estimate would be yellow . but in this particular image the bananas that it's paying attention to are not quite ripe yet and it learns to give the answer green . 
last one here is like what's the pattern actually pays attention to the tail and says stripe . so some of them are pretty incredible . and so we then had put together a demo to play around with it and these were eight questions that we asked this demo . and i was actually kind of surprised how good it was . and of course it has to have seen these kinds of answers before . it has to have seen pictures from that domain . that's actually a journalist had asked this . and i was already coming up with excuses because the hat's sort of black and it's a black background . and then what is the girl wearing shorts . and what's interesting also when you ask what it's wearing it says shorts but when you ask what color's her skirt it actually says white . robustness in some ways to the questions . and then i ask what color's the ground and said brown and then i was like well the brown's the majority color and so i asked what color is the ball . and actually got it right despite the ball being a very small part of the image . and so eventually the way i found it i broke sort of this demo was i asked is the woman about to hit the ball . and it said yes and then i asked did the woman just hit the ball . and it said yes again and i was like all right that was the last one but again it boils down to having seen enough times of certain angles i guess of the arm and then that kind of question and so on . so i don't think it's something in theory it could never pick up but it just didn't have enough training data . so in summary i hope i could show you and motivate you and excite you for your pa-4 and the various question answering projects that you're working on . cuz in the end question answering is really one of the most interesting tasks i think in natural language processing . and a lot of the tasks that other task that we looked at in the class . 
you could incorporate i encourage you actually to think about your projects in pa-4 and extensions and like could you incorporate other kinds of tasks into your data set and then see what happens when you try to train them jointly . the dynamic memory network can quite accurately solve a whole variety of different qa tasks but as we'll talk about next week's lecture in one of them there are actually also extensions to this where you can do dynamic generation of answers and pay co-attention of the inputs and the question jointly . so still more work to be done on it . we're in week ten everyone's really excited for the end of the class . well thank you so much for the people who did turn up in person to see this last week's lecture i guess . rich and me both have one lecture each with me doing the tuesday and thursday . i suspect that there probably a few people who we won't be seeing today . who are trying to catch up on their final projects . i'm sure every one of them will watch the video and catch up on what they miss during spring break . but for those of you who are here or watching . i wanted to sort of try and say a little bit about sort of bigger picture solving needs of language . and where things might be heading or should be heading . i wanted to kind of come back and say a bit more about some of the tree structured model ideas . and how those might be reinterpreted in a different context at the high level of language . and then something that's in the same vein as that is for zhedi is then gonna be talking about this kind of interesting model out of berkeley . for learning to compose for question answering . and then for the end part of it there are kind of a couple of things that we haven't said very much about in the class . which i thought i should just say a bit more about before the class ends so we'll talk about character based models . well best of luck in finishing your assignment four or your final project . so i mean there's sort of a subtle balance . 
but in terms of what you're doing i mean you should be making sure you kind of have solid baselines and well-trained models . it's obviously good to be investigating fancy architectures . but if they're sort of not working at all or it seems like they're working very badly it's sort of always hard to work out where things are . so especially if it's a different problem well you sort of wanna know well . how much would you get if you just ran a bigram logistic regression on this . and then is there reasonable evidence that this neural model was well trained . so it's not that you have to sort of infinitely tune your hyper parameters . but we sort of hope that you have sort of reasonably solid models . if you're having any problems do try and get some help . i know there are a lot of people going to office hours today . so tonight richard is again having infinite project office hours so feel free to come by . depending you can either go home and have dinner and come back at 10 pm probably and he'll still be there . and then sort of finally for microsoft azure so i mean microsoft's really been super generous at giving us more gpu hours . and so we've got a fresh new allotment of gpu hours . and i guess this afternoon i was trying to top up various people's accounts . but in general it's kind of easiest for everybody if your account doesn't run out of credits . because then it gets cancelled and you get locked out and we have to reactivate and really if you sort of know you want to do something that takes more credits than you have . feel free to get in contact on piazza beforehand . preferably don't just email me cuz in the good case one of the tas james or nish do it rather than me . okay so a kind of interesting thing that's happened in the last couple of years . 
is that a lot of the key deep learning people have really sort of redirected their mission . with this idea they could sort of solve language . learning came out of vision and vision really dominated in deep learning . but for a couple of interesting reasons sort of the same people that were the sort of most key deep learning people . such as yann lecun and yoshua bengio really they've all considerably redirected their research programs towards language . and so here's just sort of one random sort of business mag quote from businessinsider.com . which shows how this is sort of panning out in practice . so yann lecun is quoted as saying . the role of fair facebook ai research is to advance the science and the technology of ai and do experiments that demonstrate that technology for new applications like computer vision dialogue systems virtual assistants speech recognition natural language understanding translation things like that . and the thing that's sort of really surprising about that list . is well okay the first application he mentions is computer vision . and he is a 20 year computer vision guy so that may be not too surprising . but if you keep on reading after the first application . every other application he mentions is a natural language application . dialogue systems virtual assistants speech recognition natural language understanding translation . and i think coming off of that that sort of many people who haven't been long term language people . kind of feel that language will have a kind of an image net moment . where somehow someone builds a big enough complex enough neural network . that kind of the task will just be sort of measurably solved in one step . i kind of think actually that's probably not gonna happen . 
because i kinda think there are so many different language phenomena and tasks . that there's sort of just no one task . that can really sort of have the same kind of seminal okay we've solved natural language understanding . and so i wanted to with respect to that sort of mention a couple of things of what has been lost from old nlp work . so i think there's actually kind of an interesting contrast . if you go back to the sort of good old fashioned ai work in natural language processing versus a lot of what you see more recently . and that is if you sort of look back and sort of 80s 70s natural language processing work that these people had really lofty goals . that their goal was to sort of really have human level language understanding . now actually what they could do is extremely extremely modest . but nevertheless the contrast is it sort of seems like here we are today . we have much better realities as to the kind of things we can do . but it's not always clear that everyone's reaching for the stars . as opposed to just thinking okay well we can run an lstm on this language data . and that'll sort of work reasonably well and not try to get beyond that . so i thought it might be good just to have a few slides at the beginning . sort of looking at a bit of older nlp work . and seeing if there are things that we could kind of see and learn from that . and so i chose as my example peter norvig's ph.d thesis . so probably most of you vaguely at least know who peter norvig is . because of the textbook on what is artificial intelligence and the modern approach russell and norvig's book . 
and so it's just past the 30th anniversary of peter's phd thesis which was in natural language understanding . it was called a unified theory of inference for text understanding . well what you'll find if you look in his thesis . i mean in terms of the actual language processing and language understanding what's in the thesis . i mean in some sense there's shockingly little of that . so for the work for his entire thesis there is only one piece of real natural language that's analyzed in the entire dissertation . all the rest of it is sort of toy examples like bill had a bicycle . john wanted it he gave it to him and trying to analyze stuff like that . actually a bit reminiscent of the kind of baby sentences that facebook ai research lately came out with but this is the once piece of real natural language that's analyzed in the dissertation which comes from a children's story . in a poor fishing village built on an island not far from the coast of china a young boy named chang lee lived with his widowed mother . every day little chang bravely set off with his net hoping to catch a few fish from the sea which they could sell and have a little money to buy bread . but the interesting thing is that these are sort of some of things that norvig said we should be able to get out of this text and i think many of these or at least half of these are things that we're still not very good at . there is a sea which surrounds the island is used by the villagers for fishing and forms part of the coast of china . so there's a sort of interpretive understanding of the relations that are involved . and in particular the relationship between the sea and china is quite distant in the text and sort of mediated by the existence of the coast . that's the kind of stuff chang intends to trap fish in his net which is a fishing net . so there's sort of this leap of he's set off with his net hoping to catch a few fish from the sea . so it never actually says that he's gonna catch fish with the net but any human being would interpret it that way . and again that's sort of the kind of interpretive language that we still aren't very good at dealing with . the word which refers to the fish the word they refers to chang and his mother . 
so in terms of how he's gonna do this i mean interestingly the kind of perspective in the 80s was just completely diametrically opposed to what it is these days . i mean in the 1980s it was sort of believed sort of just as an assumption that was beyond question that the only way that you were going to do any kind of natural language understanding was to have a knowledge base that you could work with and reason on . so on page four of the thesis he takes it as established as we have just seen . a suitable knowledge base is a prerequisite for making proper inferences and that was sort of just no more of an argument than look we need to make inferences between these facts to understand that the net is gonna be used to catch the fish . therefore we need a knowledge base to do it over . where it's sort of what's happened in the last two decades is to sort of show that there's actually a lot of natural language that you can understand and do without having any knowledge base and just working on fairly surface forms . so in his thesis he outlines six forms of inference and tries to build a natural language understanding system that embodies them and will be able to make inferences over that passage . and so that's working out how to connect to entities . so this example is john got a piggy bank for the reason of having money for the reason of buying a present but the example from the previous slide was he had the net with him for the reason of helping catch the fish . and so you have to sort of do this contextual elaboration . these that we've talked about and we know how to do that one and we can do that one pretty well today . the third one is related to the interpretation of metaphor and so that if you have a sentence like the red sox killed the yankees that doesn't mean that they went out murdering them with knifes and guns . it means that they defeated them convincingly . and so language is full of that kind of metaphor . but all the time we're sort of using physical metaphors which are having these sort of abstract interpretations . and so another part of his dissertation was sort of trying to work out how you derived those abstract interpretations an interesting one that a lot of the time that i think we don't think of as much important and that's concretization . and so that this is you solve goal from a general description to much more specific form of that . so if you know someone is traveling in automobile then you know that the kind of traveling that at least one of the people is doing is driving the automobile cars get a bit better . and so there's that kind of concretization that you'll need to be able to do more fine grained inferences in the state of knowledge . so in norvig's thesis the situation is essentially that they don't have syntactic parsers that's good enough that they can actually parse sentences they'd like to parse like that story . 
phran program was used where possible to pause sentences . but for some input phran was not up to the task so we constructed the representation to sentence by hand instead . well you know we're actually better than that . we can actually parse sentences pretty well now . but once you're starting to do the kind of elaborations to do the sort of things that were being talked about as other goals these are kind of things that most of nlp still hasn't gotten to and maybe should be starting to get to . so i think there's still sort of big things that we still need to do in nlp . on the one hand i mean it's just true that in the last few years there's been these exciting times and a lot of systems have gone a lot better and there's sort of this sense that bilstms with attention are sort of taking over the field of natural language processing because you can try them on any task and they work better than what people used to do and you can even use them for surprising things . you can just use a bilstm for attention . as that's gonna be your natural language parser it works surprisingly well . another thing that's really exciting that's happened is these neural methods have just clearly led to a renaissance in language generation tasks . so any task where you actually have to generate pieces of natural language where side machine translation the generation side of dialogue . that all of those feels the generation sides of things were sort of fairly moribund in the first decade of the 2000s where now it just seems like we have these fantastically good neural language models . that we can configure in different ways and we can do really exciting and nice language generation . and it's also sorta super interesting time scientifically natural language processing has just sort of been this assumption that we need to be building particular kinds of representations of language and working with them . so we want to have sort of syntactic representations . and often we want to have above those things like semantic frames to represent events and relations in language . where it's what some of the recent work such as the stuff that richard was talking about last week was showing is that it now seems that there are a lot of situations in which we can build these end to end deep learning systems that don't have any of these kind of explicit localist knowledge representations that actually work very nicely . but on the other hand there are a lot of things that we still i think have barely scratched the surface off . so one of those is we still kind of have very primitive means for building and accessing memories or knowledge . so yes there's been a lot of work with lstms where the m is memory and memory networks and other things like that . 
but really all of those are models of sort of very recent short term memory . they're not really models of can have years of experience of our lives stored in our heads . and we can flexibly sort of bring forth relevant facts at the right time that all they're doing is a sort of linear scan of what's happened in the last 100 words or something like that . another thing that's pretty poor is we don't really have much in the way of models that let us formulate goals or formulate plans . and really if you're going to do a lot of things in conversation like have a meaningful dialogue you sort of have to have some goals and plans that yes you can just be shooting the breeze . but a lot of the time you wanna accomplish things which leads to sub tasks and things like that . another area where we're still really bad is we're just sort of pretty bad at inter-sentential relationships . so once we're inside one sentence the structure is usually pretty clear and we can work with that . but once we sort of try and reason between sentences or between clauses and we're usually pretty poor at that still . those things that peter norvig was talking about right . that if we wanna do using common sense knowledge that's not really the kind of thing that we've been able to build deep learning systems to do so far . okay so that's the end of part one . so for part two of today i wanted to sort of say a bit more and then sort of talk about a bit of recent work that was done by a recent student of mine sam bowman along with jean gaultier on sort of having more efficient ways of doing tree-structured models . so for my linguistic self i still think sort of having these sort of constituent pieces that you can build representations of which gives you kind of a tree structure is roughly the right kind of model . and so up until now i'd sort of shown some examples of bits of syntax and looking inside the sentiment of clauses . here's another nice example that was some work that was done at maryland by mojita iya and his fellow students . and so what they were wanting to do is the learning models of the political ideology of pieces of language . and so pieces of language could either be mutual which is shown in gray or they could have liberal or conservative pieces of political ideology and that wasn't just done at sort of a whole paragraph or sentence level . in particular they were able to sort of build this hierarchical constituent model where you could label pieces of rhetoric or ideology as conservative or liberal inside sentences . so we have examples like this one . 
they dubbed it the death tax and created a big lie about its adverse effects on small businesses . and so the model is picking out death tax as a term of conservative ideology and its adverse effects on small businesses . but then when you're sort of putting that together interestingly it seems to have learned that putting scare quotes around death tax it then regards that as a piece of liberal ideology . to the whole sentence representation when it's they dubbed it the death tax that that's then being regarded as sort of a piece of liberal ideology of representing the opposite point of view . but taxpayers do know already that tarp so that was the recovery program beginning of the obama administration was designed in a way that allowed the same corporations who were saved by huge amounts of taxpayer money to continue to show the same arrogant traits that should've destroyed their companies . so this bit here the huge amounts of taxpayer money is being identified as conservative ideology . and then it's being embedded in this sentence which is again showing liberal ideology . yeah i cannot explain that i mean it seems like it should've been colored gray yeah . aren't always perfect they try with some inference or something . and even saved it's not very clear that's what we get out . okay so those were our kind of tree recursive neural networks . they can be empirically competitive especially if you don't have 100 million words of data to train on . but in most circles they've sort of fallen out of favor and that's because they have some big disadvantages . most often they've been used with an external parser although you can use them to parse as you go but i guess that contributes to them being prohibitively slow . and you could also think that although there's something nice about these tree-structured models you might actually wonder if are they missing out on something . cuz even though it makes sense that there's sort of this tree structure of language language does also have a linear structure . you do sort of say these strings of words that go left to right . and something that people have have tree-structured models you then have words that should be very close to each other that end up very distant from each other in any kind of tree structure . and so actually the model i'm about to talk of ends up having both tree structure and linear structure . why are the tree-structured models so badly performing . 
and essentially the reason is they're not well suited to the kind of batch computations on gpus which is really the sort of centerpiece of what's allowed sort of efficient training of large deep learning models . so if you have a sequence model a sequence model can only have one structure . you're going from left to right and above each word in turn . and because of that you can take a whole bunch of sentences preferably of similar lengths and run them through and lock step in the sequence model . and the problem is if you wanna do that with treernns that you get this input specific structure that every sentence has a different structure . and so that undermines your ability to do batched computation because you're still trying to construct different structural units in the different sentences . and what happens with gpu code if you've got a batch of sentences and each one has different structure what it does is of your 32 threads one of them is computing and every time that there's something different being done on one thread versus the other threads and so that kills all your efficiency . so sam and john and co were sort of trying to then work out well could we come up with a different form of model which is at least closer to the efficiency of a sequence model while still giving us the benefits of tree-structured representation . and that led to the shift-reduce parser-interpreter neural network or spinn model . so the base model is equivalent to a treernn but because it's better for batch computation you can't completely get rid of the fact that they're different structures . it can be sort of 25 times faster or more depending on the kind of data . and it also provide an opportunity to do a sort of a mixed linear and tree-structured model that can be used alone without a parser . so it's kind of a nice integrated model and i just wanna show you a bit of that . and the starting point of it is essentially exactly the same as the dependency parsers that we saw in assignment two and in class . so for any piece of tree structure you can describe a tree structure as uniquely as a sequence of shifts and reduces . so for this structure on the left all right you're gonna take the and cat shift on twice . then you reduce them once to put them together . you shift shift to get sat down on the stack . you reduce once you reduce the second time . and so this sequence of shifts and reduces corresponds to this tree structure . 
and every other sequence of shifting reduces well either corresponds to a different all right if you start off trying to reduce before you've got nothing on the stack you can't do it . and so what we can do is that we can build a model that's sort of acting like a transition based parser in shifting and reducing . and so it's gonna have a sequence model in the middle of it . but that sequence model is then gonna be looking at a buffer of words yet to be dealt with and maintaining a stack . and inside the stack kind of like a treernn or not really "kinda like" exactly like a treernn which will build treernn representations . so at each point our lstm model is tracking along and it's predicting which thing to do reduce or shift kind of just like the dependency parsers that you build . and so depending on what it does when things reduce you're then doing a composition operation on the stack which is reshaping the stack . and while having this tracking lstm is both a simple parser and it gives us this kind of sequence context that we can just sort of also model the sequence of words . so the essence of getting this to work well is how can you implement the stack efficiently . because if the stack was just like this and you've sort of got a stack for each time step and the stack changes . well then you'd use a vast amount of memory in the stack because you've got a different complete stack at each time step . and there would also be bad for achieving this lock step advance that will make computation efficient on a gpu . so that's sort of bad in various ways so the secret of getting it to work pretty well is to say actually for each sentence we're only gonna have one stack . sort of build up representations on the stack as we go . and so it's using an efficient kind of data structure which is similar to data structures used elsewhere . they're sometimes called zipper data structures and things like that . so the idea is like this so we wanna build up the structure of spot sat down . and so when we start shifting we have this one stack which we start shifting words on to spot sat down . and so there's only ever one stack but then on this side we maintain some backpointers . and these backpointers read from right to left is sort of telling us what's on top of the stack . 
so at the moment 3 is on top of the stack followed by 2 . so then when do a reduce operation we don't delete things off the stack we just write a new thing on to the stack . so we compute using our composition operation in the treernn style . a representation of the sat down and we simply change our backpointers and say okay now we've got 1 4 on the stack . so 4 is the top of a stack and 1 is the other thing on the stack . and so then when we again do a reduce we compute a representation of the (spot (sat down)) . the only active part of the stack . and the crucial thing is in these backpointers allow us to know what's where on the stack when we do operations . but in terms of our vectors that we're using inside our forward pass and our backward pass all of the vectors are in this array here . and so we only have sort of one array per sentence that we're sort of incrementally building up . so well i mean it clearly needs to back propagate through it right . cuz it's wanting to learn so it's going to be sort of learning a matrix representation as to how to combine these two words . and then your question is how and at that point i have to give a sort of an answer of gee this was done with supervision . so the reason you'd think it wouldn't be differentiable is you'd think okay there's a choice at different points . that's where there's a shift or reduce . and if that's a hard decision then that introduces a non-differentiability . now the way we did things in this was sort of to take it in roundabout round that . so if we train the model at training time on sentences that had parses we could compute . then there's no non-differentiability and we could compute the composition functions and we could learn to predict the actions . and so in some sense which avoids the non-differentiability . 
if you didn't wanna do that and you actually sorta wanted to say let's have this uncertainty while you're learning then you'd have to do something more complex . and that then leads into ideas like reinforcement learning or or some of the other estimators that have been tried recently like the straight through estimator where effectively decisions on the forward pass . logistic function in the backward pass to do the differentiation . but doing this kind of model compared compared to a traditional tree with recurrent neural network is actually sort of super efficient . so this sort of shows for different batch sizes . and obviously it's going well out in the batch sizes but sort of shows the general point that traditionally sort of tree recursive models and just sort of really inefficient that you don't kind of get a good batch speed up effect because you got different operations at every time whereas the dotted line is just an lstm which is super efficient . and although this model starts to curve up to in the blue and it has to curve up because you are doing different operations on the stack . it's sort of just way way more efficient out to quite large batch sizes . so sam was then working to use this model on natural language inference and i think we haven't talked much about natural language inference but i know it's come up for some of you that look at the fake news challenge and things like that cuz i and others have pointed people out of it . so the idea of the natural language inference was you have a piece of text and then you have a hypothesis following whether the hypothesis follows from the piece of text is an entailment . so for a man rides a bike on a snow covered road then a man is outside that's an entailment . so for man in an apron is shopping at a market that contradicts a man in an apron is preparing dinner or whether it's neutral which means it's neither an entailment or contradiction . so two female babies eating to two female babies are enjoying chips cuz they may or may not be enjoying them even if they're eating them . so we collected this large corpus snli of these kind of sentences and the way we constructed that was we started off with one of the vision and language databases ms coco where there is a picture and the sentence describe . a man rides a bike on a snow covered road and we wanted to sorta have something where the scene was a description of a picture cuz that meant there was sort of a concrete thing that so that would hope to avoid there be uncertainty of reference but then we collected this sort of hypothesis from turkers and the turkers weren't actually shown the photo . they were just shown the passage and then they were meant to generate sentences or entailments neutrals or contradictions and then we're trying to build systems that do that and this has been a quite good task that a whole bunch of other groups that then try to do better on . for what we were doing we're kind of interested in this idea of coming up with sentence representations by building them as recursive neural networks . and so the models we were building was using spin model rules to build representation for each sentence and then running it through comparison neural network layers to find the whole sentence meaning at the center's relationships . so we have one spin model for representation . this sentence one for that one and then we're sticking it through a neural network which is then giving a probability distribution of the three actions . 
so after having being through the spin model we have a couple of fully connected neural network layers and then you've got a softmax over the three decisions . and so here are a couple of results from this . so there's sort of some previous results the sort of things that are kind of interesting here is this is lstm rnn sequence model that has accuracy of 80.6% . some of surprisingly and somewhat disappointingly over this quite large dataset so it sort of half a million samples . the tree model actually barely does better than that so it's performance was 80.9% . something that's interesting is having both at once the sequence model and the tree structure model . that actually seems to be rather nice and give you kind of a nice gain on this model . i should mention having whole sentence representation isn't the best way to do task like the sort of snli entailment contradiction task . i mean as comes up in a whole bunch of these tasks in a lot of recent work that if you wanna do even better at these tasks you'd just do better using attention models to make alignments at the word level between the different sentences . and if you do that you can do several percent better as people have shown in recent results . but nevertheless i still do believe in the sort of tree structured idea and you can sort of see places where having the tree representations allows you to get things right that you just don't get right in the lstm model . semantic facts such as negation gymnast completes her floor exercise gymnast cannot finish her exercise . well then then tree structure model is better . it also turns out to be differentially better when you just have very long examples which the lstm gets less good at . and so we now hand over to the research highlight . so as a high-level overview the papers talk about a compositional attentional for answering questions about a variety of world representations including images and also structured knowledge bases . the first complement is a collection of neural modules that can be freely composed . the figure shows actually four modules a lookup module a relate module an and module and also a find module and the second component is a network layout predictor that assembles modules into complete deep networks tailored to each question . so our current query is what cities are in georgia . and the figure shows the network layout for that particular query . 
a layout model that chooses a layout for sentence and also an execution model that by a particular layout to a world representation . so in order to obtain from the layout model there are three steps to take . first we want to represent the input sentence as a dependency tree . so the figure shows that dependency tree for the query what cities are in georgia . the second step is to associate fragments of the dependency parse with appropriate modules . so as we can see in this figure the find module is associated with city the relate module is associated with in and the lookup module is associated with georgia . and the last step is to assemble fragment into full layouts . it should be noted that each sentence could have multiple layouts . so for our example one candidate layout is shown there . it's a tree structure with the and module as a root module . and now we will just talk about how to score the kind of layouts . so in order to score a kind of layout we need to produce an lstm representation of the question . a feature based representation of the query and pass both representations through a multilayer perceptron . and then the update to the layout scoring model at each time step is simply the gradient of the log-probability of the chosen layout scaled by the accuracy of that layout's predictions . and now just talk about the execution model . so given the layout as shown in figure b we could basically assemble the corresponding modules into a full neural network as showing in figure c . and we'll just apply it to a knowledge source as shown in figure d . and basically we should note that immediate results flow between modules until your answer is produced at the root . so in this case atlanta is produced as the root which is also the answer to our query what cities are in georgia . so essentially modules are just like small neural components that take inputs or intermediate results . 
the slide now is showing a lookup modulewhose expression is also coloured in red . and the lookup module basically just produces an attention focused entirely at the index f(i) . where you can just think of the relationship f between words and positions in the input map as some sort of string matches on database fields . and we also have a relate module . so the relate module is a softmax that directs focus from one region of the input to another . the find module is sort of it's also a softmax but it computes a distribution or an indices by concatenating the parameter argument with each position of the input feature map . and passing the concatenated to the vectors through a multilayer perceptron . so the last module is the and module . typically the and module is at the root of our dependency part tree . and for the and module is sort of similar to a set intersection but it's used for attentions . in order to train an execution model we need to maximize the sum of log probability of answer labels given a world representation over a particular layout . and so our model actually achieves on both images and also structure knowledge basis . so we'll start from looking at how it performs on a visual question answering dataset . so the model is actually able to figure out what's in the sheep's ear is actually a tag it's gonna small but it's actually a tag it's also able to figure out the color of the robe the woman is wearing . and for the third imagethe model is not able to figure out that it says the man is dragging a boardwhich is fairly close so it's pretty amazing . and in terms of the numbers so we can see the model out of all the approaches it has the highest test set accuracy on the available question answering dataset . and the model can also do like general knowledge-based type of question answering . so in this case the model is actually able to figure out what national parks and in florida and also whether key largo is an island . it actually beat every other approach by at least 3% . and i will definitely recommend you to take a look at the paper and that's it . 
okay so then for my remaining time on the stage this quarter i wanted to just sort of say a bit more about a couple of things that i think of vaguely come up at some point or another but haven't really very prominently come up . but i just so wanted to say a fraction more about pointer copying models just so that that idea's in your head . so richard sort of talked about a version of these a long time ago when he sort of told a bit about his group's work on pointer sentinel mixture models . and i just want to sort of repeat the idea of this as sort of one of the other ideas that people have been using a bit in recent neural networks . so one of the central papers is this one on pointing the unknown words . and this diagram is a bit some reason they sort of did it backwards . so the source is on the right side and the target is on the left side . but the idea here is so right that we're assuming that we've run a bidirectional lstm or something like that over the source words . and then we're in the kind of usual kind of generational lstm where what we've done is sort of we've got some hidden state . and then we go to be sorta starting to generate the next word based on it . and then how is that gonna be done . so we've had already the ideas that you can just sort of generate from the hidden state based on a softmax . and we've had the idea that you could kind of use an attention model to sort of look back at the source and coding and sort of use that as input to your softmax . and those are both good ideas but in both those cases at the end of the day you're so doing a softmax over your vocabulary based on some hidden state over here . and the suggestion that's come up is that at least in some applications and those include machine translation . another thing that might be a good idea is if you could just decide that what you wanna do is copy some word that came from the source sequence . so rather than having to have a hidden state from which you can generate it from your softmax you could just say a good idea in this position would just be to copy word 17 from the source sequence . and that might be appropriate if it's something like a name that you might have a rare name in the input or some kind of lone word or anything like that that just makes sense to copy towards the output . and so from the hidden state you're sort of having a binary logistic model which is saying to what extent do you want to generate from the vocabulary softmax versus do you want to point and copy . and so this choice here you could make it a hard choice which would have the same kind of complications for making it differentiable . 
but commonly the way it's been done is just being made as a soft choice . so with probability p you're generating for the vocabulary softmax of probability 1-p . you're kind of gonna look in to do another attention distribution over the source and then you're gonna sort of just be copying a word where you're placing attention . and so that's been kind of quite an effective mechanism to sort of just be able to just copy words for the input to the output which has sort of helped for several applications . are using it for machine translation having this kind of pointer model was giving them about three and a half bleu points of performance which is a ton right . a lot of the time in mt if you can get a bleu point you think you're doing really well . so that was sort of a very big improvement . another place where it's very effective is in summarization models cuz a lot of the time in the summarization models you are wanting to sort of copy names of people and places and things like that straight from the input to the output . interestingly in the sort of the google neural machine translation paper that came their sort of recent release of their big neural machine translation models on the live servers that they sort of state that they weren't having much success from that . so they say in principle you can train a copy model that this approach is both unreliable at scale the attention mechanism is unstable when the network is deep and copying may not always be the best strategy for rare words . so they don't seem to found successful for that method . so i guess like all things you can try and see if it works for you . okay so then the final thing i wanted to talk about was a bit about work that goes below the word level . so right from the beginning of this course where we started off with is that we had words and we were gonna want to build distributed representations for words . and that then led into word2vec and all the stuff we talked about there . and i think it's fair to say that sort of for around the sort of period of 2011 12 13 14 that's what everybody was doing and that's where we still start for this course . but in the last couple of years there's now started to be a lot of work where people are going below the word level . and so i just wanted to show a bit more about that . before doing it just sort of a couple of slides of sort of context of how languages work . so most of the time in nlp with deep learning we're sort of working with it's easily processed found data . 
and so something that you're at least sort of vaguely have in your head that human writing systems aren't all the same kind of system . so that they sort of various to their nature . so that there are many writing systems that are alphabetic and basically phonemic . which is sorta if you see what the letter is how to pronounce that . so sorta something like italian is also a fairly phonemic writing system . this example here is from an australian language wambaya . but sort of it's jiyawu ngabulu that it's sort of the sounds are just read off the letters . so that contrasts with something like english which is an alphabetic writing system that as anyone who's spent time learning english knows that the correspondence between letters and sounds is much more convoluted in english . so when you have a word like thorough it's sort of not really spelled out how it sounds that sort of this complex historical stuff going on . but then there are other language systems that sort of represent slightly bigger units . so some languages represent syllabic units or moraic units so you've kind of got a syllable like yawn or gargle something that's being represented by one letter . and then there are sort of the syllabic kind of languages . there sort of two kinds there are ones where you're having a sort of individual letter is being used still on the basis of it's sound like an alphabetic form . and then you have ideographic languages for which by far the dominant sample is chinese where you're having the individual character representing a semantic components . so that also has a pronunciation but there will be lots of characters which have the same pronunciation but have different meanings attached to them . and then you get writing systems that are sorta a mixture of these . so something like modern japanese you both have characters like these two and that one which are well moraic of either just a vowel or just a syllabic consonant or but then you also get the kind of idiographic characters that japanese borrows from chinese . so you get these sort of different forms of writing systems . and then there's this question of well do your writing systems differentiate words . so some writing systems differentiate words explicitly and some don't . 
so again chinese is a famous example of a language that does not have any word boundaries marked in the text you just get a string of characters . interestingly something that most people are less aware of is that if you actually go back to ancient greek as it was written by ancient greeks . ancient greek was a language that was written with no word segmentation . that the letters were just a continuous stream . so it's sort of putting spaces between words and ancient greek was actually something that was first done by medieval monks to make ancient greek easier to understand . so it was also a no word segmentation language . you can certainly find unsegmented latin but it started they didn't use spacers but it started to become common to sort of chisel a little dot between words . and so then there was a representation of word segmentation . okay and then when you do have word segmentation there's still sort of some variety as to how much you segment things . and so there are kind of a couple of big parameters of variation of that . so a lot of languages sort of have little words that have sort of functional meaning . and languages vary how much they separate them or keep them together . so a language like french has these sort of little clitics for the first sort of pronouns . je vous ai but they sort of the vous for you is represented with a space even though it sort of joins on phonologically whereas other languages such as arabic will sort of take the similar kinds of clitic pronouns like we and it . and sort of just glum them all together and produce a slightly larger word even though it's sort of different content words are space separated in arabic . and the other languages vary is when you have compounds . so all languages pretty much have lots of compounds like life insurance company employee . but in english we handily still have the spaces between the words in life insurance company employees whereas once you go to german that's then being written as one big unsegmented word like that . okay so that's the sorta context for then going below the word level . and so there are lots of reasons to want to go below the level so if you'd like to handle a very large open vocabulary it's kind of unappealing if you need a word vector for every word . 
and that's especially true in a lot of languages where they have a lot of different word forms that represent different derivational morphologies that's sort of relationships where you sort of have causatives or things that joined onto words so you have different forms a person number and agreement and things like that . so here's a very long check word to the worst of morphology join together . so if you have languages like that it's sort of unappealing to have word vectors for every word . another place that you find things happening everywhere over modern social meeting is people use creative spellings to express a little bit more emotion . and so then you have words like good with a lot of o's in it . which probably isn't in the vocabulary of your system . and then when you want to do you often would like to go below the word level so if you'd like to christopher into czech you might want it to know it sort of translates into something that's sort of related krystof . and that sort of makes sense if you're at the character level but not if you just have sort of these individual words . and so there's a question of how you can start to deal with some of these phenomena . so in traditional linguistics the smallest semantic units were morphemes . so big words would be divided up into their individual morphemes so if you had a word like unfortunately so if it has a root of fortune and then you add on the derivational ending ate to get fortunate . you add on another derivational ending to get unfortunate and then you add on a final derivational ending ly to turn into an adverb and you get unfortunately . so this kind of use of morphology has been very little studied in deep learning though actually mitchell and me had a paper with tom luong a few years back . or we try to use the same kind of tree structured models to build up representations of morphologically complex words . most of the time people haven't done that but have done simple things . that's something that actually has a very long history so back in the earlier age of neural networks rumelhart and mcclelland proposed a representation that they humorously called wickelphones and what wickelphones were were sort of triples of letters . and so they proposed this model in some of their language models for representing inflections and learning inflectional forms of verbs in english which was a model that at the time linguists reacted to very negatively . but it's an idea that's kind of lived on there's much more recent work from microsoft . where essentially they're sort of using the same kind of letter triples . the picture from the bottom is actually from rumelhart and mcclelland's work and so when you start off with a word they represent it internally as a set of letter triples . 
so effectively the intermediate layer encoder is sort of turning any word into just the set of letter triples that are contained inside it and so that gives kind of a flat representation without needing a sequence . the nimbleness captures most of it sort of works from there and can then generate another word . and this is sort of doing it explicitly are then given vector encoding . but in some sense the idea is sort of related to when we looked at convolutional layers briefly because they were also kind of combining together multiple letters . but it was sort of doing it after it being turned into a continuous space . rather than just separately learning continuous vector representation for each letter trigram . and it seems to have been shown that using these kind of character n-gram ideas whether like these wickelphones or using convolutions can in practice give you a lot of the gains of morphemes with perhaps less suffering okay . is that you can generate word embeddings by building them and so if you're able to do that you can then generate an embedding for any new word you see when someone sort of has some weird sequence of letters and your social media text . you can just go letter by letter and generate a word representation for it . and in a way that should work better because it has some of the gains that in general people argue for deep learning . that since you can then kind of have words with similar spellings should have similar embeddings so to the extent that they're so different morphological forms or related words by derivation . you should be able to capture that commonality into your character embeddings . and if you're using these kind of character level models then you kind of don't have any problems ever with unknown words because you can just represent them all . and so using character level models in the last couple of years has just proven to work super super successfully . if i make an admission now i mean when the idea first came up of using character level models i was really pretty skeptical as to whether it would work . so from the traditional linguistic perspective but the idea had always been well yeah we have sort of morphemes . we have those units like fortune and fortunate and unfortunate fortune and ate they have meaning . but if you just have letters like a "u" . or a "f" they don't seem to have any meaning . so it sort of seemed a little bit difficult to imagine that you can learn a vector representation for f and a vector representation for u and a vector representation for n . 
and then you can start composing them together and getting useful semantic representations for words . but what people have found is actually some of these modern models like these lstm models have sufficiently powerful composition functions that actually you can learn very good word representations . by building them up letter by letter . so here's a kind of a clean version of this . and so this was some work that was done by a bunch of people at cmu . and so in some sense you're doing the obvious thing . so for word what you're doing is you're running a bidirectional lstm over the characters of the words . and so you're learning then sort of hidden representations using the lstm above those characters . then you append the two representations at the end of those sequences and just say that's your word representation for unfortunately . and then to sort of train this whole model you're then embedding it and another lstm which is then going to give you your sequence over words for some task . so we then have a word level lstm predicting the sequence of so you sort of have these . doubly recurrent models that are nested hierarchically inside each other's and so they tested out this model for two tasks . one was just the language modeling task and one was the part of speech tagging task and it just worked super successfully . so in particular they were able to show better results for part of speech tagging than people had shown with word level neural part of speech tagging models . and so that makes sense if you're sort of say well this is because we can share the similarities between words in the character level model that makes sense . and it worked though initially as i say i was kind of surprised . cuz it actually just sort surprised me you can learn effective enough character level embeddings to be able to do this . i think that's a nice clean version of this model that works very nicely . i mean effectively different people have put character level models in in all sorts of ways . so there were slightly earlier work that was calculating convolutions over characters to generate word embeddings . 
i think this paper the character-aware neural language models came up and so this is a more recent and very complex model where they're first then they've got a highway network . and then they've got an lstm network cuz it's sort of very complex but again you can also do simple things so just going right back to the word2vec beginnings that you can sort of start off with a model that has exactly the same objective and loss function as word2vec . and just say well i'm going to at the top level train my word2vec model . but rather than storing a vector for each word and updating that i'm going to kind of like the cmu work . say i'm generating the representation for each word using a character level lstm . and then i'm feeding that into my skip gram negative sampling algorithm and that works very nicely as well . so lots of stuff of that sort and so yeah so these days kind of there's just a lot of action and use of these character level models . and sort of many people are thinking it's sort of less necessary to do word level stuff . and so for my final bits i just sort of thought i'd show you again then back to a bit of neural machine translation of a couple of the ways that people are incorporating these character level models or sub word level models . so there are sort of two trends really one weight is to sort of build neural machine translation models which have sub-word units but the same architecture . and the other way is to have sort of architectures explicitly put in characters . and so i just wanna show you so one of idea that's been quite prominent which sort of gets back more to having something like morphology is this notion that's referred to as byte pair encoding . and the name byte pair encoding is kind of a misnomer but it sort of comes from the inspiration of this algorithm . so byte pair encoding is a compression algorithm that has been developed quite separately just as a way to compress stuff . and the idea of byte pair encoding is that you're learning a code book for compression by allocating codes to common sequences . so you look for common pairs of bytes and you allocate a code book place to them . and so someone had the idea maybe we could run this algorithm but do it with character ngrams rather than byte . and so this is how it works so you stop with the vocabulary of characters . and then you replace the most frequent character ngram with the new ngram . so if our dictionary is like this we have the words low lower newest wildest . 
and they occur that often we can then say well we start with a basic character in the vocab that's them . and so now we're gonna look for the commonest character bigram and their allocated as a new thing in our vocabulary . so here the commonest character bigram is es that occurs nine times so we add that to our vocabulary . and then we look again and say est that also occurs nine times let's add that to our vocabulary . that's l o seven times add that to our vocabulary and so you keep on doing this up to some limit . so you sort of say okay the size vocabulary i wanna learn is 30000 words . and so some of the things that's in your vocabulary will actually end up as words . cuz you will have sort of vocabulary if you're doing english . but the other things that you're getting are just letters and word pieces that are kind of things that are pieces of morphology . so it's kind of an empirical way to learn a vocab . and again you have problem with unknown words cuz at the end of the day you have these sort of individual letter that are part of your vocabulary . and so you can always do and so that kind of automatically decide of vocab which is sort of no longer word base in the conventional linguistic way . when you wanna translate the piece of text you just use that vocab and you greedily chop from the left . you chop off pieces that you can find in your vocab preferring the longest ones first . so it's a very simple way to maintain a small vocab . but it was actually employed very successfully by these people at the university of edinborough . and so at the 2016 workshop on machine translation a number of the language pairs were won by the edinborough team using this byte pair encoding . and actually it turns out that again if we say gee neural machine translation system that they're actually essentially using a variant of byte pairing encoding . so they've got a slightly different criterion of when to join letter sequences together that's more probabilistic and it's essentially the same kind of byte pairing encoding idea . okay and so then the final bit that i wanted to show you is some work that thang luong and me did last year . 
which was trying to get the benefits of a character level system while also still allowing user translate at the word level . and so the hope of this was to gain the best of both worlds by having the performance of a character level system while having the efficiency of a word level system . so something i haven't mentioned so far is that even though you can get very good performance results by working directly at the character level . a problem of it is it tends to make things much much slower . so if you sort of think back to that example of saying when i said you can train a word2vec style model . but instead of having word vectors you'd have a little lstm where you build a word representation from characters . you can do that but the difference is we have to go doing standard word2vec training . you're just looking up a word's representation then computing with that . if you have to run a whole lstm to calculate the words representation that's a much more computationally expensive thing . so purely character-based models tend to take well over an order of well over and a similar kind of slow down at runtime as well because you sort of are going character by character . so the question is how can you evaluate character based models can you sort of do the equivalent of word similarity . i think there's nothing that you can really do that directly does it on the characters . but i mean you don't have to go all the way to say let's run them in neural machine translation system and see if it helps . i mean the thing that people have done . and again this has proven that character based encoding works pretty successfully is to do word so you can say okay let's build up representations of words using the character lstm and then see how good the result is as a word similarity measure using the kind of word similarity metrics that we talked about when we're doing things like word2vec . and i think the results are that it doesn't work quite as well as the best results that people have gotten from word-level models but it actually works pretty close . and so that's again where it sort of seems to having done this successfully and gives you the advantage to having this open vocab . so the question is how well do the character vectors generalize across tasks . well i think they generalize as well as the kind of word vectors that we train by something like word2vec . i mean certainly people have used them across tasks by doing different things part of speech tagging and that's worked completely fine . 
again it was a kind of a hierarchical character and word based model . so the heart of it in the middle is that there's a word-level neural machine translation system . and so it was working over a fixed moderate size vocabulary . and so that's going to have unknowns in it . so when you can sort of generate an unknown vocabulary item and you can feed that through in the generation time and encoding time you're gonna have words for which there are no word vectors which here we're maybe cute would be in your vocabulary . so if you had a word not in your word vocabulary then what you're doing is then saying well we can do that by and so there are two cases . so if a word is not in your input vocabulary you can just as a pre-processing step run a character lstm that generates a word representation for it and then you'll just say that's it's word representation and you can run it through the encoder . the situation was slightly different on the decoder side . so on the decoder side what would happen is the word-level lstm could chose to generate an unknown word . and if it generated an unknown word you then sort of took the hidden state representation and used that as a starting point of another character level lstm which could then generate a word character by character . and something that we didn't do for efficiency is you might think you'd want to take the resulting meaning here and feed that back into the next timestamp and that might have been a good idea but we were interested in trying to make this as fast as possible . so actually in this model that representation was only it was still this sort of unk that was being fed back into the next timestamp . so standard kind of word-level beam search and then you're doing a char-level beam search for when you're spilling that one out . and so let's again sort of showed the strength of character level models . so this shows some results from english to czech . and if you're wanting to do character level models czechs are really good language to look at because of the fact that has a lot of morphology and complex words forms . so here are some results from machine translation systems on english-czech for the 2015 workshop on machine translation . so the system that won the competition in 2015 it got 18.8 bleu . there had been an entry that did sort of word-level neural machine translation in wmt 2015 which didn't do quite as well . that's maybe not a condemnation of word-level neural machine translation cuz it turns out the winning system had been trained on 30 times as much data and was an ensemble of three systems . 
so maybe the word level nmt was pretty good by comparison . but nicely by putting in this sort of hybrid word and character level system that that was able to sort of boost the performance of the nmt system trained on the same amount of data as this one by 2.5 bleu points putting it well above the performance of the best system . so this then is just showing you one example of the kind of places you can win in czech . so the source sentence is her 11-year old daughter shani bart said it felt a little bit weird . and so in czech 11-year-old is turning into this one morphologically complex word . and so if you're sort of running with a medium size vocabulary in a neural mt system well you're just gonna unks full of this stuff . well one solution to that would be say hey maybe we can copy and sometimes that's useful . cuz if you can copy shani that's kinda good cuz then you just get the right word copied across but doing copying isn't a good strategy if you're copying across 11-year old . because that sort of fails badly generating what you want . this hybrid word character model you could actually get this word correct because it could actually just generate it character by character as an unknown word and give you the correct answer . tada that is my central result for the day . and so remember get help on projects . hope you have good luck finishing last lecture which is on thursday . before we tackle the limits of deep learning for natural language processing . some organizational things i know some nerves are down to the wire . so we wanna say first and foremost sorry for some of the craziness around pa4 . it's a very useful class for a lot of your guys career and girls . so it will be useful even if you might get a point here less here and there less . it's a very useful class because it is so cutting edge with that cutting edge research vibe and new models . and the class size and excitement it is very hard to make everything perfect the first time . 
so thanks a lot for all your feedback on the situation with pa4 there is a lot of internal discussion in the ta staff and between chris and me and we are trying to make it as fair as possible and help you get off the ground . the main thing that is i think straightforward and everybody's happy about it is that we'll give you a short 33-hour extension for assignment 4 as well as the final project . so the new deadline that does not use any late days is 9:00 am on sunday . and then the hard deadline that sadly we cannot push any further because we'll have to actually grade the almost 700 students' projects is 9:00 am on wednesday . we have to submit the grades just a few days later to let people graduate and all that so that is the hard deadline there's no extension . the submission instructions should be on the . ideally if you do pa4 you submit to codalab to get the official number as well . it looks like there are at least a dozen or two groups for whom it would be ideal and hopefully they will get something . all right then don't forget the poster session . it is now actually just slightly before that final deadline . but really at the poster session we wanna be able to get a sense of what your project is about . so that you have of mental time between the poster session and the very final deadline you should just be spending on writing a nice report . editing nicer looking plots and things like that . cross validation experiment to tweak your performance by 1 or 2% . so we expect not too many excuses at the poster sessions saying this is just a poster . but in nine hours it will be much much better and different . really will be looking at that poster as your main project output . so the session itself is 5% of your grade the final pa4 and the final project are 27% of your grade . i'll get back to poster session in a bit . first we thought there okay there is a couple groups really struggling hard . 
we'll give them some more helper code . it's not really starter code at this point anymore . even the modifications of the starter code were just pretty minor . then there was a huge backlash of all the students who did put in all the work to get to that baseline model themselves and that backlash seem to be larger than the excitement by the students . and so again we're trying to balance things out a lot . in general i hope you appreciate the hard work that all the ta's are doing . back when i was undergrad in germany people were just like you're 10 minutes late of your assignment submission you get zero out of your assignment . if you can't make the final project or the final deadline for the midterm or something you just take the class next year . so [laugh] hopefully we're making everybody a lot happier than those times and we're trying really to be really fair . so with that said we'll give you some starter pseudo-code that is our way of trying to make the two balance the least unhappy . really the startup pseudo-code is super simple i've given it to a couple of people who were struggling with qa and who came to my office already before . but it's something that you should all be able to implement fairly quickly at this point . and so i'll walk you through a little bit . this starter code implemented properly and tuned well the right hyper-parameters and so on should get you at least above 50% f1 . and the code is essentially you just pipe your question through an lstm you get the final hidden state of a question q . you pipe your input through an lstm . you get an output at each hidden state let's call it x_i at each word in the input . and then you just take a neural network to classify with an input the two inputs are the question vector the final hidden state and each hidden state at a certain time step x_i . and then you predict the start token and you can either use the same or probably a different one to predict the end token for each question . so something as simple as that should get you something like 50% f1 score . 
and then on top of that you can do all the bells and whistles that the tas have talked about before . you can take all the elements of the hidden states of the lstm and you do an inner product with the inputs and your compute this co-attention or context matrix and lots of other extensions . but really we hope that this is something that's possible for everybody but the groups who have already put in all the work that should not be a big surprise . and they may have some version of these probably more advanced versions than that already . all right any questions about the starter code the project . so i guess the question is any advice on should we stick to what we have or use this simple baseline . i guess it depends on where you are with your f1 . if you're much above that then you probably don't have to get back to this and you probably in your current model capture something of that sort . in general these first two steps are good steps for pretty much every model so if you haven't done that just throw that in there . these you probably have done something more advanced by now and if you get that then that's fine . sometimes there's always a fine balance and you might be really annoyed with how hard this is . but this is really also what we would like you to teach and learn about the field and sometimes it's frustrating and sometimes you're really stuck . and then learning exactly how to deal with this is actually a super valuable skill well as industrial research . sometimes it's very hard to set up your problem and know where to get started from . and so as you put these together sometimes you'll have a trade off . you can tune a baseline more and get higher or you have a not well tuned baseline and add some more complex model variance to that baseline and also get better . and so it's always a fine balance . i think the default is just make sure you have a baseline that is set up that is correct . and that kind of simple baseline should get you at least 50% . really if you tune that a lot with lots of crazy interesting dropout over recurrent nets and so on you could get up to to 60% f1 with this kind of simple model . 
now you don't need to tune it to death . sometimes you basically get sort of diminishing returns right . if you tune it a little bit you get a couple of percent improvements and then the last couple of improvements of the baseline might be harder and harder . and it might be faster for you to just implement a slightly more sophisticated model . and that's true for generally all sort of people running nlp systems . all right now one last note on the poster session before we get on to the last limits . so we will talk about the dynamic co-attention network which some of you may know now . but again everybody is expected if you can not attend you have to submit a small video and ask for an exception especially scpd students . we hope that in the block that you're not assigned you can actually walk around and i can guarantee you that there's some really exciting and interesting projects out there . and it'll be just i think fun to talk to students even if you're a little sleep deprived maybe just before . i'm sure i was in most of mine . you will have a very nice lunch lots of food . and because it's public there's a lot of excitement around this . that's kind of what i meant too of yes it's much harder this class and especially this pa4 . but it is also a lot more useful than a lot of other classes . i personally know many dozens of people who took many versions of this class before and they got job offers just because of that class and what they've done and so there will be lots of companies and representatives from those companies there will be vcs and who knows you might even get some seed funding just because you have an awesome project . so it's hopefully that will make you less upset about the struggle of the last week for this project . the limits of single task learning and natural language processing . i think so far the field of deep learning and nlp has gotten very good at taking a single dataset task and model and metric and then optimizing that setting . that's kind of what we've also gone through a lot of examples in this class . 
and thanks to these end to end trainable deep learning models the speed of these improvements has also gotten better and better over time . which is really exciting to see especially if you followed the field for a long time . however if we continue to start all these projects from random parameters which we mostly do except maybe the word vectors . word vectors are great sort of to pre-train a lot of your models . we won't ever obtain a single natural language understanding system that we can just kind of converse with and in all of its complexity . and so i personally don't think that a single unsupervised task can fix that either . in fact you'll hear some people talk about this and this is certainly a point of contention . can we have a single unsupervised task and just solve that really well and then get to some kind of better ai systems . i don't think nlp will fall into that category because largely language has actually a lot of supervision and different kinds of feedback . and requires you in the end to solve a lot of different tasks . in language if you want to have a proper language system you may have to do some sentiment understanding of what you're getting given this input . but sometimes you also have to logically reason over certain kinds of facts . and other times you have to retrieve some different facts from a database or maybe logically reason over facts in the database and do some memory retrieval . and yet again other times you have to ground whatever you're talking about in and so there are a lot of different kinds of components and if we want to have a system that understands language better and better ideally that system can incorporate lots of different things . and so in a more scientific way and the way we kind of described in a lot of tasks we have different kinds of frameworks for sequence tagging sentence level kinds of classification or two sentence kinds of classification . like understanding entailment logical entailment and things like that . and we have a lot of different kinds of sequence to sequence models . and so as i mentioned a couple of slides ago we have a bunch of obstacles and here's just a couple of very recent papers . several of which i've been involved with so i'm very excited about them . and then some also one from google . 
where basically we're trying to tackle that limit the limits that we have in natural language processing especially deep nlp . the first one is one that we actually already talked about which is we didn't have a single architecture let alone a single model . again architecture might have different hyper-parameters different weights for the different tasks that you work on . and we all ready basically talked about this dynamic memory network which could also be used for question answering . and some form of that you might even be able to use for question answering . but we all ready talked about that . so i want to talk about the next obstacle which we didn't get to last time . and that is to actually jointly learn many tasks in a single model . now fully joined multitask learning is really really hard . so basically so far when people talk about multi-task learning or many-task learning they assume there's a source task and then there's a target task . and they just kind of hope that the pre-training your neural network on the source task will improve another target task . but in my case i'd ideally have both of them be trained jointly so instead of having separate decoders for instance for different languages of different classification problems . ideally we have just a single set of a very large set of different classes we might wanna predict about a certain input text input . so if we have a sequence a sequence model and we have a question about each sequence . ideally the sequence decoder can just output different kinds of answers depending on what the question was about that input . now when people do multitask learning in many cases they also just share lower layers and train those jointly but not these higher layers . so what i mean by this in natural language processing mostly we're sharing just the word vectors . we don't share other higher lstm layers for instance across a whole host of different tasks . and computer vision is actually a little further ahead in that respect . on a very large dataset like imagenet can actually be used for a lot other tasks pretty well . 
you just change the top layer of a deep convolution neural network in computer vision and you can still get pretty good accuracy and transfer a lot of the learnings from different visual task . very convincingly in nlp and in many cases you'll only read about multitask learning in the cases where the tasks where somewhat related and hence helped each other . so we know for instance part of speech tagging helps parsing . and if it knows a certain word is a determiner then it's almost clear which word should be the dependent of the other . however what you rarely ever read about is when the tasks aren't perfectly related and good matches they don't help each other . and hence not talked about very much . and so yeah these are all the issues or at least some of the issues of why multitask learning is really hard . and i think at that perimeter of the limits of deep learning for nlp . and so this is a paper that's currently in submission that basically tries to tackle that . the title of the paper is a joint many-task model: growing a neural network for multiple nlp tasks . and the final model is actually quite a monster to be honest . it has a lot of different components . fortunately we now know pretty and hence we can talk about this very paper it's not even published yet i mean it's on arxiv . but you should be able to understand all the components of this model now . and be able to implement something very similar . so i'll go over it a little bit in a high level and then we'll zoom in to the different aspects . and feel free to ask any kind of question . so the first that we'll do is we have some kind of word vector presentations . and there are actually some clever things in this paper about n-gram vectors too which sometimes you have these unknown words you can go subword tokens chris mentioned character models are in a similar kind of idea . and then the word vectors are basically given to a series of lstms . 
all of these big blocks here are lstms . and the output of one lstm is given not the just the output from the softmax but also the hidden states of the lstms as a standard when you stack multiple lstm nodes or cells on top of one another . so you have these short circuit connections . so the first lstm here will just classify a part of speech tags at every word . the next one will classify beginnings and endings of chunks . and then this one will do dependency parsing . i'll describe how to do that with a simple lstm in a second . and then when we classify dependency parses for instance we still take as input these short circuit connections from part of speech tags to each of these higher level tasks . and then at some point new tasks and higher level tasks will require you to understand two sentences at once . so then we have a simple sort of pooling scheme similar to what we described with convolusional neural networks where we pool over time for classifying relatedness and entailment . and in the end we can train this entire beast jointly in one objective function . all right before i jump into details any questions high level . great question why do we have two of them . so this is just you can think of it if you only have tasks that require one sentence you can just have one . it's just if you want to classify how related is this sentence to the other sentence we just show two . and because that's sort of the highest level we get to to have all things in there . so the question is if the relationship is symmetric wouldn't you wanna use the same system for both sentences . so we do use the same system for both sentences . identical pieces and so is this one . it's just once you put pipe them through here that you basically take into consideration where they are . 
but you can also pool the you can pool cross these two different final representations to make sure that they're symmetric as well . all right so i think it'll become clear sort of what's going on when we zoom into the model . so again we have this character n-grams as well as standard word vectors like word2vec that we've learned about . and this first layer here is a very standard part of speech tagging lstm . at every time step we essentially just have a single layer lstm and we pipe that into softmax . and then what we also do is actually we'll compute a label embedding that essentially will allow us to take into consideration some of the uncertainty that the part of speech tagger had . the main idea you can think of this basically as another layer that takes as output the softmax . but you can also write it as this kind of convex combination here where every label that you have has associated with it a vector . and you basically sum up all these vectors in a weighted sum here and the weight depends on how certain the model was to have that label at that time step . so for instance if you have 3 different you have over 40 but let's say you had 3 different part of speech tags just adjectives nouns and verbs or something . and basically each of these 3 will have a vector associated with it say a 50 dimensional random vector it's something you'll learn as well . and you have some probabilities you think like with 0.9 this is a verb and 0.05 it's an adjective or a noun . then you multiply these 3 numbers with their respective embeddings and that will determine the label embedding for y . and so now those are the outputs of the pos tagging lstm . and so to go to the next level the chunking model will actually give as input again the word vectors directly the hidden states from the pos lstm and that label embedding . these are all the inputs and then we just plug those are just concatenated and we plug that into another lstm . and that will again do something very similar where it has as output a hidden state softmax and then a label embedding for the chunking labels . and you could in theory do this a lot . and some previous similar kinds of architectures had actually thought about putting all of these into the same layer . and we compare that and we find it works better if you have these three different tasks pos chunking and dependency parsing actually all in their own lstm layer . 
cool now on dependency parsing it's a little more complicated because in the end we wanna have a tree structure right . and so dependency parsing turns out in many cases used to require some kind of beam search . but here this model actually is incredibly simple . we again have a standard bi-linear bidirectional lstm . with now four inputs the word vectors the hidden state of the chunker and the label embeddings for pos and chunking . so these are just four inputs at every time step . and now basically we'll just run a quadratic number of classifications of just saying is this word the dependent of that word or of that word or that word . run through all of them and it would just take the maximum for each of them and we just say that's the tree . now if you think about this a little bit it might not even be a proper tree . maybe none of them said i am classified as i'm the root so i have all like the potential to classify i'm the root of the tree . or maybe two things pointed the same parent or the same child or they create loops or anything like that . so in theory this might not even create proper trees but in practice surprisingly it does in like 99% of the cases . there's a very small number of cases where this very simple feed forward architecture does not give you proper tree and you can use basically some very simple deterministic rule base systems to clean up that last less than 1% of non-proper trees and just delete certain edges or add certain like the route to the tree . and then you get a proper tree . and this actually resulted in the state submitted it but since then i think one of chris's papers . just outperformed it a little bit already again . it's never ending fun race to work on pushing state of the art on these tasks . but yeah somewhat surprising no beam search required just feed-forward computation . and you get pretty good trees most of the time . all right any questions around the dependency parsing module . 
you could do a lot more things to improve and actually add a proper beam search and go through several of the scenarios or something like that . proper sql you can do because you have these continuous vectors usually and not sql is also mostly for consistency parsing as dependency parsing and so on . but you could do a lot more clever things and slow it down . we just all of this computation is parallelizable it's super fast there's no extra infrastructure needed for any kind of tree search . all right now the last level is basically to train multiple sentences for different tasks such as semantic relatedness . and what we do here is basically have a simple temporal max pooling so that last hidden stage of this lstm is basically . just will produce a feature vector at every time step . and you will now just look at across all the feature the hidden dimensions of all the time steps where's largest value and you just pick that one . so it's kind of why we call it temporal max-pooling and you can then look at again these simple things like inner-products between those features and vector distances and so on . extract some features and pipe that into another softmax to classify both relatedness and entailment kinds of relationships . so it looks kind of complicated but really it uses all the components that we've carefully went through in class just in a clever new way . now sadly when you just say all right this is my whole model . now back propagate every time you had a softmax we use our standard cross entropy error . and you just throw that into it it doesn't quite work right away . there's one extra idea that you have to use and call this sort of successive regularization . where basically inside each mini-batch you allow the model to first focus on different tasks and then as you go higher you will regularized the weights of the lower levels to not change too much and that too much is defined by this regularization term delta here . so this is basically then one of the novelties of how to make the training more robust and actually result in the end with a final system that gets the state of the art on four out of the five tasks that we looked at . and so again intuitively here you have at the end of the first mini-batch where you focused on just part of speech tagging you have a set of weights theta that define your label embeddings and you now say when you train the next to not move too far away . from those weights that were really well tuned for part of speech tagging . and then as you go higher and higher you basically try to keep more things the same . 
but if the higher level task really wants to change a certain weight it can still do it . that's right so the question is as you train inside each mini-batch or really almost like the whole epoch you can focus first on each of the different tasks and you do that in a way that you start with the lower level tasks and then you move up through the network that's right . so each mini-batch is actually focused on a single task . so each mini-batch focuses on one task but as you go and you finish on that you go to the next task . when you go to the next task you have a soft sort of regularization or clamp on those previous slides . so that's something that could actually work for various projects to some folks had the idea of using snli or entailment classification as a pre-training step . for question answering and those are all kinds of ideas that you could try as well . so there are most of those tasks that joint training actually helps . there's a lot of complexity in getting all these numbers and the various ablation studies of using the successive regularization yes or no . there's a lot of experiments that went into this task . basically overall these two are sort of the summary of the table . when you look at all the tasks separately and they train all separately versus they're jointly trained they basically all improve . and these are all tasks that have you don't see huge improvements . relatedness here is actually the lower the better . for some of the higher level tasks on smaller data sets you also get larger improvement . so in general joint training with different tasks often helps . it helps more when you have less data per task right . if you have a simple task that is only a binary classification problem for instance or something like snli where you have is this entailment contradictory or neutral and you have hundreds of thousands of examples for each of the three labels then you can probably just get away with training everything on just that dataset . of their more complex output spaces so machine translation for instance or the smaller your data set this is the more you benefit from trying to jointly train with different having some unsupervised then maybe some semi-supervised things where you have you continue to train . and supervised word vectors together with some supervised tasks and so on . 
this result here is in parenthesis because the part of speech tagging and chunking subsets are actually sorry the dependency and chunking results actually overlapping on the dev and test set . and so obviously chunking will help a lot in the dependency part . you know that inside this chunk everything should point to one another inside the dependency tree . and so this result is a little too optimistic so we have one that just trains on these two jointly . you still have an improvement but it's less strong . so these in parenthesis numbers when you carefully look at your training in your dev and in your test sets you realize there's some overlap . any questions around the experiments over the set of this model . now these are just some more results of the various just a subset of the many people who have worked on all these different tasks and sort of the comparison . and this is generally something that i've encouraged you to do in all your proper projects but also something that you'll see in most good papers . you usually have two sets of tables . one set of tables is about to all other people's best models . and then the other subset of tables is about understanding your model better with ablations and modifications to your model and decisions that you made about your model . and so this set of tables here is basically the comparison to all the other folks who have worked on those tasks . and in many cases it's basically the state of the art model . and this is just one of the many tables of this paper that tries to understand all of the various combinations and which tasks help which other tasks . all right any questions about joint many task learning so the question is what's the key insight that made this model work to be honest so having a better word representations with character n-grams help just a little bit . in the paper you'll see how much they all help . and then having the short circuit connections helped and we have a table that shows all the deltas for having the short circuit connections . from all of the lower level tasks outputs and label embeddings directly to the higher level tasks . and then the successive regularization helped a little bit also so yeah . 
there's no single insight other than of course having this main model . so we also have a table that shows how much it helps to have three layers for all these tasks versus a three layer lstm where all the tasks are output at the same height or same depth of the network . and we show that it works better if they're actually sort of each task has its own lstm . so yeah it's a combination of those and because there's so many moving pieces there's so many over a dozen tables in the paper that show how much each helps for each of the five different tasks . no many of them we invented in this paper . and they're so the question is and this is something i brought up myself right . can you actually add some of and so the word vectors for instance are an idea that you could add to all the other models . doesn't really make sense unless you have successive layers which no one had really done for more than two tasks before . some of the model architectures and the differences are just very novel and then you have to think of what models would actually do this . the majority of papers published on these different tasks aren't extendable in that kind of way right . they're for instance graphical models where it wouldn't be obvious to just plug this vector into this other thing and something would happen . so it's hard to use these insights on a lot of these previous models . or they have convolutional operators instead of lstms and so you don't have a nice sort of at this time step i have this representation and things like that . but yeah at least the word vectors and character engrams that's a pretty general insight that a lot of people could use . now another obstacle that we also discussed already briefly before is that we don't have zero shot word predictions . and what do i mean by this . in almost all the cases the various models that we described like the machine translation models have softmax at the end . and you can only predict the words that you've seen at training time . and we've also already covered how to fix this with pointers . and you'll see now in pa4 already that we also have there not just a pointer to a single word but pointers to spans of words so beginning and end token pointers . 
and actually a interesting side note here . but you can also in this pa4 and for general question answering try to predict a sequence of single words with a set of pointers like this . it actually turns out to not work as well as pointing to the beginning learning to point to the beginning token and then the end token . that works better by 2 to 5% or so depending on how you do it than pointing to a sequence of different words . basically you make two decisions versus having to make five decisions if you point to a span of five words . all right now let's have our research highlight on neural turing machines . hi everyone today i'll be presenting on neural turing machines . one on the neural turing machine itself . and then a second paper on differential neural computers which was both of these papers were from deepmind . and we'll be seeing the architecture proposed in the first paper and then the results from the second paper . the architecture modification of the second paper is only slight and we can really just wanna take the high level idea that these architectures have introduced . so all the neural networks far excel at pattern matching . so you might have heard of deepmind's agent that played atari games such as breakout with superhuman performance . and these tests are relatively easy for to make very reactive decisions . however when it comes to reasoning from knowledge neural networks still struggle at that . consider the problem of finding the shortest path . now in our introductory algorithm classes any algorithm such as dfs or breadth-first search usually requires us to store which in the current architectures that we have seen so far it's really hard for networks to store that information . so the solution to this is having more memory . but you might be worried like lstms didn't they already have memory cells . it is a valid question but this is not the right kind so if you understand systems peak if you consider lstm's memory cell as a cache . 
and this is where neural turing machines come in . so you can okay yeah in this architecture diagram the controller is an rnn . and it decides whether to read and write from the memory cells . and we'll see how both of these operations are implemented . if you have taken previous systems classes at stanford you might have realized that memory is inherently very fundamentally discrete . so how do we make it differentiable because we need to optimize it using back propagation . and the answer to that is our friendly method of attention mechanism . which is read and write everywhere but just for different extents . and you'll see about how we go about doing that . so how does reading from memory work . so we have this memory vector and we have been provided with an attention vector corresponding to it . so in this case the first element i'm zero indexing here . the first element of the attention vector is blue which means it has high value . and so we read the first element from the memory vector itself . and it's a weighted sum so given the attention vector the reading would be different . similarly in terms of writing we have our old memory and we have a write value . we want to write everywhere but how much do we write each value in the memory by . and againwe use so you can see that the second element is blue here . and although the write value and opposition locations you can see that at the new memory . the vector has shrunk just because of different magnitudes . 
it's a convex combination of the write value as well as the attention . so in both of these cases of read and write we assumed that we had a correct attention vector . and how do we go about actually getting that . the controller has a query vector and it looks at each point in the memory and performs a dot product . and only to get which one it's more similar to . so in this diagram blue indicated high similarity and pink indicates very high dissimilarity . we perform a softmax get the memory that has the most attention . now we have the attention from the previous step . we interpolate from that attention to finally get what part of the memory vector finally we can perform the shift vector . now this is what enables us to read at different locations around that focused attention . and we then sharpen it to get our this final attention distribution is then fed into the read and write operations . we can now see your result i'm not sure if the media has been incorporated . just know that this video is from the differential neural computers which is a slightly new architecture compared but uses the overlying same principle of having an external memory bank . so in this case our task is to infer relations from a family tree . in most cases it's a graph traversal problem as well as a storage problem . and standard lstm would struggle really hard at this problem which is where neural turing machines really shine . keep in mind that the memory vector is being updated as we see for each one right . just like to acknowledge the papers and the resources i used and back to richard thanks a lot . and that is that we actually have multiple superfluous if you will word representations . so i mentioned that we share word2vec and glove kinds of pre-training vectors . 
and now if we train a output such as for machine translation or language modeling . we'll actually have another set of weights in the softmax one vector for every single word that we have in the output the softmax output . now what that means is at the top here we have this large softmax . it is the size of our vocabulary times the hidden dimensions of the lstm . and in the input we also have word vectors for every word vectors . so again the same size v times now a really cool paper and result and idea that actually came from two students who took this class or 224d last year . was to tie these two vectors together just say they have to be the exact same vectors . so your softmax weights for every word are the exact same as your input word vectors . and you train them both jointly you just back propagate . take the same derivatives but now they are actually the same . it's very easy to implement if you don't have to take the derivatives yourself and tensor flow and so on . they also have some really nice theory about the softmax and various sort of temperatures when you do this . but we're not gonna go into all those details . but basically it's a very simple idea and it turns out to quite significantly help . so here we basically have again this language modeling task over the penn treebank . we mentioned that at this pointer sentinel idea got 70.9 . and then these very very large sort of 38 different lstms . for instance with 2.5 billion parameters i get 68 . but this simple idea where we basically tie the word vectors together with just 51 million parameters . i can get the lowest test perplexity when that paper came out . 
which is kind of incredible like the amount again the speed in which this perplexity has now been reduced more and more . and we're better and better able to predict this next word is kind of incredible . so it's a very simple idea that you can actually use every time you have an output space that includes all the words in your vocabulary . as well as word vectors you can use this idea and one you're reducing one of the largest sets of parameters in your model . so you use less ram you can have larger mini batches or you can train faster use less gpu ram and everything . and it's more statistically efficient whenever you see a word . and the output it benefits also its input representation . so very neat idea very simple any questions about this idea . it's one of those nice examples where everybody kind of assumes you just have a softmax and you have word vectors . so nobody really thinks about it and then sometimes people think about it . and question some of the basic find a way to do a better job so . one of the best projects from that class . now obstacle 5 is something that's very relevant to pa4 . so we spend a little bit more time on it but basically tackles the problem that in many cases questions that we might ask a system have representations that are independent of the current context or so a kind of fun example is the question may i cut you should be interpreted very differently if i am holding a knife or whether you're standing in line right . and so you might want to have your question be reinterpreted given the context and the input and the reason i brought up the dynamic memory network is that this is in some ways a further refinement of this kind of idea . you will still have some kind of document encoder you'll have some kind of question encoders you 'll have an answer module but this answer module actually predict indices of the answers . and then you have this coattention encoder instead of this episodic memory module you have seen before and now this coattention encode looks kind of complicated . and is a little bit complicated in real life but not too badly so so let's walk a little bit through it and the paper gives you all the equations . and this is a reasonable model to try to implement again once you to have your baselines implemented and bug free . you can really actually in many ways start from just this first step here similar to the pseudocode i gave you . 
and then several of these modules you can actually add one by one and see for each one how much it improves . the first author of this paper that's exactly what he did . he looked at it looked at errors and then tried to add more coattention . incorporate all the facts again for multiple time steps and things like that . so there's kind of a hill climbing on the architecture kind of approach . so and on a very high level let's say you have a question queue here . and you have the hidden states of an lstm . and you have some document input d here . and you have m + 1 steps here . you actually have this sentinels too that's why it's +1 . now what you can do is essentially take the inner products between all these hidden states . and that's how you get these sort of context matrices and then you can multiply these again with the hidden states in these products . and you can concatenate various combinations of these products between these two sets of vectors . so you have these outer products compute these context vectors and then you concatenate them in multiple ways until you have the final state here . and now that one you'll pipe each input here you pipe it through a bidirectional lstm again . and that will now be the question dependent interpretation of every word in your input document . so basically just a lot of inter products and outer products between the hidden states of two lstms . such that you understand how related is this time step of this question at this word of the question to this time step at that word at the input document . lots of inner and outer products and then you try to agglomerate all these different facts again in the bidirectional lstm . and now once you have an output a hidden state of that lstm that will be given as input to a classifier that essentially tries to identify . 
and classify with these highway networks basically just neural networks with short circuit connections . at each location of this now question dependent input representation you classified which of these is the start token . and that start token is then given its input to yet another neural network that will now take the previous start token that we classified together with a potential end token across all these different vectors . you hear from the question dependent input representation to classify the output . and you can do that multiple times and once they don't change the input and the start and the end tokens are the same from the previous time step you'll basically stop . so the reason we call this dynamic here is that you do this multiple times and your first iteration might be wrong . but you give that input so the argmax is the highest resulting hidden state . this could be the 51st time step for instance the word turbine . you give that as input to this lstm which was then given again its output given to the input of another iteration of this attempt at predicting the start and end token . now in a simpler world let's say when you eventually get to this model but you might implement the whole thing and you might be very optimistic just implement the whole thing and then it doesn't work . well you just take out all the different things and you try to do the simplest thing which starts exactly at that pseudo code i had in the very beginning of the class . you just have lstm for input lstm for question . and then you pipe each state of the input into a neural network and you try to classify start and end token . and you might have some outer products between them and you plug those into a straight up neural network and you classify start and end token . then you might concatenate these two outer products and just classify those start end token . if you eventually have that whole coattention encoder you could then say all right now i just classify independently the start and the end tokens from that representation of the question dependent encorder . just independent one classifier for the start token one classifier for the end token . and each time it will take some time and you run some experiment but as long as you sort of incrementally improve each step you know that you didn't introduce a bug . and so whenever there is sort of general bug fixing you wanna have you wanna try to identify where your bugs might be as you build the larger and larger system . and so if you start from something simple that you know works reasonable well and is bug free then each time you add something to it . 
and it improves the accuracy you can be fairly certain that there's no new part not always but for the most part . and so this is a you know in the end a very complex system that puts a lot of we actually again have sort of introduced all of the basic components basically of this but there again sort of put together in a very novel way . and you already know the stanford question answering dataset unless of course you're doing a project that has nothing to do with pa 4 . so i'll just describe it a little bit briefly sorry for the folks who are doing pa 4 and are intricately familiar with this already . so the stanford question answering dataset is a really great dataset of 100000 plus question answer input triplets . and the way it's constructed is that for each question the answer has to be a particular span in the input paragraph for the most part . sort of short documents but really mostly paragraphs . so when you ask what is donald davis credited with what's great also is they actually have multiple people answering the same question so one ground truth answer might be davis is credited with coining the modern name packet switching and inspiring numerous packet switching networks in europe . another person might just say he's credited with just coining the modern name packet switching and inspiring numerous packet switching networks or even shorter just coining the modern name packet switching . and we would assume that all of them are reasonably correct and close enough and if your model predicts one that it's good enough . great data set now again these whenever you put a results table in it's already deprecated actually one thing that was really great to see i just noticed today . is the model now this is the squad website . again sorry to bore the folks who are working on pa4 and not on the problem set . it's a really great new phenomenon that i think we'll see also as we push the limits of not just deep learning for nlp but i think in general of machine learning and ai . so have proper trained dev test splits nobody sees the test set . you have to submit your code so that makes it more reproducible in the future too if people are willing to open source their codes of course you don't have to here . and it's i think in general a great way to improve the science of what is mostly an engineering discipline we're creating new systems and so you see here different systems and now you also can see when they were submitted . now there's kind of this is my group submit it . this is when that paper came out and so since the last four months we worked on other things of the art anymore . and there are lots of people who are just this week submitted more but at the time of submission this was kind of this dynamic co-attention network was the best model on squad the first one sort of push it above 80 . 
what's also great is to actually have human baseline and that is something that will make sense for you too sometimes . and i have had several students groups also and in their problem set work on a task and then they say i look at my errors now which is great always do careful error analysis something we would definitely want to see in your report in the posters . when does your model fail what can it not capture yet . and sometimes you look at your errors and you actually say the official ground truth label is actually kind of wrong . there's also just people they were busy they had to make money on amt or something crowd workers right . maybe they weren't properly filtered and so on . and eventually you might hit an upper limit of just what that data set can ever give you . and so it's good to have this kind of human baseline . here the human baseline is sort of 91 in terms of f1 or the exact match of 82 . and you know once you push above that really you're just fitting to the noise of that data set in some sense . and so that is good if you're at that level and it also helps to feel less bad if you have a new data set you created it yourself . it's good to know that it's okay to be at 85 because if i ask two people they would only agree in 85% of the cases . so this inter-annotator to consider as your pushing your numbers sort of higher and higher . any questions on squads the dynamic content neural network yeah . i don't actually know all the details of who they asked it may have been just the first author . it's the turkers and their interannotator agreement . so maybe okay so then if that's the case then basically you can look at how often do these training set . so how often do people actually agree with when they write their answers . so here there's perfect agreement between the humans but here it might not be . so one might say what did the church claim could be avoided with money . 
god's punishment for sin or or late medieval catholic church versus just a catholic church . so they're different sometimes different people agree differently . and it doesn't make sense for your model to try to agree more with any single human than humans between one another . how do you say its performance exceeds human performance . so you can try to do that by basically saying all right humans agree this often with other humans . you can create an output that other humans would be more likely to agree with than with one another that's one way . or you say i will take five or ten experts in the world about a certain thing . this actually becomes more important for like medical diagnosis when you wanna also make those kinds of claims or just train really accurate algorithms . you could basically take a group of experts and you only select those where the majority of the experts agree on what the output should be . and if you then agree more often with the majority than any single doctor would agree with that majority then you can claim super human accuracy . so what are the principles behind sort of claiming a novel algorithm . so i guess in some ways it's kind of out of the scope of the question cuz it's a legal question . i think in general novelty of algorithms is something that is also in the eyes of the reader so that's not really a good scientific answer to the question . no i guess in general a lot of these papers are submitted to conferences and so the question whether they're novel enough kind of often is subjective and in the eyes of the reviewer . which can also not always be the right thing because two or three reviewers can also be wrong . so then here's a nice visualization again something i would encourage you all to do for your projects and problem sets . i would just basically in this case trying to understand if this dynamic encoder having an extra lstm layer on top of just predicting a single start and end token once will actually help . and here we can kind of see it helping . so as you go through this it's kind of hard to read but basically this is an input and then you see the outputs of the classifier of this highway network . and how certain it is that a certain word is a start token so 66 end token 66 with just a single word as a start token versus having the start token be 84 and the end token be 94 . 
and actually it switches from the first attempt at classifying the right span to the second and in this case more correct . all right now the second to last obstacle one thing you've noticed in a lot of things and in a lot of these more complex models is that we actually use recurrent neural networks as the basic building block for a lot of the different deep learning nlp systems that we have . and sadly those recurrent neural network blocks are usually quite slow . and unlike convolutional neural networks they can't be parallelized as easily . and so the idea here is to basically take the best and parallelizable parts from rnns and convolutional neural networks respectively . and try to combine them in one model and this resulted in the quasi-recurrent neural network by james stephen and caiming and me . and this is essentially the description of this quasi-recurrent neural network . so in general the very first layer of an lstm through the single-word vector you might be able to parallelize . but then as soon as you actually take into consideration the previous time step ht-1 in your lstm cell you have to wait until that's computed before you can compute your new one . on the other hand in the convolutional neural network you can parallelize the convolution really well because it only depends on two consecutive inputs . but then with the max pulling you don't actually get a hidden state at every time step . but for many things like sequence classification or identifying spans and things like that we would actually like to have such a hidden representation at every time step . and so the idea on a high level of the qrnn is to have a parallelizable convolutional layer . and then have a parallelizable element-wise pooling layer at each feature i mentioned and computes these gates that we already know . so in some ways in combines the cnn that we looked at with the gated and lstm type gates . and so we can write this as a very simple description right . this is something that should look familiar to you . but instead of having xht- 1 here you just have xt- 1 and xt . so you don't have to wait until you computed the previous hidden time step . you're just making these gating decisions based on two consecutive word vectors . 
and you have multiple layers of these so this is just the first layer here . and you basically just have a standard neural network . it's not recurrent just concatenating two input vectors at a time . you have tanh or sigmoids depending on what kind of gates you have . so now this you can rewrite as a convolutional operator where you have a set of weights wz over your input x . you just basically multiply it's a pretty discreet computation . once you write it like this you can also think of larger filter sizes or windows . you could actually have xt- 2 xt- 1 and xt in each time step for instance . does this make sense as an operator . compute the gates at each time step . so the question is you're splitting a cell and then you're parallelizing across each dimensions . so why's this parallel and why can we parallelize this . let's say you have these five word vectors here x1 x2 x3 x4 and x5 . now at each time step what you do is you basically take two as input . take these two as input and you compute a vector such as z all right . and now you do this basically for all these for all these pairs . now the reason we can parallelize this is basically because we can concatenate a large matrix that just has x1 x2 and then x2 x3 and x3 x4 and so on . and we can basically preprocess our input in this format and then just multiply that same matrix with all of those . and hence we can parallelize all of them . none of these computations depend so that's why this can be parallelize basically just smartly preprocessing the input . 
and then the element-wise gate here can also be parallelized across channels . so all of these are just elements-wise multiplications of these gates and of the hidden states . and so you just multiply let's say you have 100 features 100 of these computations over time can be done independently of one another . so ht here for the first dimension of my feature channel can be multiplied independently of h2 of the feature channel and h3 and so on . the ith element of the feature channel is independent of all the non-i feature channels . parallelize this part across time but this only across feature channels . so here you have the ith element of ht depends only on the ith element of f h and z . but now you parallelize differently you parallelize across the feature channels not across time . so you basically parallelize here parallelize this . then once you have all of these you parallelize this again and you can parallelize this again . but you have to wait you can't compute the third layer before you compute the first and the second . so what's great about this is it turns out to sometimes actually be better than lstm for a couple of parameters and settings and tasks that we ran . and it's certainly a lot faster especially once you're implemented properly with cudnn kernels and cuda kernels and really dig in . if you just kind of multiply it in python you might not be able to optimize the architecture as well . and you won't get these kinds of speed ups . so depending on your batch size each of your mini-batches and depending on the sequence lengths you can get up to sort of 16x speed ups . but if you have very large batch sizes and very short sequences then of course that parallelization will buy you less and you'll only get a 1.4 speed up or so . when you look at how much of the computation for this kind of model is now spent on what kind of operation . what's amazing is basically if the q-rnn the recurrent types of multiplications and just computation is actually very small now . we have large vocabulary in our softmax . 
the majority of time here is spent on the softmax classification only . and then there's a little bit of just optimization overhead getting things onto the gpu reading and getting the word vectors and all of that stuff . sometimes they're also sort of easier to interpret cuz we have this now independent feature dimensions . and so it can actually go into this demo okay can't see it where we re-visualize this . and this is also you don't have to do this but if you have extra time you already have good performance on your models interactive plots to interact . if you have your question you can write a little javascript or some maybe even just command line thing . type in a question and see what the answer is give it a new kind of input a new kind of document . see if you can break it how it breaks or what the activations look like and things like that . so here we basically had a qrnn trained on sentiment analysis . and then looked at the activations as it goes over a very large and very long document . so here you can see what is there to say about this movie this movie is simply gorgeous . so once you hit gorgeous you see that a lot of the different activations for several of the neurons really strongly change in their activation so right at this location here . simply gorgeous a true feast for the eyes . and now some of these hidden activations stay the same no matter what other sort of content there is . so game set standard for 3d role playing games seven years ago this movie sets the standard for future cg movies . and then you've got these trailers blah blah blah . so this is kind of this idea that i mentioned in the very beginning about having these different gates and now this is for sentiment and nothing changes much . just kind of talks about content but then when the movie does not disappoint some of the neurons will switch again . and then there's another sort of seemingly pretty important change in this review so that doesn't sound super positive so a lot of these neurons again will switch around . and then at the end here i recommend this movie to everyone and you see okay several of the neurons again turning on very strongly and eventually classified this as a positive review . 
so those are super nice to have if you can try to visualize your model that way . and now in the last five minutes i want to talk about very recent paper from quoc le also graduate from stanford now in the google brain team actually a founding member of it . working with somebody else zoph here where basically they realized and this is very good insight more and more of our time as researchers is spent on creating complex it would be better if we could actually have an ai select the right and again putting more a back into ai and human architecture design . so in some ways and this is kind of an introspective sort of end thought also for the class . the beginning where we said we do all this feature engineering back in the day in the field . and now look everything is much better cause now we have these architectures that end to end trainable and they learn all these features . but as were now trying to improve numbers more and more and performance and create new kinds of capabilities for we catch ourselves designing architectures more and more just like we used to design features anymore . we're humans we want to use our intelligence in some positive way . and so basically the idea here is to use artificial intelligence to find the right architecture for a whole host of different kinds of problems or for a very specific problem . and without going into too many details the main basic controller that we'll have is also going to be a recurrent neural network . but the outputs of that recurrent neural network are actually architecture hyper parameters if you will . so how many hidden layers should i have or how big should my hidden layer be . at each time step of i will output those kinds of features . and then whenever it outputs that it will try to train a child network with that kind of architecture to get a certain accuracy on a single task . now it's very hard to actually as you make these various discreet kinds of decisions about the whole architecture so they use reinforcement learning which we haven't really covered in class . and in the last two minutes of class we're not going to able to really do it any justice so it's just a different learning regime than the standard back propagation . this is kind of we trained the cnn what these outputs would look like at one time step . it might predict a number of filters and then the filter height and filter width and the stride size how far do you skip to either the next words . or in computer vision how many pixels do you jump over and so on . so basically this kind of architecture selects its own architecture for specific problem that you say is its reward . 
and remember when i told you the numbers are getting better and better this is again this data set on language modeling where just a couple months ago we're super excited to have these pointers and when we got to 70 . and then we're super excited cuz we're tied to word vectors and we got to 66 or a 68 . now with this incredible new idea to actually human ingenuity is still important they also share the embedding . so it's sharing word vectors and soft mix outputs . it's not something that that model could have ever predicted and it's something that helps a lot but in general they choose different kinds of cells . instead of lstms they learn what kinds of cells should be used at every recurrent time step and with that get to an amazing 62 perplexity . it's really incredible how quickly and just when you thought you had a good intuition of why lstms work well this is basically the kind of architecture that this model comes up with . and there is no more like this gate and what happens when this gate happens . this kind of the model figured out how to do it well and did it in the end incredibly well . are still a lot of limits that we need to tackle as a field still can't do general purpose question answering . we still can't really do complex multitask learning where we might have the same architecture do machine translation and question answering and sentiment analysis and so on . we don't have systems that so over images or speech together with logical reasoning together with memory based retrieval there's still a lot of work to be done . and really all the systems right now require us to have tons of data but i can introduce a new word to you like jane hit john with an uftcha . i just made up the word uftcha but now you can make lots of various assumptions about how heavy it could be is it a physical object and not a mental concept . how big it would be how heavy and all these different logical kind of conclusions from a single example . all the systems we've looked at in this class require ton of different kinds of examples and lot's of statistical patterns that we essentially need to match . all right with that congratulations you've made it . thanks to all the tas thanks to chris . welcome back to the second class of cs224n /ling 284 natural language processing with deep learning . so this class is gonna be almost the complete opposite of the last class . 
so in the last class it was a very high level picture of sort of trying from the very top down . sort of say a little bit about what is natural language processing what is deep learning why it's exciting why both of them are exciting and how i'd like to put them together . so for today's class we're gonna go completely to the opposite extreme . we're gonna go right down to the bottom of words now for some of you this will seem like tedious repetitive baby math . but i think that there are probably quite a few of you for is just going to be useful . and this is really the sort of foundation on which everything else builds . and so if you don't have sort of straight the fundamentals right at the beginning of how you can use neural networks on the sort of very simplest kind of structures it's sort of really all over from there . so what i'd like to do today is sort of really go slowly and carefully through the foundations of how you can start to do things with neural networks in this very simple case of learning representations for words . and hope that's kind of a good foundation that we can build on forwards . and indeed that's what we're gonna keep on doing building forward . so next week richard is gonna keep on doing a lot of math from the ground up to try and really help get straight some of the foundations of deep learning . okay so this is basically the plan . so tiny bit word meaning and no [laugh] . introduce this model of learning introduced by thomas mikolov and and so there are many other ways that you could think about having representations of words . and next week richard's gonna talk about some of those other mechanisms . but today i wanna sort of avoid having a lot of background and comparative commentary . so i'm just gonna present this one way of doing it . and you'd also pretty study the good way of doing it so it's not a bad one to know . okay so then after that was it gonna be one of the features of this class . we decided that all the evidence says that students can't concentrate for 75 minutes . 
so we decided we'd sort of mix it up a little and hopefully also give people an opportunity to sort of get more of a sense of what some of the exciting new work that's coming out every month in deep learning is . and so what we're gonna do is have one ta each time do a little research highlight . which will just be sort of a like you a little bit about some recent paper and why it's interesting exciting . we're gonna start that today with danqi . then after that i wanna go sort of carefully through the word to vec objective function gradients . refresher little on optimization mention the assignment tell you all about word2vec that's basically the plan okay . so we kinda wonder sort of have word vectors as i mentioned last time as a model of word meaning . and i just wanna give kind of a few words of context before we dive into that and do it anyway . okay so if you look up meaning in a dictionary cuz a dictionary is a storehouse of word meanings after all . what the webster's dictionary says is meaning is the idea that is represented by a word phrase etc . the idea that a person wants to express by using words signs etc etc . in some sense this is fairly close to what is the commonest linguistic way of thinking of meaning . you have a linguistic sign like a word and then it has things that it signifies in the world . so if i have a word like glasses then it's got a signification which includes these and there are lots of other pairs of glasses i can see in front of me right . and those things that it signifies the denotation of the term glasses . that hasn't proven to be a notion of meaning that's been very easy for people to make much use of in computational systems for dealing with language . so in practice if you look at what computational systems have done for meanings of words over the last several decades . by far the most common thing that's happened is people have tried to deal with the meaning of words by making use of taxonomic resources . and so if they're english the most famous taxonomic resource is wordnet . and it's famous maybe not like websters is famous . 
because it's free to download a copy and that's much more useful than having a copy of webster's on your shelf . and it provides a lot of taxonomy information about words . so this little bit of python code . this is showing you getting a hold of word net using the nltk which is one of the main python packages for nlp . then i'm saying well tell me about the hypernym the kind of things that it's the kind of . and so for panda it's sort of heading up through carnivores placentals mammals up into sort of abstract types like objects . or on the right hand side i'm sort of asking for the word good will tell me about synonyms of good . and part of what your finding there is well wordnet is saying well the word good has different senses . so for each sense let me tell you some synonyms for each sense . so one sense the second one is sort of the kind of good person sense . and they're suggesting synonyms like honorable and respectable . where this pair is good to eat and that's sort of meaning is ripe . okay so you get this sort of sense of meaning . that's been that's been a great resource but it's also been a resource that people have found in practice . it's hard to get nearly as much value out of it as you'd like to get out of it . i mean there are a whole bunch of reasons . i mean one reason is that at this level of this sort of taxonomic relationships you lose an enormous amount of nuance . so one of those synonym sets for good was adept expert good practiced proficient skillful . but i mean it seems like those mean really different things right . it seems like saying i'm an expert at deep learning . 
means something slightly different so there's a lot of nuance there . there's a lot of incompleteness in wordnet so for a lot of the ways that people use words more flexibly . so if i say i'm a deep-learning ninja or something like that that that's not in wordnet at all . what kind of things you put into these synonym sets ends up very subjective right . which sense distinctions you make and which things you do and don't say are the same it's all very unclear . it requires even to the extent that it's made it's required many person years of human labor . and at the end of the day it's sort of it's kind of hard to get anything accurate out of it in the way of sort of word similarities . like i kind of feel that proficient is more similar to expert than good maybe . okay so therefore that's sort of something of a problem . and it's part of this general problem of discrete or categorical representations that i started on last time . so the fundamental thing to note is that for sorta just about all nlp apart from both modern deep learning and a little bit of neural net work nlp that got done in the 1980s that it's all used atomic symbols and if we think of that from our kind of jaundiced neural net direction using atomic symbols is kind of like using big vectors that are zero everywhere apart from a one and one position . so what we have is we have a lot of words in the language that are equivalent to our symbols and we're putting a one in the position in the vector that represents the particular symbol perhaps hotel . and these vectors are going to be really really long . i mean how long depends on how you look at it . so sometimes a speech recognizer might have a 20000 word vocabulary . but if we're kinda building we might use a 500000 word vocabulary so that's very long . and google released sort of a 1-terabyte corpus of web crawl . and while the size of the vocabulary in that is 13 million words so that's really really long . so it's a very very big vector . and so why are these vectors problematic . 
i'm sorry i'm not remembering my slides so i should say my slides first . okay so this is referred to in neural net land as one-hot in coding because there's just this one on zero in the vector . and the reason why it's problematic is it doesn't give any inherent notion of relationships between words . so very commonly what we want to know is when meanings and words and phrases are similar to each other . so for example in a web search application if the user searches for dell notebook battery size we'd like to match a document that says dell laptop battery capacity . so we sort of want to know that notebooks and laptops are similar and size and capacity are similar so this will be equivalent . we want to know that hotels and motels are similar in meaning . and the problem is that if we're using one-hot vector encodings they have no natural notion of similarity . so if we take these two vectors and say what is the dot product between those vectors it's zero . they have no inherent notion of similarity . and something i just wanna stress a little since this is important is note this problem of symbolic encoding applies not only to traditional rule base logical approaches to natural language processing but it also applies to basically all of the work that was done in probabilistic statistical conventional machine learning base natural language processing . although those latin models normally had real numbers they had probabilities of something occurring in the context of something else that nevertheless they were built over symbolic representations . so that you weren't having any kind of capturing relationships between words and the models each word was a nation to itself . okay so that's bad and we have to do something about it . now as i've said there's more than one thing that you could do about it . and so one answer is to say okay gee we need to have a similarity relationship between words . let's go over here and start building completely separately a similarity relationship between words . and of course you could do that . but i'm not gonna talk about that here . what instead i'm going to talk about and suggest is that what we could do is we could where the representation of a word encodes its meaning in such a way that you can just directly read off from these representations so what we're gonna do is have these vectors and do something like a dot product . 
and that will be giving us a sense of the similarity between words . okay so how do we go about doing that . and so the way we gonna go about doing that is by making use of this very simple but extremely profound and widely used nlp idea called distributional similarity . so this has been a really powerful notion . so the notion of distributional similarity is that you can get a lot of value for representing the meaning of a word by looking at the context in which it appears and doing something with those contexts . so if i want to know what the word banking means what i'm gonna do is find thousands of instances of the word banking in text and i'm gonna look at the environment in which each one appeared . and i'm gonna see debt problems governments regulation europe saying unified and i'm gonna start counting up all of these things that appear and by some means i'll use those words in the context to represent the meaning of banking . the most famous slogan that you will read everywhere if you look into distributional similarity is this one by jr firth who was a british linguist who said you shall know a word by the company it keeps . but this is also really exactly the same notion that wittgenstein proposed in his later writings where he suggested a use theory of meaning . where somewhat controversially this not the main stream in semantics he suggested that the right way to think about the meaning of words is understanding their uses in text . so essentially if you could predict which textual context the word would appear in then you understand the meaning of the word . okay so that's what we're going to do . so what we want to do is say for each word we're going to come up for it a vector and that dense vector is gonna be chosen so that it'll be good at predicting other words that appear in the context of this word . well each of those other words will also have a word that are attached to them and of similarity measures like dot product between those two vectors . and we're gonna change them as well to make it so that good at being able to be predicted . so it all kind off gets a little bit recursive or circular but clever algorithm to do that so that words will be able to predict their context words and vice-versa . and so i'm gonna go on and say a little bit more about that . but let me just underline one bit of terminology that was appearing before in the slide . and then we've had distributed representations where we have these dense vectors to represent the meaning of the words . now people tend to confuse those two words . 
and there's sort of two reasons they confuse them . one is because they both start with distribute and so they're kind of similar . and the second reason people confuse them is because they very strongly co-occur . so that distributed representations and meaning have almost always up until now been built by using distributional similarity . but i did just want people to gather that these are different notions right . so the idea of distributional similarity is a theory about semantics of word meaning that you can describe the meaning of words by as a use theory of meaning understanding the context in which they appear . so distributional contrasts with didn't really explain denotational right . the denotational idea of word meaning is the meaning of glasses is the set of pairs of glasses that are around the place . and distributed then contrasts so the one-hot word vectors are localist representation where you're storing in one place . you're saying here is the symbol glasses . it's stored right here whereas in distributed representations something over a large vector space . and we're now gonna sorta be heading into part two which is what is word2vec . okay and so i'll go almost straight into this . but this is sort of the recipe in general for what we're doing for learning neural word embeddings . that aims to predict between a center word and words that appear in it's context . kind of like we are here the distributional wording . and we'll sort of have some perhaps probability measure or predicts the probability of the context given the words . and then once we have that we can have a loss function as to whether we do that prediction well . so ideally we'd be able to perfectly predict the words around the word so the minus t means the words that aren't word index t so the words around t . if we could predict those perfectly from t we'd have probability one so we'd have no loss but normally we can't do that . 
and if we give them probability a quarter then we'll have sort of three quarters loss or something right . so we'll have a loss function and positions in a large corpus . and so our goal will be to change the representations of words so as to minimize our loss . and at this point sort of a miracle occurs . it's sort of surprising but true that you can do no more than set up this kind of prediction objective . make it the job of every words word vectors to be such that they're good at predicting their words that appear in their context or vice versa . you just have that very simple goal and you say nothing else about how this is gonna be achieved but you just pray and depend on the magic of deep learning . and this miracle happens and outcome these word vectors that are just amazingly powerful at representing the meaning of words and are useful for all sorts of things . and so that's where we want to get into more detail and say how that happens . so that representation was apart from the wt yes what is this w minus t mean . i'm actually not gonna use that notation again in this lecture . but the w minus t minus is sometimes used to mean everything except t . so wt is my focus word and w minus t is in all the words in the context . okay so this idea that you can learn low dimensional vector representations is an idea that has a history in neural networks . it was certainly present in the 1980s parallel distributed processing era including work by rumelhart on learning representations by back-propagating errors . it really was demonstrated for word representations in this pioneering early paper by yoshua bengio in 2003 and neural probabilistic language model . i mean at the time sort of not so many people actually paid attention to this paper this was sort of before the deep learning boom started . where the sort of showed how much value you could get from having distributed representations of words and be able to predict other words in context . but then as things started to take off that idea was sort of built on and revived . so in 2008 collobert and weston started in the sort of modern direction by saying well if we just want good word representations we don't even have to necessarily make a probabilistic language model that can predict learning our word representations . 
and that's something that's then being continued in the model that i'm gonna look at now the word2vec model . that the emphasis of the word2vec model was how can we build a very simple of words of text that will produce exceedingly good word representations . the basic thing word2vec is trying to do is use theory of meaning predict between every word and its context words . now word2vec is a piece of software i mean actually inside word2vec it's kind of a sort of a family of things . so there are two algorithms inside it for producing word vectors and there are two moderately efficient training methods . so for this class what i'm one of the algorithms which is a skip-gram method and about neither of the moderately efficient training algorithms . instead i'm gonna tell you about the hopelessly inefficient training algorithm but is sort of the conceptual basis of how this is meant to work and that the moderately efficient ones which i'll mention at the end . and then what you'll have to do to actually make this a scalable process that you can run fast . and then today is also the day when we're handing out assignment one and major part of what you guys get to do in assignment one is to implement one of the efficient training algorithms and to work through the method one of those efficient training algorithms . so this is the picture of the skip-gram model . so the idea of the skip-gram model is for each estimation step you're taking one word as the center word . so that's here is my word banking and then what you're going to do is you're going to try and predict words in its context out to some window size . and so the model is going to define a probability distribution that is the probability of a word appearing in the context given this center word . and we're going to choose vector representations of words so we can try and maximize that probability distribution . and the thing that we'll come back to . but it's important to realize is there's only one probability distribution this model . it's not that there's a probability distribution for the word one to the left and the word one to the right and things like that . we just have one probability distribution of a context word which we'll refer to as the output because it's what we produces the output occurring in the context close to the center word . so that's what we kinda wanna do so we're gonna have a radius m and then we're going to predict the surrounding words from sort of word to m after our center word . and we're gonna do that a whole bunch of times in a whole bunch of places . 
and we want to choose word vectors such as that we're maximizing the probability of that prediction . so what our loss function or objective function is is really this j prime here . so the j prime is saying we're going to so we're going to take a big long amount of text we take the whole of wikipedia or something like that so we got big long sequence of words so there are words in the context and real running text and we're going to go through each position in the text . and then for each position in the text we're going to have a window of size 2m around it m words before and m words after it . and we're going to have a probability distribution that will give a probability to a word appearing in the context of the center word . and what we'd like to do is set the parameters of our model so that these probabilities of the words that actually do appear in the context of the center so the parameters in this model of these theta here that i show here and here . after this slide i kinda drop the theta over here . but you can just assumed that there is this theta . it's going to be the vector representation of the words . the only parameters in this model of the vector representations of each word . there are no other parameters whatsoever in this model as you'll see pretty quickly . so conceptually this is our objective function . we wanna maximize the probability of this predictions . in practice we just slightly tweak that . firstly almost unbearably when we're working with probabilities and we want to do maximization we actually turn things into log probabilities cuz then all that products turn into sums and our math gets a lot easier to work with and so that's what i've done down here . and the question is hey wait a minute you're cheating windows size isn't that a parameter of the model . and you are right this is the parameter of the model . so i guess i was a bit loose there . actually it turns out that there are several hyper parameters of the model so i did cheat . it turns out that there are a few hyper parameters of the model . 
one is windows sized and it turns out that we'll come across a couple of other fudge factors later in the lecture . and all of those things are hyper parameters that you could adjust . but let's just ignore those for the moment let's just assume those are constant . and given those things aren't being adjusted the only parameters in the model the factor representations of the words . what i'm meaning is that there's sort of no other probability distribution with its own parameters . so we've gone to the log probability and the sums now and and then rather than having the probability of the whole corpus we can sort of take the average over each positions so i've got 1 on t here . and that's just sort of a making it per word as sort of a kinda normalization . so that doesn't affect what's the maximum . and then finally the machine learning people really love to minimize things rather than maximizing things . and so you can always swap between maximizing and minimizing when you're in plus minus land by putting a minus sign in front of things . and so at this point we get the negative log likelihood the negative log probability according to our model . and so that's what we will be formally minimizing as our objective function . so if there were objective function cost function loss function all the same this negative log likelihood criterion really that means that we're using this our cross-entropy loss which is gonna come back to this next week so i won't really go through it now . but the trick is since we have a one hot target which is just predict the word that actually occurred . under that criteria the only thing that's left in cross entropy loss is the negative probability of the true class . well how are we gonna actually do this . how can we make use of these word vectors to minimize that negative log likelihood . well the way we're gonna do it is we're gonna come with the probably given the center word which is constructed out of our word vectors . and so this is what our probability distribution is gonna look like . so just to make sure we're clear on the terminology i'm gonna use forward from here . 
so c and o are indices in the space of the vocabulary the word types . so up here the t and the t plus j where in my text there are positions in my text . those are sort of words 763 in words 766 in my text . but here o and c in my vocabulary words i have word types and so i have my p for words 73 and 47 in my vocabulary words . and so each word type they're going to have a vector associated with them so u o is the vector associated with context word in index o and vc is the vector that's associated with the center word . and so how we find this probability distribution is we're going to use this where we're taking dot products between the the two word vectors and then we're putting them into a softmax form . so just to go through that kind of maximally slowly right . so we've got two word vectors and which means that we so take the corresponding terms and multiply them together and sort of sum them all up . so may adopt product is sort of like a loose measure of similarity so the contents of the vectors are more similar to each other the number will get bigger . so that's kind of a similarity measure through the dot product . and then once we've worked out dot products between words we're then putting it in this softmax form . so this softmax form is a standard way to turn numbers into a probability distribution . so when we calculate dot products they're just numbers real numbers . so we can't directly turn those into a probability distribution so an easy thing that we can do is exponentiate them . because if you exponentiate things that puts them into positive land so it's all gonna be positive . and that's a good basis for having a probability distribution . and if you have a bunch of numbers that come from anywhere that are positive and you want to turn them into a probability distribution that's proportional to the size of those numbers there's a really easy way to do that . which is you sum all the numbers together and you divide through by the sum and a probability distribution . so that's then denominated that is normalizing to give a probability and so when you put those together that then gives us this form that we're using as our softmax form which is now giving us a probability estimate . so that's giving us this probability estimate the word vector representations . 
that is an extremely good question and i was hoping to delay saying that for just a minute but you've asked and so i will say it . yes you might think that one word should only have one vector representation . and if you really wanted to you could do that but it turns out you can make the math considerably easier by saying now actually each word has two vector representation that has one vector representation when it synthesis the word . and it has another vector representation when it's a context word . so that's formally what we have here . so the v is the center word vectors and the u are the context word vectors . and it turns out not only does that make the math a lot easier because the two when you do optimization rather than tied to each other . it's actually in practice empirically works a little better as well better who would not choose that . so yes we have two vectors for each word . yeah so the question is well wait a minute you just said this was a way to actually you also simultaneously screwed with the scale of things a lot . the reason why this is called a softmax function is because it's kind of close to a max function because when you exponentiate things the big things get way bigger and so they really dominate . and so this really sort of blows out in the direction of a max function but not fully . it's still a sort of a soft thing . so you might think that that's a bad thing to do . doing things like this is the most standard underlying a lot of math including all those super common logistic regressions you see another class's way of doing things . so it's a good way to know but people have certainly worked on a whole bunch of other ways . and there are reasons that you might think they're interesting but i won't do them now . yeah so the question was when i'm dealing with the context words am i paying attention to where they are or just their identity . yeah where they are has nothing to do with it in this model . it's just what is the identity of the word somewhere in the window . 
so there's just one probability distribution and one representation of the context word . now you know it's not that that's necessarily a good idea . there are other models which absolutely pay attention to position and distance . and for some purposes especially more syntactic purposes rather than semantic purposes that actually helps a lot . but if you're sort of more interested it turns out that not paying attention help you rather than hurting you . yeah so the question is how wait a minute is there a unique solution here . could there be different rotations that would be equally good . and the answer is yes there can be . i think we should put off discussing this cuz actually there's a lot to say about optimization in neural networks and there's a lot of exciting new work . and the one sentence headline is it's all good news people spent years saying that minimal work ought to be a big problem and it turns out it's not . but i think we better off talking about that in any more detail . okay so yeah this is my picture of what the skip gram model ends up looking like . it's a bit confusing and hard to read but also i've got it thrown from left to right . right so we have the center we then have a matrix of the representations of center words . so if we kind of do a multiplication of this matrix by that vector . we just sort of actually select out the column of the matrix which is then the representation of the center word . then what we do is we have a second matrix which stores the representations of the context words . and so for each position in the context i show three here because that was confusing enough . we're going to multiply the vector by this matrix which is the context word representations . and so we will be picking out sort of the dot products of the center word with each context word . 
and it's the same matrix for each position right . we only have one context word matrix . and then these dot products we're gonna soft max then turn into a probability distribution . and so our model as a generative model is predicting the probability of each word appearing in the context given that a certain word is the center word . and so if we are actually using it generatively it would say well the word you should be using is this one here . but if there is sort of actual ground truth as to what was the context word we can sort of say well the actual ground truth was this word appeared . and you gave a probability and so that's the basis so if you didn't do a great job at prediction then there's going to be some loss okay . but that's the picture of our model . okay and so what we wanna do is now learn parameters these word vectors in such a way that we do as good a job at prediction as we possibly can . and so standardly when we do these things what we do is we take all the parameters in our model and put them into a big vector theta . and then we're gonna say we're gonna do optimization to change those parameters so as to maximize objective function of our model . so what our parameters are is that for each word we're going to have a little d dimensional vector when it's a center word and when it's a context word . and so we've got a vocabulary of some size . so we're gonna have a vector for aardvark as a context word a vector for art as a context word . we're going to have a vector a vector of art as a center word . so our vector in total is gonna be of length 2dv . there's gonna be a big long vector that has everything that was in what was shown in those matrices before . and so after the break i'm going to be so going through concretely how we do that optimization . but before the break our special guest danqi chen . i'm danqi chen and i'm the head ta of this class . 
so today i will start our first research highlight session and i will introduce you a paper from princeton . the title is a simple but tough-to-beat baseline for sentence embeddings . so today we are learning the word vector representations so we hope these vectors can encode the word meanings . but our central question in natural language processing and also this class is that how we could have the vector representations that encode the meaning of sentences like natural language processing is fun . so with these sentence representations we can compute the inner product of the two vectors . so for example mexico wishes to guarantee citizen's safety and mexico wishes to avoid more violence . so we can use the vector representation to predict these two representation to use as features to do some sentence classification task . so given a sentence like natural language processing is fun we can put our classifier on top of the vector representations and predict if sentiment is positive . so there are a wide range of measures that compose word vector vector representations . so the most simple way is to use the bag-of-words . so the bag-of-words is just like the vector representation of the natural language processing . it's a average of the three single word vector representations the natural language and processing . later in this quarter we'll learn a bunch of complex models such as recurrent neural nets the recursing neural nets and the convolutional neural nets . but today for this paper from princeton i want to introduce that this paper introduces a very simple unsupervised method . that is essentially just a weighted bag-of-words sentence representation plus remove some special direction . so the first step is that just like how we compute the average of the vector representations they also do this but each word has a separate weight . and the p(w) it means the frequency of this word . so this basically means that the average representation down weight the frequent words . so for the step 2 after we compute all of these sentence vector representations we compute the first principal components and also subtract the projections onto this first principle component . you might be familiar with this if you have ever taken cs 229 and also learned pca . 
so in this paper they also give a probabilistic interpretation about why they want to do this . so basically the idea is that given the sentence representation the probability of the limiting or single word they're related to the frequency of the word . and also related to how close the word is related to this sentence representation . and also there's a c0 term that means common discourse vector . so first they take context parents on the sentence similarity and they show that this simple approach is much better than the average of word vectors all the tfidf rating and also all the performance of other sophisticated models . and also for some supervised tasks they're also doing pretty well like the entailment and sentiment task . so and we'll go back from there . all right so now we're wanting to sort of actually work through our model . so this is what we had right . we had our objective function where we wanna minimize negative log likelihood . and this is the form of the probability distribution up there where we have these sort of word vectors with both center word vectors and context word vectors . and the idea is we want to change our parameters these vectors so as to minimize the negative log likelihood item maximize the probability we predict . so if that's what we want to do how can we work out how to change our parameters . gradient yes so what we're gonna have to do at this point is to start to do some calculus to see how we can change the numbers . so precisely what we'll going to want to do is to say well we have this term for working out log probabilities . so we have the log of the probability of the word t plus j word t . well what is the form of that . so we have the log of v maybe i can save a line . and then what we're gonna want to do is that we're going to want to change this so that we have i'm sorry minimized in this objective . so let's suppose we sort of so what we're gonna want to do is start working out the partial derivatives of this with respect to the center vector which is then going to give us how we can go about working out in which way to change this vector to minimize our objective function . 
okay so we want to deal with this . so what's the first thing we can do with that to make it simpler . so this is a log of a division so we can turn that into a log of a subtraction and then we can do the partial derivatives separately . so we have the derivative with vc of the log of the exp of u0^t vc and then we've got equals 1 to v of exp of u w^t vc . and at that point we can separate it into two pieces right cuz when there's addition or subtraction we can do them separately . so we can do this piece 1 and we can do the work out the partial derivatives of this piece 2 . so piece 1 looks kind of easy so let's start here . so what's the first thing i should do to make this simpler . cancel some things out log and x inverses of each other so they can just go away . so for 1 we can say that this is going to be the partial derivative with respect to vc of u0^t vc . okay that's looking kind of simpler so what is the partial derivative of this with respect to vc . u0 so this just comes out as u0 . okay and so i mean effectively this is the kind of level of calculus that you're gonna have to be able to do to be okay on assignment one that's coming out today . so it's nothing that life threatening hopefully you've seen this before . but nevertheless we are here using so vc here is not just a single number it's a whole vector . so that's sort of the math 51 cme 100 kind of content . now if you want to you can pull it all apart . and you can work out the partial derivative with respect to vc some index k . and then you could have this as the sum of l = 1 to d of (u0)l (vc)l . and what will happen then is if you're working out of with respect to only one index then all of these terms will go away apart from the one where k equals l . 
and you'll sort of end up with that being the (uo)k term . and i mean if things get confusing and complicated i think it can actually and your brain is small like mine it can actually be useful to sort of go down to the level of working it out with real numbers and actually have all the indices there and you can absolutely do that and it comes out the same . but a lot of the time it's sort of convenient if we can just work out vector derivatives okay . so now this was the easy part and we've got it right there and we'll come back to that okay . so then the trickier part is we then go on to number 2 . so now if we just ignore the minus sign for a little bit so we've then got the partial derivatives with respect to vc of the log of the sum from w = 1 to v of the exp of uw^t vc well how can we make progress with this half . yeah so that's right before you're going to do that . the chain rule okay so our key tool that we need to know how to use and we'll just use everywhere is the chain rule right . so neural net people talk all the time about backpropagation it turns out that backpropagation is nothing more than the chain rule with some efficient storage of partial quantities so that you don't keep on calculating the same quantity over and over again . so it's sort of like chain rule with memorization that is the backpropagation algorithm . so now key tool is the chain rule so what is the chain rule . so within saying okay well what overall are we going to have is some function where we're taking f(g(u)) of something . and so we have this inside part z and so what we're going to be doing is that we're going to be taking the derivative of the outside part then and then we're gonna be taking the derivative of the inside part so for this here so the outside part here's our f . and then here's our inside part z . so the outside part is f which is a log function . and so the derivative of a log function is the one on x function . so that we're then gonna be having that this is 1 over the sum of w equals 1 to v of the exp of uw^t vc . and then we're going to be multiplying it by what do we get over there . so we get the partial derivative with respect to with respect to vc of this inside part . the sum of and it's a little trickier . 
we really need to be careful of indices so we're gonna get in the bad mess if we have w here and we reuse w here . we really need to change it into something else . so we're gonna have x equals 1 to v . and then we've got the exp of ux transpose vc . so that's made a little bit of progress . we want to make a bit more progress here . so what's the next thing we're gonna do . we can do the same trick of we can do each part of the derivative separately . so x equals 1 to big v of the partial derivative with respect to vc of the exp of ux^t vc . okay now we wanna keep going what can we do next . this is also the form of here's our f and here's our inner values v which is in turn sort of a function . yeah so we can apply the chain rule a second time and so we need the derivative of x . x so this part here is gonna be staying . the sum of x equals 1 to v of the partial derivative . so it's still exp at its value of ux t vc . and then we're having the partial derivative with respect to vc of uxt vc . and then we've got a bit more progress to make . so we now need to work out what this is . right so that's the same as sort of back over here . at this point this is just going to be that' s coming out as ux . 
and here we still have the sum of x equals 1 to v of the x of ux t vc . so at this point we kind of wanna put this together with that . cuz we're still i stopped writing that . but we have this one over the exp of uw transpose vc . can we put those things together in a way that makes it prettier . so i can move this inside this sum . cuz this is just the sort of number that's a multiplier that's distributed through . and in particular when i do that i can start to sort of notice this interesting thing that i'm going to be reconstructing a form that looks very like this form . it looks very like the softmax and so i can then be saying that this is the sum from x equals 1 to v of the exp of ux transpose vc over the sum of w equals 1 to v . so this is where it's important that i have x and w with different variables of the x of u w transpose vc times u of x . and so well at that point that's kind of interesting cuz this is kind of exactly the form that i started of with for my softmax probability distribution . what we're doing is that that part is then being the sum over x equals one to v of the probability of [inaudible] . the probability of o given the probability of x given c times ux . so that's what we're getting from the denominator . and then we still had the numerator . what we have here is our final form is u0 minus that . it's sort of a form that you always get from these softmax style formulations . there was the actual output context word appeared . and this has the form of an expectation . so what we're doing is right here . 
though we're working out the probability of every possible word appearing in the context and based on that probability we get taking that much of that ux . so this is in some this is the expectation vector . it's the average over all weighted by their likelihood of occurrence . what we're going to want to be doing is changing the parameters in our model . in such a way that these become then finding the maximum and minimum for us to minimize . anyway so precisely doing things like this is what will expect you to do for assignment one . and i'll take the question but let me just mention one point . so in this case i've only done this for the vc the center vectors . we do this to every parameter of the model . we're also gonna do it for those . it's very similar cuz if you look at the form of the equation there's a certain symmetry between the two . but we're gonna do it for that as well but i'm not gonna do it here . so right so this is a sum right . and this is just the number at the end of the day . so now i've got my sum with every term in that divided through by this number . and then i say wait a minute the form of this piece here is precisely my softmax probably distribution of x given c . and so then i'm just rewriting it as probability of x given c . where that is meaning i kind of did double duty here . but that's sort of meaning that you're using this probability of x given c using this probability form . the probability that x occurs as a context word of center word c . 
we've just assumed some fixed window size m . so maybe our window size is five and so we're considering sort of ten words five to the left five to the right . so that's a hypergrameter and that stuff's nowhere . we're not dealing with that we just assume that god's fixed that for us . the problem so it's done at each position . so for any position and all of them are treated equivalently for any position the probability that word x is the word that occurs within this window at any position given the center word was of c . why do we choose the dot product as our basis for coming up with this probability measure . and you know i think the answer is there's no necessary reason that there are clearly other things that you could have done and might do . on the other hand i kind of think in terms of vector algebra it's sort of the most obvious and simple thing to do . because it's sort of a measure of the relatedness and similarity . i mean i sort of said loosely it was a measure of similarity between vectors . someone could have called me on that because if you say well wait a minute . if you don't control for the scale of the vectors you can make that number as big as you want and that is true . so really the common measure of similarity between vectors is the cosine measure . where what you do is in the numerator . you take a dot product and then you divide through by the length of the vectors . so you've got scale and variance and you can't just cheat by making the vectors bigger . and so that's a bigger but to do that you have to do a whole lot more math and it's not actually necessary here because since you're sort of predicting every word against every other word . if you sort of made one vector very big to try and make some probability of word k being large . well the consequence would be it would make the probability of every other word be large as well . 
so you kind of can't cheat by lengthening the vectors . and therefore you can get away with just using the dot product as a kind of a similarity measure . and if we were going to argue you could sort of argue with me and say no look this is crazy because by construction this means the most likely word to appear in the context of a word is itself . that doesn't seem like a good result different words occur . and you could then go from there and say well no let's do something more complex . why don't we put a matrix to mediate between the two vectors to express what appears in the context of each other it turns out you don't need to . now one thing of course is since we have different representations for the context and center word vectors it's not necessarily true that the same word would be highest because there're two different representations . but in practice they often have a lot of similarity between themselves not really that that's the reason . it's more that it's sort of works out pretty well . because although it is true that you're not likely to get exactly the same word in the context you're actually very likely to get words that are pretty similar in meaning . and are strongly associated and when those words appear as the center word you're likely to get your and so at a sort of a macro level you are actually getting this effect that the same words are appearing on both sides . more questions yeah there are two of them . do i do the behind person first and then the in front person . so i haven't yet done gradient descent . and maybe i should do that in a minute and i will see try then . we've just clicked to the huge amount text . so if our word at any position we know what are the five words to the left and the five words to the right and that's the truth . and so we're actually giving some word appearing in that context and we can say well actually the word that appeared there was household . what probability did you give to that and there's some answer . time is running out so maybe i'd sort of just better say a little bit more before we finish which is sort of starting to this optimization . 
so this is giving us our derivatives we then want to use our derivatives to be able to work out our word vectors . and i mean i'm gonna spend a super short amount time on this the hope is through 221 229 or similar class . you've seen a little bit of optimization and you've seen some gradient descent . and so this is just a very quick review . so the idea is once we have gradient set at point x that if what we do is we subtract off a little fraction of the gradient that will move us downhill towards the minimum . and so if we then calculate the gradient there again and subtract off a little fraction of it we'll sort of start walking down towards the minimum . and so that's the algorithm of gradient descent . so once we have an objective function and we have the derivatives of the objective function with respect to all of the parameters our gradient descent algorithm would be to say you've got some current parameter values . we subtract off a little fraction of that and that will give us new parameter values which we will expect to be give us a lower objective value and we'll walk towards the minimum . and in general that is true and that will work . so then to write that up as python code it's really sort of super simple that you just go in this while true loop . you have to have some stopping condition actually where you evaluating the gradient of given your objective function your corpus and your current parameters so you have the theta grad and then you're sort of subtracting a little fraction of the theta grad after the current parameters and then you just repeat over . and so the picture is so the red lines that are sort of the contour lines of the value of the objective function . and so what you do is when you calculate the gradient it's giving you the direction of the steepest descent and you walk a little bit each time in that direction and you will hopefully walk smoothly towards the minimum . now the reason that might not work is if you actually take a first step and you go from here to over there you've greatly overshot the minimum . so it's important that alpha be small enough that you're still walking calmly down towards the minimum and then all work . and so gradient descent is the most basic tool to minimize functions . so it's the conceptually first thing to know but then the sort of last minute . what i wanted to explain is actually we might have 40 billion tokens in our corpus to go through . and if you have to work out the gradient of your objective function relative to a 40 billion word corpus that's gonna take forever so you'll wait for an hour before you make your first gradient update . 
and so you're not gonna be able train your model in a realistic amount of time . so for basically all neural nets doing naive batch gradient descent hopeless algorithm you can't use that . so instead what we do is used stochastic gradient descent . so the stochastic gradient descent or sgd is our key tool . and so what that's meaning is so we just take one position in the text . so we have one center word and the words around it and we say well let's adjust it at that one position work out the gradient with respect to all of our parameters . and using that estimate of the gradient in that position we'll work a little bit in that direction . if you think about it for doing something like word vector learning this estimate of the gradient is incredibly incredibly noisy because we've done it at one position which just happens to have a few words around it . so the vast majority of the parameters of our model we didn't see at all . walking a little bit in that direction isn't even guaranteed to have make you walk downhill because it's such a noisy estimate . but in practice this works like a gem . it's not only that doing things this way is orders of magnitude faster than batch gradient descent because you can do an update after you look at every center word position . it turns out that neural network algorithms love noise . so the fact that this gradient descent the estimate of the gradient is noisy actually helps sgd to work better as an optimization algorithm and neural network learning . and so this is what we're i have to stop there for today even though the fire alarm didn't go off . i'm richard and today we'll talk a little bit more about word vectors . but before that let's do three little organizational items . first we'll have our first coding session this week . a bunch of programming for you as the first and only one where you will do everything from scratch . so do get started early on it . 
the coding session is mostly to help you chat with other make sure you have everything set up properly your environments and everything so you can get into the exciting deep learning parts right away . it's excited to help you find companies to work at and talk about your career . and then my first project advice office hour's today i'll just grab a quick dinner after this and then i'll be back here in the huang basement to chat . mostly about projects so we encourage you to think about your projects early and so we'll start that today . very excited to chat with you if wanna just bounce off ideas in the beginning that will be great . i think just like outside like right here in front of the class . and we have a calendar on the website and you can find all our office hours on the calendar . we'll add the names of who's doing the office hours especially for chris and mine all right great . but then where it gets really interesting is we're actually asked what word2vec really captures . we have these objective functions we're optimizing . and we'll take a bit of a look and analyze what's going on there . and then we'll try to actually capture the essence of word2vec a little more effectively . and then also look at our first analysis of intrinsic and extrinsic evaluations for word vectors . by the end you actually have a good sense of how to evaluate word vectors and you have at least two methods under your belt on how to train them . so let's do a quick review of word2vec . we ended with this following equation here where we wanted to basically predict the outside vectors from the center word so lets just recap really quickly what that meant . so let's say i have the beginning of a corpus and it says something like i like deep learning or just and nlp . now what we were gonna do is we basically wanna compute the probability . let's say we start with these word vectors in this is our first center word and that's deep . so we wanna first compute the probability of the first outside word i given the word deep and that was something like the exponent here of uo . 
so the u vector is the outside word and so that's in our case i here transposed the deep . and then we had this big sum here and the sum is always the same given for a certain vc . now how do we get this v and this u . we basically have a large matrix here with all the different word vectors for all the different words . so it starts with vector for aardvark . and a and so on all the way to maybe the vector for zebra . and we had basically all our center words v in here . and then we have one large matrix where we have again all the vectors and so on all the way to zebra . and when we start in our first window through this corpus we basically collect take that vector for deep here this vector v plug it in here and then we wanna maximize this probability . and now we'll take the vectors for u for all these different words like i like learning and and . so the next thing would be i for like or the probability of like given deep . and that'll be the exponent of u like transpose of v deep . and again we have to divide by this pretty large sum over the entire vocabulary . so it's essentially little classification problems all over . so that's the first window of this corpus . now when we move to the next window we basically move one over . and now the center word is learning and we wanna predict these outside words . so now we'll take for this next the second window here . this was the first window the second window . we'll now take the vector v for learning and the u vector for like deep and nlp . 
so that was the skip gram model that we talked about in the last lecture just explained again with the same notation . but basically you take one window at a time you move that window and you keep trying to predict the outside words . that's a good question so how do you actually develop that . you start with all the numbers all these vectors are just random . little small random numbers often sampled uniformly between two small numbers . and then you take the derivatives with respect to these vectors in order to and you essentially take the gradient here of each of these windows with sgd . and so when you take the derivatives that we went through in latin last lecture with respect to all these different vectors here you get this very very large sparse update . cuz all your parameters are essentially all the word vectors . and basically these two matrices with all these different column vectors . and so let's say you have 100 dimensional vectors and you have a vocabulary of let's say 20000 words . so that's a lot of different numbers that you have to optimize . and so these updates are very very large . but they're also very sparse cuz each window you usually only see five words if your window size is two . we'll get to that once we look at the evaluation of these word vectors . this cost function is not convex it doesn't matter sorry i should repeat all the questions sorry for the people on the video . so the first question was how do we choose the dimensionality . it turns out most of the objective functions pretty much almost of them in this lecture are not convex and so initialization does matter . tricks on how to circumvent getting stuck in very bad local optima . as long as you initialize with small random numbers especially in these word vectors it does not tend to be a problem . all right so we basically run sgd it's just a recap of last lecture . 
run sgd we update now our cost function here at each window as we move through the corpus right . and so when you think about these updates and you think about implementing that which you'll very soon for problem set one you'll realize well this entire vector here sorry . this vector of all these different numbers and i explicitly actually keep around these zeros you have very very large updates and you'll run out of memory very quickly . and so what instead you wanna do is either have very sparse matrix operations where you update only specific columns . for this second window you only have to update the outside vectors for like deep and nlp and inside vector for learning . or you could also implement this as essentially a hash where you have keys and values . and the values are the vectors and the keys are the word strings . all right now when i told you this is the skip-gram model i actually kind of lied a little bit to teach it to you one step at a time . it turns out when you do this computation here the upper part is pretty simple right . this is just the hundred-dimensional vector and you multiply that with another hundred-dimensional vector . but at each window and again you go through an entire corpus right . you do this one step at a time one word at a time . and for each window you do this computation . and you do also this gigantic sum . and this sum goes over the entire vocabulary . again potentially 20000 maybe even a million different words in your whole corpus . all right so each window and that's not very efficient . and it turns out you also don't teach the model that much . at each window you say deep learning or learning does not co-occur with zebra . it does not co-occur with 20000 other words . 
cuz most words don't actually appear with most other words it's pretty sparse . and so the main idea behind skip-gram is a very neat trick which is we'll just train a couple of binary logistic regressions for the true pairs . so we keep this idea of wanting to optimize and maximize this inner product of the center word and the outside words . but instead of going through all we'll actually just take a couple of random words and say how about these random words from the rest of the corpus don't co-occur . and this leads us to the original objective function of the skip-gram model which sort of as a software package is often called word2vec . and the original paper title was distributed representations of words and phrases and their compositionality . and so the overall objective function is as follows . basically you go again through each window . so t here corresponds to each window and then we have two terms here . the first one is essentially just a log probability of these two center words and outside words co-occurring . and so the sigmoid here is a simple element wise function . we'll use the sigmoid function a lot . you'll have to really be able to take derivatives of it and so on . it just takes any real number and squashes it to be between zero and one . and that's for you learning people good enough to call it a probability . you wanna have proper measures and so on so it's not quite that much but it's a number between zero and one . and then we basically can call this here a term that we basically wanna maximize the log probability of any questions about the first term . this is very similar to before but then we have the second term here . and the original description but really we can have some clear notation that essentially just shows that we're going to randomly sub sample a couple of the words from the corpus . and for each of these we will essentially try to minimize their probability of co-occurring . 
and so one good exercise is actually for you in preparation for midterms . and what not to prove to yourself that one of sigmoid of minus x is the same as one minus sigmoid of x . that is a nice little quick proof to get into the zone . and so basically this is one minus the probability of this . so we'd subsample a couple of random words from our corpus instead of going through an aardvark doesn't appear . zebra doesn't appear with learning and so on . we'll just sample five or ten or so and then we minimize their probabilities . and so usually we take and this is again a hyperparameter one that will have to evaluate how much it matters . i will take k negative samples for the second part here of the objective functions for each window . and then we minimize the probability that these random words appear around the center word . and then the way we sample them is actually from a simple uniform or unigram distribution here . we basically look at how often do the words generally appear and then we sample them based on that . but we also take the power of three-fourth . if you play around with this model for long enough you say well maybe it should more often sample some of these rare words cuz otherwise it would very very often sample the and a and other stop words . and would probably never ever sample aardvark and zebra in our corpus so you take this to the power of three-fourth . and you don't have to implement this function we'll just give it to you cuz you kind of have to compute the statistics of how often each word appears in the corpus . but we'll give this to you in the problem set . all right so any questions around the skip-gram model . that's right so the question is is it a choice of how to define p of w . and it is a choice you could do a lot of different things there . 
but it turns out a very simple thing like just taking the unigram distribution . how often does this word appear works well enough . so people haven't really explored more complex versions than that . should we make sure that the random samples here aren't the same as exactly this word . yes but it turns out that the probability for a very large corpora is so tiny that the very very few times that ever happens is kind of irrelevant . it's relatively small and it's an interesting trade-off that you'll observe in actually often as you go through the corpus you could do an update after each window but you could also say let's go through five windows collect the updates and then make a really a step in your.. . gradient descent and we'll go through a lot these kind of options later in the class . all right last question on skip gram what does jt(theta) represent . so theta is often a parameter that we use for all the variables in our model . so in our case here for the skip-gram model it's essentially all the u vectors and all the v vectors . later on when we call we'll call a theta it might have other parameters of the neural network layers and so on . and j is just our cost function and t is at the tth time step or the tth window as we go through our corpus . so in the end our overall objective function that we actually optimize is the sum of all of them . but again we don't wanna do one large update of the entire corpus right . we don't wanna go through all the windows collect all the updates and then make one gigantic step cuz that usually doesn't work very well . so good question i think last lecture we talked a lot about minimization . here we have these log probabilities and in the paper you wanna maximize that . once you have probabilities you usually wanna maximize the probability of the actual thing that you and then other times when we call it a cost function we wanna minimize the cost and so on . all right so in word2vector's another model which you won't have to implement unless you want to get bonus points . but we will ask you to take derivatives of and so it's good to understand it at least in a very simple conceptual level . 
and it's very similar to the skip-gram model . the center word from the sum of the surrounding words . so very simply here we sum up the vector of and of nlp and of deep and of like and we have the sum of these vectors . and then we have some inner products with just the vector of the inside . and basically that's called the continuous bag of words model . you'll learn all about the details and the definition of that in the problem set . so what actually happens when we train these word vectors right . we optimize this objective function and we take gradients and after a while something kind of magical happens to these word vectors . and that is that they actually start to cluster around similar kinds of meaning and sometimes also similar so when we zoom in and again this is usually these vectors are 25 to even 500 or thousand dimensional this is just a pca visualization of these vectors . and what we'll observe is that tuesday and thursday and weekdays cluster together number terms cluster together first names cluster together and so on . so basically words that appear in similar context turn out to often have dissimilar meaning as we discussed in previous lecture . and so they essentially get similar vectors a sufficient number of sets . basically we went through each word in the corpus . we looked at the surrounding words in the window . now what we are essentially doing there is trying to capture the coocurrence of words . how often does this word cooccur with the other word . and we did that one count at a time . i make an update to both of this vectors . and then you go over the corpus and then you probably will eventually see deep and you make again a separate update step . when you think about that it's not very efficient right . 
why now we just go to the entire corpus once count how often this deep and learning cooccur of these two words cooccur and then we make one update step that captures the entire count instead of one sample at the time . and yes we can do that and that is actually a method that came historically before word2vec . and there are different options of how we can do this . the simplest one or the one that is similar to word2vec at least is that we again use a window around each word and we basically just we don't update anything we don't do any sgd . and once we have the counts then we do something to that matrix . and so when we look at just the window of length maybe two like in this example here or maybe five some small window size around each word what we'll do is we'll capture not just the semantics but also some of the syntactic information of each word . namely what kind of part of speech tag is it . so verbs are going to be closer to one another . then the verbs are to nouns for instance . if on the other hand we look at co-occurrence counts that aren't just around the window but entire document so i don't just look at each window . but i say this word appears with all these other words in this entire wikipedia article for instance or this entire word document . then what you'll capture is actually more topics and this is often called latent semantic analysis a big popular model from a while back . and basically what you'll get there is you'll ignore the part of speech that you ignore any kind of syntactic information and just say well swimming and boat and water and weather and the sun they're all kind of appear in this topic together in this document together . so we won't go into too many details for these cuz they turn out for a lot of other downstream tasks like machine translation or so and we really want to use these windows but it's good knowledge to have . so let's go over a simple example of what we would do if we had a very small corpus and wanna collect these windows and then compute word vectors from that . so it is technically not cosine cuz we are not normalizing over the length and technically we are not optimizing inner products of these probabilities and so on . so the question is in all these visualizations here we kind of look at euclidean distance . and it's true we're actually often are going to use inner products kinds of similarities . so yes in some cases euclidean distance works reasonably well still despite not doing this in fact we'll see one evaluation that is entirely based or partly based on euclidean distances and partly inner products . so it turns out both work well despite our objective function only having this . 
and even more surprising there're a lot of things that work quite well on this despite starting with this kind of objective function . we often yeah so if despite having only this inner product optimizations we will actually also do often very well in terms of euclidean distances . well it get's complicated but there are some interesting relationships between the ratios of the co-occurence counts we don't have enough time to dive into the details but if you are interested in that i will talk about a paper . i mentioned the title of the paper in five or ten slides that will help you understand that a little better and gain some more intuition yep . all right so window based co-occurrence matrices . so let's say that's to find our window length as just 1 for simplicity . usually we have more commonly and we assume we have a symmetric window so we don't care if a word is to the left or to the right of our center word . so this is essentially what a window based co-occurrence matrix would be for this very very simple corpus . we just look at the word i and then we look at which words appear next to i . and so we look at i we see like twice so we have number two here . and we see enjoy once so we put the count one here . and then we know we have the word like . and so like co-occurs twice with the word i on it's left and once with deep and once with nlp . and so this is essentially we go through all the words in a very large corpus and we compute all these counts super simple . now you could say well that's a vector already right . you have a list of numbers here and that list of numbers now represents that word . and you already kinda capture things like well like and enjoy have some overlap so maybe they're more similar . so you already have a word vector right . but now it's not a very ideal word vector for a couple of reasons . the first one is if you have a new word in your vocabulary that word vector changes . 
so if you have some downstream machine learning models now to take that vector's input they always have to change and there's always some parameter missing . also this vector is going to be very high-dimensional . of course for this tiny corpus it's small but generally we'll have tens of thousands of words . so you'll have sparsity issues if you try to train a machine learning model on this afterwards and that moves up in a much less robust downstream models . and so the solution to that is lets again have the similar idea to word2vec and have just don't store all of the co occurrence counts every single number . but just store most of the important information the fixed small number of dimensions similar to word2vec those will be somewhere around 25 to 1000 dimensions . and then the question is okay how do we now reduce the dimensionality we have these very large co-occurrence matrices here . in the realistic setting we'll have 20000 by 20000 or even a million by a million very large sparse matrix how do we reduce the dimensionality . and the answer is we'll just use very simple svd . all right good the majority of people if you're not then i strongly suggest you go to the office hours and brush up on your linear algebra . but basically we'll have here this x hat matrix which is going to be our best rank k approximation to our original co-occurrence matrix x . and we'll have basically these three simple matrices with orthonormal columns . u we often call also our left-singular vectors and we have here s the diagonal matrix containing all the singular values usually from largest to smallest . and we have our matrix v here our orthonormal rows . and so in code this is also extremely simple we can literally implement this in just a few lines if we have this is our corpus here and this is our co-occurrence matrix x . then we can simply run svd with one line of python code and then we get this matrix u . and now we can take the first two columns here of u and plot them right . and if we do this in the first two dimensions here we'll actually get similar kinda visualization to all this other ones i've showed you right . but this is a few lines of python code to create that kinda word vector . and now it's kinda reading tea leaves none of these dimensions we can't really say this dimension is noun the verbness of a word or something like that . 
but as you look at these long enough you'll definitely observe some kinds of patterns . so for instance i and like are very frequent words in this corpus and they're a little further to the left so that's one . like and enjoy are nearest neighbors in this space so that's another observation they're both verbs and so on . so the things that were being liked flying and deep and other things are closer together and so on . so such a very simple method you get a first approximation to what word vectors can and should capture . are there any questions around this svd method in the co-occurrence matrix . it's a good question is the window always symmetric . and the answer is no we can actually evaluate asymmetric windows and symmetric windows and i'll show you the result of that in a couple of slides . all right now once you realize wow this is so simple and it works kinda well and you're a researcher you always wanna try to improve it a little bit . and so there are a lot of different hacks that we can make to this co-occurrence matrix . so instead of taking the raw counts for instance as you do this you realize well a lot of representational power in this word vectors is now captured by the fact that the and he and has and a lot of other very very frequent words co-occur with almost all the nouns . like the appears in the window of pretty much every noun out there . and it doesn't really give us that much information that it does over and over and over again . and so one thing we can do is all right whatever the co-occurs with the most and a lot of other one of these function words we'll just maximize the count at 100 . or i know some people do this also we just ignore a couple of the most frequent words cuz they really we have a power law distribution or zipf's law where basically the most frequent words appear much much more frequently than other words and then it peters out . and then there's a very long tail of words that don't appear that often but those very rare words often have a lot of semantic content . then another way we can change this the way we compute these counts is by not counting all the words equally . so we can say well words that appear right next to my center word get a count of one . or words that appear and they're five steps away five words away only you get a count of 0.5 . and so that's another hack we can do . 
and then instead of counts we could compute correlations and set them to 0 . you get the idea you can play a little around with this matrix of co-occurrence counts in a variety of different ways and sometimes they help quite significantly . so in 2005 so quite a long time ago people used this svd method and compared a lot of different ways of hacking the co-occurrence matrix and modifying it . and basically found quite surprising and awesome results . and so this is another way we can try to visualize this very high dimensional space . again these vectors are usually around 100 dimensions or so so it's hard to visualize it . and so instead of projecting it down to just 2d here they just choose a couple of neighbours and which word is closest to what other word and they find that wrist and ankle are closest to one another . and the next closest one is arm and so on . and so different extremities cluster cities clustering together and american cities are closer to one another than cities from other countries and country names are close together and so on . as svd around these windows you capture a lot of different kinds of information . in fact it even goes to syntactic and chromatical kinds of patterns that are captured by this svd method . so show showed shown or take took taken and so often similar kinds of patterns . and it goes further and even more semantic in the verbs that are very similar and related to these kinds of nouns . often appear even in roughly similar so swim and swimmer clean and janitor drive and driver teach and teacher . they're all basically have a similar kind of vector difference . and intuitively you would think well they appear of context in which they appear . and there's some intuitive sense of why why this would happen as you're trying to capture these co-occurrence counts . so if it was german instead of english . so it's actually a sad truth of a lot of natural language processing research that the majority of it is in english . it turns out this works for a lot of other languages . 
evaluation metrics often for these other languages and evaluation data sets which we'll get to in a bit . but we would believe that it works for pretty much all languages . now there's a lot of complexity because some languages like finnish or german have potentially a lot of different words cuz they have much richer morphology right . and so you get more and more rare words and the less good counts you have of them and the harder it is to use will get us to character-based natural language processing which we'll get to in a couple weeks . but in general this works for pretty much any language . well svd while being very simple and one nice line of python code is actually computationally not always great especially as we get larger and larger matrices . so we essentially have this quadratic cost here in the smaller dimension . so either if it's a word by word co-occurrence matrix or even a word by document we'd assume this gets very very large . incorporate new words or documents into into this whole model cuz you have to rerun this whole pca or sorry the svd singular value decomposition . and then on top of that svd and how we optimize that is quite different to a lot of the other downstream deep learning methods that we'll use like neural networks and so on . it's a very different kind of optimization . and so the word to vec objective function is similar to svd you look at one window at a time . and that is very similar to how we optimize most of the other models in this lecture and in deep learning for nlp . and so basically what we came with with post-doc and chris' group so jeffery pennington me and chris is a method that tries to combine the best of both worlds . so let's summarize what the advantages and disadvantages are of these two different kinds of methods . based methods based on svd and the co-occurence matrix . and we have the window-based or direct prediction methods like the skip-gram model . it's relatively fast to train unless the matrix gets very very large but we're making very efficient usage of the statistics that we have right . we only have to collect the statistics throw away the whole corpus . and then we can try a lot of different things on just these co-occurence counts . 
sadly when you do this it captures mostly word similarity and not various other patterns that the word2vec model captures and we'll show you what those are in evaluation . and we give often disproportionate importance to these large counts . and we can try various ways of lowering the importance very frequent words have . the disadvantage of the skip-gram of model is that it scales with a corpus size right . you have to go through every single window which is not very efficient and henceforth you also don't really make very efficient usage of the statistics that you have overall of the data set . however we actually get in may cases much better performance on downstream tasks . and we don't know yet those downstream tasks that's why we have the whole lecture for this whole quarter . but for a variety of different problems like an entity recognition or part of speech tagging and so on . things that you'll implement in the problem sets it turns out the skip-gram like models turn out to work slightly better . and we can capture various complex patterns some of which are very surprising and we'll get to in the second part of this lecture . and so basically what we tried to do here is combining the best of both of these worlds . and the result of that was the glove model our global vectors model . so let's walk through this objective function a little bit . again theta here will be all our parameters . so in this case again we have these u and these v vectors . but they're even more symmetric now we basically just go through all pairs of words that might ever co-occur . so we go through these very large co-occurrence matrix that we computed in the beginning and we call p here . and for each pair of words in this entire corpus the distance between the inner product here and the log count of these two words . so again this is just this kind of matrix here that we're going over . we're going over all elements of this kind of co-occurrence matrix . 
but instead of running the large svd one such count at a time here . so i have the square of this distance and then we also have this term here f which allows us to weight even lower some of these very frequent kinds of co-occurrences . so the for instance will have the maximum amount that we can weigh it inside this overall objective function . all right so now what this allows us to do is essentially we can train very quickly . cuz instead of saying all right we'll optimize that deep and learning co-occur in one window and then we'll go in a couple windows later they co-occur again . and we update again with just one say or a deep learning co-occur in this entire corpus . which could now be in all of wikipedia or in our case all of common crawl . which is most of the internet that's kind of amazing . it's a gigantic corpora with billions of tokens . and we just say all right deep and learning in these billions of documents co-occur 536 times or something like that . and then we'll just optimize basically this inner product to be closed and it's value to the log of that overall account . and because of that it scales to very large corpora . which is great because the rare words appear not very often and just build hours to capture even rarer like the semantics of very rare words . and because of the efficient usage of the statistics it turns out to also work very well on small corpora and even smaller vector sizes . so now you might be confused because individualization we keep showing you a single vector but here we again just like with the skip gram vector we have v vector it's the outside vectors and the inside vectors . and so let's get rid of that confusion and basically tell you that there are a lot of different options of how you get eventually just a single vector from having these two vectors . you could concatenate them but it turns out what works best is just to sum them up . and if we just sum them that turns out to work best in practice . and so that also destroys certain things should happen but it turns out in practice this works best yeah . v again so u here are again just and so here just like with the skip-gram we had the inside and the outside vectors . 
here u and v are just the vectors in the column and the vectors in the row . they're essentially interchangeable and because of that it makes even more sense to sum them up . you could even say well why don't you just have one set of vectors . but then you'd have a more a less well behaved objective function here because you have the inner product between two of the same sets of parameters . and it turns out in terms of the optimization having the separate vectors during optimization and combining them at the very end just was much more stable . is it common also time for skip-gram to sum them up . and it's a good it's good whenever you have these choices and they seem a little arbitrary also for all your projects . the best thing to always do is like well there are two things . and the true answer especially as you get closer to your project and to more research and novel kinds of applications the best answer is always try all of them . and then have a real metric a quantitative of measure of how well all of them do and then have a nice little table in your final projects description that tells you very concretely what it is . and once you do that many times you'll gain some intuitions and you'll realize alright for the fifth project you just realized well summing them up usually works best so i'm just going to continue doing that . especially as you get into the field it's good to try a lot of these different knobs and hyperparameters . they're all in the same scale here . really they are quite interchangeable especially for the glove model . alright i will try to repeat it . so the question is does the magnitude of these vectors matter . but in the end you will see them basically in very similar contexts a lot of times . and so in this log here they will eventually have to so they will have to go to a certain size of what these log counts usually are . and then the model just figures out that they are in the end roughly in the same place . there's nothing in the optimization that pushes some vectors to get really really large except of course the vectors of words that appear very frequently and that's why we have exactly this term here to basically cap the importance of the very frequent words . 
yes so the question is and i'll just phrase it the way it is which is right . the skip-gram model tries to capture co-occurrences one window at a time . and the glove model tries to capture the counts of the overall statistics of how often these words appear together all right . so now we can look at some fun results . and basically we found the nearest neighbors for frog were all these various words . and we're first a little worried but then we looked them up . and realize alright those are actually quite good . so you'll see here even for very rare words glove will give you very very good nearest neighbors in this space . and so next we will do the evaluation but intermission with arun . so far we've seen that word vectors encode similarity we see that similar concepts are even distributed in euclidean space near each other . and the question i want you to think about is what do we do about polysemy . suppose you have a word like tie . all right tie could mean something like a tie in a game . so maybe it should be near this cluster . it could be a piece of clothing so maybe it should be near this cluster or it could be an action like braid twist should be near this cluster . so this paper by sanjeev arora and the entire group they seek to answer this question . and one of the first things they find is that if you have an imaginary you could split up tie into these polysemous vectors . you had tie one every time you talk about this sport event . then you can show that the actual tie that is a combination of all of these words lies in the linear superposition of all of these vectors . you might be wondering how is this vector close to all of them but that's because we're projecting this into a 2d plane and so it's actually closer to them in other dimensions . 
now that we know that this tie lies near or in the plane of the different senses we might be curious to find out the different senses of a word are . suppose we can only see this word tie could we computationally find out to some core logistics that tie had a meaning about sport clothing etc . so the second thing that they're able to show is that there's an algorithm called sparse coding . i don't have time to discuss exactly what sparse coding how the algorithm works but let me describe the model . the model says that every word vector you have is composed as the sum of a small selected number of what are called context vectors . so these context vectors there are only 2000 that they found for their entire corpus but every word like tie is only composed of a small number of these context vectors . so the context vector could be something like sports etc . there's some noise added in but that's not very important . and so if you look at the type of output that you get for something like tie you see something to do with clothing with sports . very interestingly you also see output about music . some of you might realize that actually makes sense . and now we might wonder how this is qualitative . is there a way we can quantitatively evaluate how good the senses we recover are . so it turns out yes you can and here's the sort of experimental set-up . so for every word that was taken from wordnet a number of about 20 sets of related senses were picked up . so a bunch of words that represent that sense like tie blouse or pants or something totally unrelated like computer mouse and keyboard . and so now they asked a bunch of grad students because they're guinea pigs to differentiate if they could find out which one of these words correspond to tie . and they also asked the algorithm if it could make that distinction . the interesting thing is that the performance of this method that i alluded to earlier is about at the same level as the non-native grad students that they had surveyed . the native speakers do better on the task . 
so in summary word vectors can indeed capture polysemy . it turns out these polysemies the word vectors are in the linear superposition of the polysemy vectors . you can recover the senses that a polysemous word has with sparse coding . and the senses that you that of a non-native english speaker . so now on to evaluating word vectors . and you say well how well does this actually work . and we already came up with these questions . how much does it matter how do we choose them . and these are all the answers now . so in a very high level and this will be true for a lot of your projects as well you can make a high level decision of whether you will have an intrinsic or an extrinsic evaluation of whatever project you're doing . and in the case of word vectors that is no different . so intrinsic evaluations are usually on some specific or intermediate subtask . so we might for instance look at how well do these vector differences or vector similarities and inner products correlate with human judgments of similarity . of these kinds of evaluations in the next couple of slides . the advantage of intrinsic evaluations is that they're going to be very fast to compute . you have your vectors you run them through this quick and you get a number out and you then can claim victory very quickly . and then or you can modify your model and try 50000 different little knobs and combinations and tune this very quickly . it sometimes helps you really understand very quickly how your system works what kinds of hyperparameters actually have an impact on this metric of similarity for instance . it's not clear sometimes if your intermediate or intrinsic evaluation and improvements actually carry out to be a real improvement in some task real people will care about . and real people is a little tricky definition . 
i guess real people normal people who want to just have a machine translation system or a question answering system or something like that . natural language processing researchers in the field . and so sometimes you actually observe people trying to optimize their intrinsic evaluations a lot . and they spent years of their life optimizing them . and other people later find out well it turns out those improvements on your intrinsic task when i actually applied your better word vectors or something to name entity recognition or part of speech tagging or machine translation i don't see an improvement . so then the question is well how useful is your intrinsic evaluation task . so as you go down this route and a lot of you will for their projects you always wanna make sure you establish some kind of correlation between these . now the extrinsic one is basically evaluation on a real task . and that's really where the rubber hits the road or the proof is in the pudding or whatever . you have your new word vectors and you're like i took the pearson correlation instead of the raw count of my core currents matrix . i think that's the best thing ever . now i wanna evaluate whether that word vector really helps for machine translation . and you say all right now i'm gonna take my word vectors and plug them into this machine translation system . and that turns out to take a week to train . and then you have to wait a long time and now you have ten other knobs and before you know it the year is over . and you can't really just do that every time you have a tiny little improvement on your first early word vectors for instance . so that's the problem it takes a long time . and then often people will often make the mistake of tuning a lot of different subsystems . and then they put it all together into the full system the real task like machine translation . and something overall has improved but now it's unclear which part actually gave the improvement . 
maybe two parts where actually one was really good the other one was bad . they cancel each other out and so on . so you wanna basically when you use extrinsic evaluations be very certain that you only change one thing that you came up with or one aspect of your word vectors for instance . and if you then get an improvement on your overall downstream task then you're really in a good place . go through some of these intrinsic word vector evaluations . came out just very recently with the word2vec paper was these word vector analogies . where basically they found which was initially very surprising to a lot of people that you have amazing kinds of semantic and syntactic analogies that are captured through these cosine distances in these vectors . so for instance you might ask what is man to woman and the relationship of king to another word . man to woman is like king to queen . and so it turns out that when you just take vector of woman you subtract the vector of man and you add the vector of king . and then you try to find the vector that has the largest cosine similarity . it turns out the vector of queen is actually that vector that has the largest cosine similarity to this term . and it works for a lot of different kinds of very intuitive patterns . so let?셲 go through a couple of them . so you'd have similar things like if sir to madam is similar as man to woman or heir to heiress or king to queen or emperor to empress and so on . so they all have a similar kind of relationship that is captured very well by these cosine distances in this simple euclidean subtractions and additions . you have similar kinds of companies and their ceo names . and you can take company title minus ceo plus other company and you get to the vector of the name of the ceo of that other company . and it works not just for semantic relationships but also for syntactic relationships so slow slower or slowest in these glove things has very similar kind of differences and so on to short shorter and shortest or strong stronger and strongest . you can have a lot of fun with this and people did so here are some even more fun ones like sushi- japan + germany which as a german i'm mildly offended by . 
and of course it's very intuitive in some ways . maybe it should have been [inaudible] or whatever . while this is very intuitive and for some people in terms of the actual semantics that are captured here you might really wonder why this has happened . and there is no mathematical proof of why this has to fall out but intuitively you can kind of make sense of it a little bit . superlatives for instance might very often in similar kinds of ways . maybe most for instance appears in front of a lot of superlative . or barely might appear in front of certain words like slower or shorter . it's barely shorter than this other person . and since in these vectors you're capturing these core occurrence accounts as you take out basically one concurrence you subtract that one concurrence intuitively it's a little hand wavy . there's no like again here this is not a nice mathematical proof but intuitively you can see how similar kinds of words appeared and you subtract those counts and hence you arrive in similar kinds of places into vector space . now first you try a couple of these and you're surprised that this works well . and then you want to make it a little more quantitative . all right so this was a qualitative sub sample of some words where this works incredibly well . it's also true that when you really play around with it for a while you'll find something things that are like audi minus german goes to some crazy sushi term or something . it doesn't always make sense but there are a lot of them where it really is surprisingly intuitive . and so people essentially then came up with a data set to try to see how often does it really appear and does it really work this well . and so they basically collected this word vector analogies task . you can download all of them on this link here . this is again the original word2vec paper that discovered and described these linear relationships . and they basically look at chicago and illinois and houston texas . 
with a lot of different analogies where this city appears in that state . of course there are some problems and as you optimize this metric more and more you will observe like well maybe that city name actually appears in different states have the same name . and then it kind of depends on your corpus that you're training on whether or not this has been captured or not . but still a lot of people it makes a lot of sense for most of them to optimize these at least for a little bit . here are some other examples of analogies that are in this data set that are being captured and just like the capital and the world of course you know as those change if it doesn't change in your corpus that's also problematic . but in many cases the capitals of countries don't change and so it's quite intuitive and here's some examples of syntactic relationships and analogies that are basically we have several thousands of these analogies and now we compute our word vectors we've tuned some knob we changed the hyperparameter instead of 25 dimensions we have 50 dimensions and then we evaluate which one is better for these analogies . and again here is another syntactic one with past tense kinds of relationships . dancing to danced should be like going to went . now we can basically look at a lot of different methods and we don't know all of these in the class here but we know the skip gram sg and the glove model . and here is the first evaluation that is quantitative and basically looks at the semantic and the syntactic relationships and then just average in terms of the total . and just says how often is exactly this relationship true for all these different analogies that we have here in the data set . and it turns out that when both of these papers came out in 2013 and 14 basically glove was the best at capturing these relationships . and so we observe a couple of interesting things here . one it turns out sometimes more dimensions don't actually help in capturing these relationships better so thousand dimensional vectors work worst than 300 dimensional vectors . another interesting observation and that is something that is somewhat sadly true for pretty much every deep learning model ever is more data will work better . if you train your word it will work better than on 6 billion tokens . here we have the same 300 dimensions . again we only want to change one thing to understand whether that one change actually has an impact . how come the performance sometimes goes down . it turns out it also depends on what you're training your word vectors on . 
it turns out wikipedia for instance is really great because wikipedia has very good descriptions of all these capitals in all the world . but now if you take news and let's say if you take us news and in us news you might not have abuja and ashgabat mentioned very often . well then the vectors for those words will also not capture their semantics very well and so you will do worse . and so some not bigger is not always better it also depends on the quality of the data that you have . and wikipedia has less misspellings than general internet texts and so on . and it's actually a very good data set . and so here are some of the evaluations and we have a lot of questions of like how do we choose this hyperparameter the size and so on . this is i think a very good and careful analysis that geoffrey had done here three years ago on a variety of these different hyperparameters that we've observed and and so this is also a great sort of way that you should try to emulate for your projects . whenever i see plots like this i get a big smile on my face and your grades just like improve right away . you make certain mistakes in your plots . here we look at basically the symmetric context the asymmetric context is where we only count words that have happened after the current word . we ignore the things that's before but it turns out symmetric usually works better and so a vector dimension here is a good one to evaluate . and we basically observe that when they're very small it doesn't work as well in capturing these analogies but then after around 200 300 it actually kind of peters out and then it doesn't get much better . so the main number we often look at here is the overall accuracy and that's in red here . so one mistake you could make when create such a plot is you can prove you have some hyperparameter and you have some kind of accuracy . this could be the vector size and you create a nice plot and you say look things got better . and then my comment if i see a plot like this would be well why didn't you go further in this direction . it seems to just be going up and up . you should find your plots until they actually kind of peter out and you say all right now i really found the optimum value for this hyperparameter . so another important thing to evaluate here is the window's size and there are sometimes considerations around this . 
maybe the 200 worked 300 works slightly better than 200 . but larger word vectors also means more ram right . your software now needs to store more data . and you need to you might want to ship it to the cellphone . and now yes you might get 2% improvement on this intrinsic task . but you also have 30% higher ram requirements . and maybe you say well i don't care about those 2% or so improvement in accuracy on this intrinsic task . i still choose a smaller word vector . so that's a legit argument but in general here we're just trying to optimize this metric . and so we wanna look at carefully what these are . all right now window's size again this is how many words to the left and to the right of each of the center compute the counts for . turns out around eight or so you get the highest . the longer the windows are the more times you have to compute these kind of expressions . and then for asymmetric context it's actually slightly different all right any question around these evaluations . to compare glove and the skip gram model cuz they're very different kinds of training regimes . one goes through the one window at a time the other one first computes all the counts and then works on the counts . so this is kind of us trying to do well and when you compare them directly . so what we did here is we looked at the negative samples . so remember we had that sum and the objective function for the skip gram model of how many words we want to push down the probability of cuz they don't appear in that window and that is one way to increase training time and in theory do better on that objective . versus different iterations of how often do we go over this cocurrence counts to optimize each pair in the cocurrence matrix for glove . 
and in this evaluation glove did better regardless of how many hours you sort of trained both models . and this is more data helps that the argument already made . so here gigaword is i think mostly a news corpus . so news despite being more actually it does not work quite as well overall and especially not for semantic relationships and analogies but common crawl which is a super large data set of 42 billion tokens works best . all right so now these amazing analogies of king minus man plus woman and so on were very exciting . before that people used often just correlation judgements . so basically they asked a bunch of people often grad students to give on a scale of one to ten how similar do you think these two words are . so tiger and cat when you ask three or five humans on a scale from one to ten how similar they are they might say one might say seven the other eight the other six or something like that and then you average . and then you get basically a score here of similarities our computer and internet are seven . but stock and cd are not very similar at all . so a bunch of people will say on a scale from one to ten it's only 1.3 on average . we could try to basically say all right . we want to train word vectors such that the vectors have a high correlation and their distances be it cosine similarity or euclidian distance or you can try different distance metrics too and look at how close they are . you take the word of sweden and you look in terms of cosine similarity and you basically find lots of words that are very very close by or have the largest cosine similarity and you basically get norway and denmark to be very close by . and so if you have a lot of these kinds of data sets and this one wordsim353 has basically 353 such pairs of words . and you can look at how well do your vector distances correlate with these human judgements . so the higher the correlation the more intuitive we would think are the distances in this large vector space . and again glove does very well here across a whole host of different kinds of datasets like the wordsim 353 and again the largest training dataset here did best for glove . any questions on word vector similarities and correlations . now basically intrinsic's evaluations have this huge problem right . 
we have these nice similarities but who knows . maybe that doesn't actually improve the real tasks that we care about in the end . and so the best kinds of evaluations but again they are very expensive are those on real tasks or at least subsequent kinds of downstream tasks . and so one such example is named entity recognition . it's a good one cuz it's relatively simple . you might want to run a named entity recognition system over a bunch of your corporate emails . to understand which person is in relationship to what company and where do they live and the locations of different people and so on . it's actually a useful system to have a named entity recognition system . and basically we'll go doing a named entity recognition in the next lecture . but as we plug in different word vectors into these downstream models that we'll describe in the next lecture we'll observe that for many of them glove vectors again do very very well on these downstream tasks . we'll go through the actual model that works here later . well so you're not optimizing anything here you're just evaluating . you've trained your word vectors with your objective function from skip-gram and you fix them and then you just evaluate them . and so what you're evaluating here now is you look at for instance sweden and norway and they have a certain distance between them and then you want to basically look at the human measure of how similar do humans think these two words are . and then you want these kinds of human judgements of similarity to correlate well with the cosine distances of the vectors . you think the vectors are capturing similar kinds of intuitions that people have and hence they should be good . and again intuitively it would make sense that if sweden has good cosine similarity and you plugged it into some other downstream system that that system will also get better at capturing named entities . because maybe at training time it sees the vector of sweden and at test time it sees the vector of norway and at training time you told that sweden is a location and so a test time it might be more likely to correctly identify norway or denmark also as a location . because they're actually and we'll go actually through example of how we train word vectors and so on in the next lecture . so i think we have until 5:50 so we got 8 more minutes . 
so let's look briefly at simple single word classification . so you know we talked about these word vectors and i basically showed you the difference between starting with these very simple co-occurrence counts and these very sparse large vectors versus having small dense vectors like word2vec . and so the major benefits are basically that because similar words cluster together we'll be able to classify and different kinds of words that we might not see in the training data set . so for instance because countries cluster together and our goal is to classify location words then we'll do better if we initialize all these country words to be in a similar part of the vector space . it turns out later we'll actually fine tune these vectors too . so right now we learned an unsupervised objective function . it's unsupervised in the sense that we don't have human labels that we assigned to each input we just basically took a large corpus of words and we learned with these unsupervised objective functions . but other tasks where that doesn't actually work as well . so for instance sentiment analysis turns out to not be a great downstream task for some word vectors because good and bad might actually appear in similar contexts . i thought this movie was really good or and so when your downstream task is sentiment analysis it turns out that maybe you can just initialize your word vectors randomly . so this is kind of a bummer after listening to us for many hours on how word vectors should be trained . but fret not it's in many cases word vectors are helpful as your first step for your deep learning model just not always . and again that will be something that you can evaluate . can i just initialize my words randomly or the word2vec or the glove model . so as we're trying to classify words what we'll use is the softmax . and so you've seen this equation already in the very beginning in the first slide of the lecture . but we'll change the notation a little bit because all the math that will follow will be easier to go through with this kind of notation . so this is going to be the softmax that we'll optimize . it's essentially just a different word term for logistic regression . and we'll in many cases have generally a matrix w here for our different classes . 
so x for instance could be in a simplest form just a word vector . we're just trying to classify different word vectors with no context of just like are these locations or not . it's not very useful but just for pedagogical reasons let's assume x our input here is just a word vector . and i want to classify is it a location or is it not a location . and then we give it basically these different kinds of word vectors that we compute it for instance for sweden and norway and then we want to classify is also a location yes or no . and so our softmax here might just have in the simplest case two two doesn't really make sense so let's say we have multiple different classes and each class has one row vector here . and so this notation y is essentially the number of rows that we have so the specific row that we have . and we have here inner product with this rho vector times this column vector x . and then we normalize just like we always do for logistic regression to get an overall vector here for all the different classes that sums to 1 . so w in general for classification will be a c by d dimensional matrix . where d is our input and c is the number of classes that we have . and again logistic regression just a different term for softmax classification . and the nice thing about the softmax is that it will generalize well above for multiple different classes . and so basically this is also something we've already covered . so the loss function will use a similar term for all the subsequent lectures . loss function cost function and objective functions we kind of use interchangeably . and what we'll use to optimize the softmax is the cross entropy loss . and so i feel like the last minute i'll just give you one extra minute cuz if we start now it'll be too late . and then we'll do some interesting things with updating these word vectors that we so far have learned in an unsupervised way . we'll update them with some real supervision signals such as sentiment and other things . 
then we'll look at the first real model that is actually useful and you might wanna use in practice . well other than of course the word vectors but one sort of downstream task which is window classification and we'll really also clear up some of the confusion around the cross entropy error and how it connects with the softmax . and then we'll introduce the famous neural network our most basic lego block that we may start to call deep to get to the actual title of this class . and then we'll actually introduce another loss function the max margin loss and take our first steps into the direction of backprop . so this lecture will be i think very helpful for problem set one . we'll go into a lot of the math number two in the problem set . so i hope it'll be very useful and i'm excited for you cuz at the end of this lecture you'll feel hopefully a lot better about the magic of deep learning . all right are there any organizational questions around problem sets or programming sessions with the tas . awesome thanks to the tas for clearing up everything . cool so let's be very careful about our notation today because that is one of the main things that a lot of people trip up over chain-rules and so on . so let's start at the beginning and say all right we have usually a training dataset of some input x and some output y . x could be in the simplest case words in isolation just a single word vector . it's not something you would usually do in practice . but it'll be easy for us to learn that way . so we'll start with that but then we'll move to context windows today . and then eventually we'll use the same basic building blocks that we introduce today for sentences and documents and then complex interactions for everything . now the output in the simplest case it's just a single label . it's just a positive or a negative kind of sentence . it could be the named entities of certain words in their context . it can also be other words so in machine translation for instance you might wanna output eventually a sequence of other words as our yi and we'll get to that in a couple weeks . 
and yeah basically they have multiword sequences as potential outputs . all right so what is the intuition for classification . in the standard machine learning case so not yet the deep learning world we usually just for something as simple logistic regression basically want to define and learn a simple decision boundary where we say everything to the left of this or in one direction is in one class and the other one all the other things in the other class . and so in general machine learning we assume our inputs the xs are kinda fixed they're just set and we'll only train the w parameter so we'll compute the probability of y given the input x with this kind of input . and so one notational comment here is for the whole dataset we often subscript with i but then when i drop the i we're just looking at a single example of x and y . eventually we're going to overload at the subscript a little bit and look at the indices of certain vector so if you get confused i'll try to make it clear which one is which . we mentioned it before but we wanna really carefully define and recall the notation here cuz we'll go and take derivatives with respect to all of these parameters . so we can tease apart two steps here for computing this probability of y given x . the first thing is we'll take the y'th row of w and multiply that row with x . and so again this notation here when we have wy . and that means we'll have we're taking the y'th row of this matrix . and then multiplying it here with x . now if we do that multiple times for all c from one to our classes . so let's say this is 1 2 3 the 4th row and multiply each of these . so then we get four numbers here . and then we'll basically pipe this vector through the softmax to compute a probability distribution that sums to one . cuz it's just gonna keep on going from here . and i get that sometimes in general from previous sort of surveys it seems to be that 15% of the class are usually bored when we go through all of these like all of these derivatives . 15% are super overwhelmed and then the majority of people are like okay it's a good speed i'm learning something i'm getting it and you're making progress . so sorry for the 30% for whom this is too slow or too fast . 
you can probably just skim speed it up if you're watching online . if you're super familiar with taking if it's a little overwhelming then definitely come to all the office hours . we have an awesome set of tas who will help you . all right now we let's look at a single example of an x and y that we wanna predict . in general we want our model to essentially maximize the probability of the correct class . we wanted to output the right class at the end by taking the argmax of that output . and maximizing probability is the same as maximizing log probability it's the same as minimizing the negative of that log probability and that is often our objective function . so why do we call this the cross-entropy error . well we can define the cross-entropy in the abstract in general as follows . so let's assume we have the ground truth or gold or target probability distribution we use those three terms interchangeably . basically what the ideal target in our training dataset the y and we'll assume that that is one at the right class and zero everywhere else . so if we have for instance five classes here and it's the center class . its the third class and this would be one and all the other numbers would be zero . so if we define this as p in our computed probability that our softmax outputs as q then we would define here the cross-entropy is basically this sum over all the classes . and in our case p here is just one-hot vector that's really only 1 in one location and 0 everywhere else . so all these other terms are basically gone . and we end up with just log of q and that's exactly the log of what our softmax outputs all right . and then there are some nice connections to kullback-leibler divergence and so on . i used to talk about it but we don't have that much time today . familiar of this in stats you can see this as trying to minimize the kullback-leibler divergence between these two distributions . 
but really this is all you need to know for the purpose of this class . so this is for one element of your training data set . now of course in general you have lots of training examples . so we have our overall objective function we often denote with j over all our parameters theta . and we basically sum these negative log probabilities of the correct classes that we index here a sub-index with yi . and basically we want to minimize this whole sum . so that's our cross-entropy error that we're trying to minimize and we'll take lots of derivatives off in a lot of the next couple of hours . so this is the general ml case where we assume our inputs here are fixed . so we are not multiplying a vector here so p(c) is the probability for that class so that's one single number . so the cross entropy a single number our main objective that we're trying to minimize or our error that we're trying to minimize . now whenever you write this f subscript y here we don't want to forget that f is really also a function of x our inputs right . it's sort of an intermediate step and it's very important for us to play around with this notation . so we can also rewrite this as w y that row times x and we can write out that whole sum . and that can often be helpful as you are trying to take derivatives of one element at a time to eventually see the bigger picture of the whole matrix notation . all right so often we'll write f here in terms of this matrix notation . so this is our f this is our w and this is our x . so just standard matrix all right now most of the time we'll just talk about this first part of the objective function but it's a bit of a simplification because in all your real applications you will also have this regularization term here . as part of your overall objective function . and in many cases if it's the w matrix of our we'll essentially just try this part of the objective function . we'll try to encourage the model to keep all the weights as small as possible and you can kind of assume if you want as a bayesian that you can have a prior a gaussian distributed prior that says ideally all these are small numbers . 
this regularization term it will start to overfit more and more . and in fact this kind of plot is something that you will very often see in your projects and even in the problem sets . and when i took my very first statistical learning class the professor said this is the number one plot to remember . so i don't know if it's that important but it is very very important for all our applications . and it's basically a pretty abstract plot . you can think of the x-axis as a variety of different things . for instance how powerful your model is . how many deep layers you'll have or how many parameters you'll have . or how many dimensions each word vector has . or how long you trained a model for . you'll see the same kind of pattern with a lot of different x-axis and then the y-axis here is essentially your error . or your objective function that you're trying to optimize and minimize . and what you often observe is the more powerful your model gets the better you are on the better you can fit these x-i y-i pairs . but at some point you'll actually start to over-fit and then your test error or your validation or development set error will go up again . we'll go into a little bit more details on how to avoid all of that throughout this course and in the project advice and so on . but this is a pretty fundamental thing and just keep in mind that for a lot of the implementations and your projects you will want this regularization parameter . but really it's the same one for almost all the objective functions so we're going to chop it and mostly focus on actually fitting our dataset . so basically you can think of this in terms of if you really care about one specific number then you can adjust all your parameters such that it will exactly and if you force it to not do that it will kind of be a little smoother . and be less likely to fit exactly those points and hence often generalize slightly better . and we'll go through a couple of examples of what this will look like soon . 
in general machine learning we'll only optimize the w here the parameters of our softmax classifier . and hence our updates and gradients will only be pretty small so in many cases we only have you maybe our word vectors are hundred so if we have three classes and 100 dimensional word vectors we're trying to classify we'd only have 300 parameters . now in deep learning we have these amazing word vectors . and we actually will want to learn not just the softmax but also the word vectors . we can back propagate into them and we'll talk about how to do that today . hint it's going to be taking derivatives . but the problem is when we update word vectors conceptually as you are thinking through this you have to realize this is very very large . and now all of the sudden have a very large set of parameters right . let's say your word vectors you know 10000 words in your vocabulary . all of the sudden you have an immensely large set of parameters so on this kind of plot you're going and so before we dive into all this optimization i want you to get a little bit of an intuition of what it means to update word vectors . so let's go through a very simple example where we might want to classify single words . again it's not something we'll do very often but let's say you want to classify single words as positive or negative . and let's say in our training data set we have the word tv and telly and say you know this is movie reviews and if you say this movie is better suited for tv . it's not a very positive thing to say about a movie that's just coming out into movie theaters . and so we would assume that in the beginning telly tv and television are actually all we learn something with word2vec or glove vectors and we train these word vectors on a very very large corpus and it learned all these three words appear often in a similar context so they are close by in the vector space . and now we're going to train but our smaller sentiment data set only includes in the training set the x-i y-i as tv and telly and not television . so now what happens as we train these word vectors . well they will start to move around . we'll project sentiment into them and so you now might see telly and tv that's a british dataset so like to move somewhere else into the vector space . and now when we want to test it we would actually now misclassify this word because it's never been moved . 
the take home message here will be that if you have only a very small training dataset . that will allow you especially with these deep models to overfit very quickly you do not want to train your word vectors . you want to keep them fixed you pre-trained them with nice glove or word2vec models on a very large corpus or you just downloaded them from the cloud website and you want to keep them fixed cuz otherwise you will not generalize as well . however if you have a very large dataset it may be better to train them in a way we're going to describe in the next couple of slides . so an example for where you do that is for instance machine translation where you might have many hundreds of megabytes or gigabytes of training data and you don't really need to do much with the word vectors other than initialize them randomly and then train them as part of your overall objective . all right any questions around generalization all right it might still be magical how we're training this so that's what we're gonna describe now . so we rarely ever really classify single words . really what we wanna do is classify words in their context . and there are a lot of fun and interesting . issues that arise in context really that's where language begins and grammar and the connection to meaning and so on . so here a couple of fun examples of where context is really necessary . so for instance we have some words that actually auto-antonyms so so for instance to sanction can mean to permit or to punish . and it really depends on the context for you to understand which one is meant or to seed can mean to place seeds or to remove seeds . so without the context we wouldn't really understand the meaning of these words . and in one of the examples that you'll see a lot which is named entity recognition let's say we wanna find locations or people names we wanna identify is this the location or you may also have things like paris which could be paris in france or paris hilton . and you might have paris staying in paris and you still wanna understand which one is which . or if you wanna use deep learning for financial trading and you see hathaway you wanna make sure that if it's just a positive movie review from anne hathaway . you're not all the sudden buying stocks from berkshire hathaway right . and so there are a lot of issues that are fun and interesting and complex that arise in context . and so let's now carefully walk through this first useful model which is window classification . 
so we'll use as our first motivating example here 4-class named entity recognition where we basically wanna identify a person or location or organization or none of the above for every single word in a large corpus . and there are lots of different possibilities that exist . but we'll basically look at the following model . which is actually quite a reasonable model . and also one that started in 2008 . so the first beginning by collobert and weston a great paper to do the first kind of useful state of the art text classification and word classification context . so what we wanna do is basically train a softmax classifier by assigning a label to the center word and then concatenating all the words in a window around that word . so let's take for example this subphrase here from a longer sentence . we basically wanna classify the center word here which is paris in the context of this window . and we'll define the window length as 2 . 2 being 2 words to the left and 2 words to the right of the current center word that we're trying to classify . this whole window as the concatenation of these five word vectors . and just in general throughout all of this lecture all my vectors are going to be column vectors . sadly in number two of the problem set they're row vectors . eventually all these programming frameworks they're actually row-wise first and so it's faster in the low-level optimization to use row vectors . for a lot of the math it's actually i find it simpler to think of them as column vectors so . we're very clear in the problem set but so basically we'll define this here as one five d dimensional column vector . so we have t dimensional word vectors we have five of them and we stack them up in one column all right . now the simplest window classifier that we could think of is to now just put the softmax on top of this concatenation of five word vectors and we'll define this our x here . our inputs is just the x of the entire window for this concatenation . 
and we have the softmax on top of that . and so this is the same with sadly the subscript y for the correct current class . it's tough i went through [laugh] several iterations it's tough to have like prefect notation that works through the entire lecture always . so our overall objective here is again this whole sum over all these probabilities that we have or negative log of those . so now the question is how do we update these word vectors x here . one x is a window and x is now deep inside the softmax . all right well the short answer is we'll take a lot of derivatives . but the long answer is you're gonna have to do that a lot in problem set one and maybe in the midterm . so let's be a little more helpful and actually go through some of the steps and give you some hints . so some of this you'll actually have to do in your problem set so i'm not gonna go through all the details . but i'll give you a couple of hints along the way and then you can know if you're hitting those and then you'll see if you're on the right track . so step one always very carefully define your variables their dimensionality and everything . so y hat will define as the softmax probability of the vector . so the normalized scores or the probabilities for all the different classes that we have . vector where it's all zeroes except at the ground truth index of the class y where it's one . and we'll define our f here as f of x again which is this matrix multiplication . which is going to be a c dimensional vector where capital c is the number of classes that we have all right . carefully define all of your variables and keep track of their dimensionality . it's very easy when you implement this and you multiply two things and they have wrong dimensionality and you can't actually legally multiply them you know you have a bug . and you can do this also in a lot of your equations . 
but maybe at the end you have some time . and you could totally grade it by yourself in the first pass by just making sure that all your dimensionality of your matrix and vector multiplications are correct . all right the second tip is the chain rule we went over this before but i heard there's a little bit of confusion still in the office hours . so let's define this carefully for a simple example and then we'll go and give you a couple more hints also for more complex example . so again if you have something very simple such as a function y which you can defined here as f of u and u can be defined as g of x as in the whole function y of x can be described as f of g of x then you would basically multiply dy u times the udx . and so very concretely here this is sort of high school level but we'll define it properly in so here you can basically define u as g(x) which is just the inside in the parentheses here so x cubed + 7 . it can have y as a function of f(u) where we use 5 times u just replacing the inside definition here . so it's very simple just replacing things . and now we can take the derivative with respect to u and we can take the derivative with respect to x(u) . and then we just multiply these two terms and we plug in u again . so in that sense we all know in theory the chain rule . but now we're gonna have the softmax and we're gonna have lots of matrices and so on . so we have to be very very careful about our notation . and we also have to be careful about understanding which parameters appear inside what other higher level elements . so f for instance is a function of x . so if you're trying to take a derivative with respect to x of this overall soft max you're gonna have to sum over all of the different classes inside which x appears . and you'll see here this first application but not just of fy again this is just a subscript the y element of the effector which is the function of x but also multiply it then here by this . so when you write this out another tip that can be helpful is for this softmax part of he derivative is to actually think of two cases . one where c = y the correct class and one where it's basically all the other incorrect classes . and as you write this out you will observe and come up with something like this . 
so don't just write that as your thing you have to put in your problems the steps on how to get there . bur basically at some point you observe this kinda pattern when you now try to look at all the derivatives with respect to all the elements of f . and now when you have this you realize okay at the correct class we're actually subtracting one here and all the incorrect classes you will not do anything . now the problem is when you implement this it kind of looks like a bunch of if statements . if y equals the correct class for my training set then subtract 1 that's not gonna be very efficient . also you're gonna go insane if you try to actually write down equations for more complex neural network architectures ever . and so instead what we wanna do is always try to vectorize a lot of our notation as well as our implementation . in this case is you can actually observe that well this 1 is exactly 1 where t our hot to target distribution also happens to be 1 . and so what you're gonna wanna do is basically describe this as y(hat)- t so it's the same thing as this . and don't worry if you don't understand how we got there cuz that's part of your problem set . you have to at some point see this equation while you're taking those derivatives . and now the very first baby step towards back-propagation is actually to define this term in terms of a simpler single variable and we'll call this delta . we'll get good we'll become good friends with deltas because they are sort of our error signals . when you start with this chain rule you might want to sometimes use explicit sums before and look at all the partial derivatives . and if you do that a couple of times at some point you see a pattern and then you try to think of how to extrapolate from those patterns of single partial derivatives into vector and matrix notation . so for example you'll see something like this here in at some point in your derivation . so the overall derivative with respect to x of our overall objective function for one element for one element from our training set x and y is this sum . and it turns out when you you take here this row vector but then you transpose it and becomes an inner product well if you do that multiple times for all the c's and you wanna get in the end a whole vector out it turns out you can actually just re-write the sum as w transpose* the delta . so this is one error signal here and we multiply the transpose of our softmax weights with this . and again if some of these are not clear and you're confused and then you'll see that it's really just re-write this in vector notation . 
all right now what is the dimensionality of the window vector gradient . so in the end we have this derivative of the overall cost here for one element of our training set with respect to x . all right so each say we have a window of five words . now what should be the dimensionality of this derivative of this gradient . that's right it's five times the dimensionality . and that's another really good way and one of the reasons we make you implement this from scratch if you have any kinda parameter and you have a gradient for that parameter and they're not the same dimensionality you'll also know you screwed up and there's some mistake or bug in either your code or your map . and way to check your own equations . so the final derivative with respect to this window is now this five vector because we had five d-dimensional now of course the tricky bit is you actually wanna update your word vectors and not the whole window right . the window is just this intermediate step also . so really what you wanna do is update and take derivatives with respect to each of the elements of your word vectors . and so it turns out very simply that can be done by just splitting that error that you've got on the gradient overall at the whole window and that's just basically the concatenation of the reduced of all the different word vectors . and those you can use to update your word vectors as you train the whole system . is there a mathematical notation for the word vector t other than it's just variable t . or that seems like a fine notation . you can see this as a probability distribution that is very peaked . just a single vector with all zeroes except in one location . you can always just write out and it's also something very important . you always wanna define everything so that you make sure that the tas know that you're thinking about the right thing as you're writing out your derivatives you write out the dimensionality you define them properly you can use dot dot dot if it's a larger dimensional vector . target distribution [inaudible] do we still have two vectors for each word . we essentially when we did glove and word2vec and had these two u's and v's for all subsequent lectures from now on we'll just assume we have the sum of u and v and that's our single vector x for each word . 
so the question is does this gradient appear in lots of other windows and it does . so if you the answer is yes . if you have the word "in" that vector here and the gradients will appear in all the windows that have and same with museums and so on . and so as you do stochastic gradient descent you look at one window at a time you update it then you go to the next window you update it and so on . now let's look at how we update these concatenated word vectors . so basically as we're training this if we train it for instance with sentiment we'll push all the positive words in one direction and the other words in other direction . if we train it for named entity recognition and eventually our model can learn that seeing something like in as the word just before the center word would be indicative for that center word to be a location . so now what's missing for training this full window model . well mainly the gradient of j with respect to the softmax weights w . and so we basically will take similar steps . we'll write down all the partial derivatives with respect to wij first and so on . and then we have our full gradient for this entire model . and again this will be very sparse and you're gonna wanna have some clever ways of implementing these word vector updates . so you don't send a bunch of zeros around at every single window cuz each window will only have a few words . so in fact it's so important for your code in the problem set to think carefully through your matrix implementations that it's worth to spend two or three slides on this . so there are essentially two very expensive operations in the softmax . actually later in the lecture we'll find a way to deal with the exponent . but the matrix multiplication can also be implemented much more efficiently . so you might be tempted in the beginning to think this is probability for this class and this is the probability for that class . and so implemented a for loop of all my different classes and matrix multiplications one row at a time . 
so let's go through some very simple python code here to show you what i mean . so essentially always looping over these word vectors instead of concatenating everything into one large matrix . so let's assume we have 500 windows that we want to classify and let's assume each window has a dimensionality of 300 . these are reasonable numbers and let's assume we have five classes in our softmax . and so at some point during the computation we now have two options . so w here are weights for the softmax . now the word vectors here that you concatenated for each window . we can either have the list of a bunch of separate word vectors or we can have one large matrix so d many rows and n many windows . so we have 500 windows so we have 500 columns here in this 1 matrix . and now essentially we can multiply the w here for each vector separately or we can do this one matrix multiplication entirely . and you literally have a 12x speed difference . and sadly with these larger models one iteration or something might take a day eventually for more complex models large data sets . so the difference is between literally 12 days or 1 day of you iterating and making your deadlines and everything . so it's super important and now sometimes people are tripped up by what does it mean to multiply and do this here . essentially it's the same thing that we've done here for one softmax but what we did is we actually concatenated . a lot of different input vectors x and so we'll get a lot of different unnormalized scores out at the end . and then we can tease them apart again for so you have here c times t dimensional matrix for the d dimensional input . so using the same notation yeah dimensional of each window times d times n matrix to get a c times n matrix . so these are all the probabilities here for any questions around that . so it's super important all your code will be way too slow if you don't do this . 
and so this is very much an implementation trick . and so in most of the equations we're not gonna actually go there cuz that makes everything more complicated . and the equations look at only a singular example at a time but vectorize all your code . yeah matrices are your friend use them as much as you can . also in many cases especially for this problem set where you really understand the nuts and bolts of how to train and optimize your models . you will come across a lot of different choices . it's like i could implement it this way or that way . and you can go to your ta and ask should i implement this way or that way . but you can also just use time it as your magic python and just let make a very informed decision and gain intuition yourself . and just basically wanna options that you have in your code a lot of the time . all right so this is was just a pure softmax and now the softmax alone is not play powerful . because it really only gets with this linear decision boundaries in your original space . training data that could be okay and you kind of used a not so powerful model almost as an abstract regularizer . but with more data so if we have here a bunch of words and we don't wanna update our word vectors softmax would only give us this linear decision boundary which is kind of lame . and it would be way better if we could correctly classify these points here as well . and so basically this is one of the many motivations for using neural networks . cuz neural networks will give us much more complex decision boundaries and allow us to fit much more complex functions to our training data . and you could be snarky and actually rename neural networks which sounds really cool . just wouldn't have quite the same ring to it but it's essentially what they are . so let's define how we get from the symbol of logistic regression to a neural network and beyond and deep neural nets . 
so let's demystify the whole thing by starting defining again some of the terminology . and we can have more fun with the math and then one and a half lectures from now . we can just basically use all of these lego blocks . so bear with me this is going to be tough . and try to concentrate and ask questions if you have any cuz we'll keep building now a pretty awesome large model that's really useful . so we'll have inputs we'll have a bias unit we'll have an activation function and output for each single neuron in our larger neuron network . so let's define a single neuron first . basically you can see it as a binary logistic regression unit . we're going to have inside again a set of weights that we have in a product with our input . so we have the input x here to this neuron . and in the end we're going to add a bias term . so we have an always on feature and that kind of defines how likely should this neuron fire . and by firing i mean have a very high probability that's close to one . and f here is always from now on going to be this element wise function . in our case here the sigmoid that just squashes whatever this sum gives us in our product plus the bias term and basically just squashes it to be between 0 and 1 . all right so this is the definition of the single neuron . now if we feed a vector of inputs through all this different little logistic regression functions and neurons we get this output . and now the main difference between just predicting directly a softmax and deep learning is that we'll actually not force this to give directly the output . but they will themselves be inputs to yet another neuron . and it's a loss function on top of that neuron such as cross entropy that will now govern what these intermediate hidden neurons . 
or in the hidden layer what they and the model can decide itself what it should represent how it should transform this input inside these hidden units here in order to give us a lower error at the final output . and it's really just this concatenation of these hidden neurons these little binary logistic regression units that will allow us to build very deep neural network architectures . now again for sanity's sake we're going to have to use matrix notation cuz all of this can be very simply described in terms of matrix multiplication . so a1 here is where going to be the final activation of the first neuron a2 in second neuron and so on . so instead of writing out the inner product here or writing even this as an inner product plus the bias term we're going to use matrix notation . and it's very important now to pay attention to this intermediate variables that we'll define because we'll see these over and over again as we use a chain rule to take derivatives . so we'll define z here as w so we'll basically have here as many bias terms and this vector has the same dimensionality as the number of neurons that we have in this layer . and w will have number of rows for the number of neurons that we have times number of columns for the input dimensionality of x . and then whenever we write a of f(z) what that means here is that we'll actually apply f element wise . so f(z) when z is a vector is just f(z1) f(z2) and f(z3) . and now you might ask well why do we have all this added complexity here with this sigmoid function . later on we can actually have other kinds of so called non linearities . this f function and it turns out that if we don't have the non-linearities in between and we will just stack a couple of this linear layers together it wouldn't add a very different function . in fact it would be continuing to just be a single linear function . and intuitively as you have more hidden neurons you can fit more and more complex functions . so this is like a decision boundary you can think of it also in terms of simple regression . if you had just a single hidden neuron you kinda see here almost an inverted sigmoid . if you have three hidden neurons you could fit this kind of more complex functions and with ten neurons each neuron can start to essentially over fit and try to be very good all right now let's revisit our single window classifier and instead of slapping a softmax directly onto the word vectors we're now going to have an intermediate hidden layer between the word vectors and the output . and that's when we really start to gain an accuracy and expressive power . so let's define a single layer neural network . 
we have our input x that will be again of multiple word vectors . we'll define z and we'll define a as element wise on the areas a and z . and now we can use this input to our final classification layer . far was the softmax but let's not rederive the softmax . we've done it multiple times now you'll do it again in a problem set and introduce an even simpler one and walk through all the glory details of that simple classifier . and that will be a simple unnormalized score . essentially be the right mechanism for various simple binary classification problems where you don't even care that much about this probability z is 0.8 . you really just cares like is it one is it in this class or is it not . and so we'll define the objective function for this new output layer in a second . well let's first understand the feed-forward process . and well feed-forward process is what you will end up using a test time and for before you can take derivative . always be feed-forward and then backward to take the derivatives . so what we wanna do here is for example take basically each window and then score it . and say if the score is high we want to train the model such that it would assign high scores to windows where the center word is a named entity location . such as paris or london or germany or stanford or something like that . now we will often use and you'll see a in a lot of papers this kind of graph so it's good to get used to it . there are various other kinds and we'll try to introduce them slowly throughout the lecture but this is the most common one . so we'll define bottom up what each of these layers will do and then we'll take the derivatives and learn how to optimize it . now x window here is the concatenation of all our word vectors . so let's hear and i'll ask you a question in a second let's try to figure out the dimensionality here of all our parameters so that you're i know you're with me . 
vectors here is four dimensional and we have five of these word vectors in each window that are concatenated . so x is a 20 dimensional vector . and again we'll define it as column vectors . and then lets say we have in our first hidden layer lets say we have eight units here . so you want an eight unit hidden layer as our intermediate representation . and then our final scores just of our w given what i just said . we have one more transfer so it's going to be eight rows and 20 columns right . and you can always whenever you're unsure and you have something like this then this will have some n times d . and then multiply this and then this will have this will always be d and so these two always have to be the same right . so all right now what's the main intuition behind this extra layer especially for nlp . well that will allow us to learn non-linear interactions between these different input words . whereas before we could only say well if in appears in this location always increase the probability that the next word is a location . now we can learn things and patterns like if in is in the second position increase the probability of this being the location only if museum is also the first vector . so we can learn interactions between these different inputs . so do i have a second w there . so the second layer here the scores are unnormalized so it'll just be u and because we just have a single u this will just be a single column vector and we'll transpose that to get our inner product to get a single number out for the score . sorry yeah so the question was so yeah that's in some sense our second matrix but because we only have one hidden neuron in that layer we only need a single vector . all right so now let's define the max-margin loss . it's actually a super powerful loss function often is even more robust than the cross entropy error in softmax and is quite powerful and useful . basically you want to give a high score to windows where the center word is a location . 
and we wanna give low scores to corrupt or incorrect windows where the center word is not a named entity location . so museum is technically a location but it's not a named entity location . and so the idea for this training objective of max-margin is to essentially try to make the score of the true windows larger than the ones of the corrupt windows smaller or lower . and we define good enough as being different by the value of one . and this one here is a margin . you can often see it as a hyperparameter too and set it to m and try different ones but this is continuous and we'll be able to use sgd . so now what's the intuition behind the softmax sorry the max-margin loss here . if you have for instance a very simple data set and you have here a couple of training samples . and here you have the other class c what a standard softmax may give you is a decision boundary that looks like this . most standard softmax classifiers will be able to perfectly separate these two classes . and again this is just for illustration in two dimensions . these are much higher dimensional problems and so on . but a lot of the intuition carries through . so now here we have our decision boundary and this is the softmax . now the problem is maybe that was your training data set . but your test set actually might include some other ones that are quite similar to those stuff you saw at training but a little different . in contrast to this what the max margin try to increase the margin between the closest points of your training data set . so if you have a couple of points here and you have different points here . we'll try to maximize the distance between the closest points here and essentially be more robust . so then if at test time you have some things that are kinda similar but not quite there you're more likely to also correctly classify them . 
so it's a really great lost or objective function . now in our case here when we say a sc for one corrupt window . in many cases in practice we're actually going to have a sum over multiple of these . and you can think of this similar to the skip-gram model where we sample randomly so you really only need for this kind of training a bunch of true examples of this is a location in this context . and then all the other windows where you don't have that as your training data are essentially part of your negative class . all right any questions around the max-margin objective function . we're gonna take a lot of derivatives of it now . that's right is the corrupt yes that's exactly right . so you can think of any other window that doesn't have as its center location just as the other class . all right now how do we optimize this . we're going to take very similar steps to what we've done with cross entropy but now we actually have this hidden layer and we'll take our second to last step towards the full back-propagation algorithm which we'll cover in the next lecture . so let's assume our cost j here is larger than 0 . in the very beginning you will initialize all your parameters here again . either randomly or maybe you'll initialize your word vectors to be reasonable . but they're not gonna be quite perfect at learning in this context in the window what is location and what isn't . and so in the beginning all your scores are likely going to be low cuz all our parameters u and w and b have been initialized to small random numbers . and so i'm unlikely going to be great at distinguishing the window with a correct location at center versus one that is corrupt . and so basically we will be in this regime . after a while of training eventually you're gonna get better and better . and then intuitively if your score here for instance of the good window is five and one of the corrupt is just two then you'll see 1- 5 + 2 is less than 0 so you just basically have 0 loss on those elements . 
and that's another great property of this objective function which is over time you can start ignoring more and more of your training set cuz it's good enough . error to these examples and so you can start to focus on your objective function only on the things that the model still has trouble to distinguish . all right so let's in the very beginning assume most of our examples will j will be larger than 0 for them . and so what we're gonna have to do now is take derivatives with respect to all the parameters of our model . those are u w b and our word vectors x . so we always start from the top and then we go down because we'll start to reuse different elements and just the simple combination of taking derivatives and lead us to back propagation . so derivative of s with respect to u . s was just u transpose times a and so we all know that derivative of that is just a . so that was easy first element first derivative super straight forward . now it's important when we take the next derivative to also be aware of all our definitions . how we define these functions that so s is basically u transpose a a was f(z) and z was just wx + b . all right it's very important to just keep track . that's like almost 80% of the work . now let's take the derivative like i said first partial of only one element of w to gain intuitions . and then we can put it back together and have a more complex matrix notation . so we'll observe for wij that it will actually only appear in the ith activation of our hidden layer . so for example let's say we have a very simple input with a three dimensional x . then we'll observe that if we take the derivative with respect to w23 . so the second row and the third column of w well that actually only is needed in a2 . you can compute a1 without using w23 . 
that means if we take we really only need to look at the ith element of the vector a . and hence we don't need to look at this whole inner product . well as we're taking derivatives with w we need to be again aware of where does w appear and all the other parameters are essentially constant . so u here is not something we're taking a derivative off . so what we can do is just take it out just as like a single number right . we'll just get it outside put the derivative inside here . so a subscript i so that's where wij appears . now ai was this function so why don't we just write this carefully out and now this is first application of the chain rule with derivative of ai with respect to zi and then zi with respect to wij . and then end of it it looks kind of overwhelming but each step is very clear . and each step is simple we're really writing out all the glory details . so application of the chain rule well ai is just f of zi and f was just an element y function on a single number zi . so we can just rewrite ai with its definition of f of zi and we keep this one intact all right . and now derivative of f we can just for now assume is f prime . we'll just define this as f prime for now . it's also just a single number so no harm done . now we're still in this part here the derivative of zi with respect to wij . well let's define what zi was zi was just here . the w of the ith row times x plus the ith element of b . so let's just replace zi with it's definition . so we have our f prime and we have now the derivative with respect to wij of just this inner product here . 
and we can again very carefully write out well the inner product is just this that's just the sum and now when we take the derivative with respect to wij all the other ws are constants . they fall out and so basically it's only the xk the only one that actually appears so basically this derivative is just xj . all right so now we have this whole expressions of just taking carefully chain rule multiplications definitions of all our terms and so on . and now basically what we're gonna want to do is simplify this a little bit cuz we might want to reuse different parts . and so we can define this first term here actually happens to only use subindices i . and it doesn't use any other subindex . so we'll just define uif prime of zi for all the different is as delta i . at first notational simplicity and and one thing that's very helpful for you to do is actually look at also the derivative of the logistic function here . which can be very conveniently computed in terms of the original values . and remember f of z here or f of zi of each element is always just a single number . so we wanna ideally use hidden activation functions that are very fast to compute . and here we don't need to compute we're not gonna recompute f of zi cuz we already did that in the forward propagation step . all right now we have the partial derivative here with respect to one element of w . but of course we wanna have the whole gradient for the whole matrix . so now the question is with the definitions of this delta i for i of this matrix and xj for all the different elements of the input . what would be a good way of trying to combine all of these different elements to get a single gradient for the whole matrix w if we have two vectors . so essentially we can use delta times x transpose namely the outer product to get all the combinations of all elements i and all elements j . and so this again might seem but if you just think again of the definition of the outer product here . and you write it out in terms of all the indices you'll see that turns out to be exactly what we would want in one very nice very simple equation . so we can kind of think of this delta term actually as the responsibility of the error signal that's now arriving from our overall loss into this layer of w . 
and that will eventually lead us to flow graphs . and that will eventually lead us to you not having to actually go through all this misery of taking all these derivatives . but this is really the nuts and bolts of how this works yeah . yeah the question is this outer product will get all the elements of i and j . so when we have delta times x transposed . then now we have basically here x is usually this vector . so now let's take the right notation . so we wanna have derivative with respect to w . w was a 2x3 dimension matrix for example 2x3 . we should be very careful of our notation . so now the derivative of j with respect to our w has to in the end also be a 2x3 matrix . and if we have delta times x transposed then that means we'll have to have a two-dimensional delta which is exactly the dimensions that are coming in . mentions that we have for the number of hidden units that we have . times this one dimensional basically row vector times xt which is a 1 x 3 dimensional vector that we transpose . well that's basically multiplying now standard matrix multiplication . so now the last term that we haven't taken derivatives of off the [inaudible] is our bi and it'll eventually be very similar . we can pull ui out we're going to take f prime assume that's the same . so now this is our delta i . these are very similar steps for bi . but in the end we're going to just end up with this term and that's just going to be one . 
and so the derivative of our bi element here is just delta i and we can again use all the elements of delta to have the entire gradient for the update of b . excellent so this is essentially almost back-propagation . we?셶e so far only taken derivatives and using the chain rule . and first thing when i went through this this is like a lot of the magic of deep learning is just becoming a lot clear . we?셶e just taken derivatives we have an objective function and then we update the parameters of these large functions . now the main remaining trick is to re-use derivatives that we've computed for the higher layers in computing derivatives for the lower layers . you could not use it and it would just be very very inefficient to do . but this is the main insight of why we re-named taking derivatives as back propagation . so what is the last derivatives that we need to take . for this model well again it's in terms of our word vectors . so let's go through all of those . basically we'll have to take the derivative of the score with respect to every single element of our word vectors . where again we concatenated all and now the problem here is that each word vector actually appears in both of these terms . and both hidden units use all of the elements of the input here . so we can't just look at a single element . we'll really have to sum over both of the activation units in the simple case here where we just have two hidden units and three dimensional inputs . keeps it a little simpler and there's less notation . so then we basically start with this . i have to take derivatives with respect to both of the activations . and now we're just going to go through similar kinds of steps . 
we defined s as u transpose times our activation . that was just ui then ai was just f of w and so on . now what we'll observe as we're going through all these similar steps again is that we'll actually see the same it's ui x f prime of zi . and what that means is and that's really one of the big insights . fairly trivial but very exciting but what's still different now is that of course we have to take the the derivative with respect . to each of these to this inner product here in xj where we basically dumped the bias term cuz that's just a constant when we were taking this derivative . and so this one here again xj is just inner product it's the jth element of this matrix this inner product let me take the derivative . so now we have this sum here and now comes again this tricky bit of trying to simplify this sum into something simpler in terms of matrix products . and again the reason we're getting towards back propagation is that we're reusing here these previous error signals and elements of the derivative . now the simplest the first thing we'll observe here as we're doing this sum is that sum is actually also a simple inner product where we now take the jth column . so this again this dot notation when the dot is after the first and next we take the row here we take the column . but then of course we transpose it so it's a simple inner product for getting us a single number . just the derivative of this element of the word vectors and the word window . so once we have the derivatives for all these different variables what's the sequence in which we update them and update them all in parallel . we just take one step in all the elements that we now had a variable in or have seen that parameter in . and the complexity there is in standard machine learning you'll see in many models just like you see all your parameters like your w in all the examples . and ours it's a little more complex because most words you won't see in a specific window and so you only update the words that you see in that window . and if you assumed all the other ones you'd just have very very large quite sparse updates and that's not very ram efficient great question . so now we have this simple multiplication here and the sum is just is just inner product . so far so simple and we have our d dimension vector which i mentioned is two dimensions . 
we have the sum over two elements . now really we would like to get the full gradient here with respect to all xjs for j equals one to three and its simple case or five d if we have a five word large window . so now the question is how do we combine this single element here . into a vector that eventually gives us all the different gradients for all the xij . and j equals 1 to however long our window is is anybody follow along this closely . so basically our final derivative and final gradient here for . our score s with respect to the entire window is just w transpose times delta . super simple very fast to implement i can easily think about how to vectorize this again by concatenating multiple deltas from multiple windows and so on . and it can be very efficiently all right now the error message is delta that arrives at this hidden layer has of course the same dimensionality as its hidden layer because we're updating all the windows . and now from the previous slides we also know that when we update a window it really means we now cut up that final gradient here into the different chunks for each specific word in that window and that's how we update our first large neural network . so let's put all of this together again . so our full objective function here was this max and i started out with saying let's assume it's larger than zero so you have this identity here . so this is simple indicator function if . the indication is true then it's one and if not it's zero . and then you can essentially ignore that pair of correct and corrupt windows x and xc respectively . so our final gradient when we have these kinds of max margin functions is essentially implemented this way . and we can very efficiently multiply all of this stuff . so this is just that this is not right . this is our [inaudible] but you still have to take the derivative here but basically this indicator function is the main novelty that we haven't seen yet . the gist of the question is how to we make sure we don't get stuck in local optima . 
and you've kinda answered it a little bit already which is indeed because of the stochasticity you keep making updates anyway it's very hard to get stuck . in fact the smaller your the more stochastic you are as in the fewer windows you look at each time you want to make an update the less likely you're getting stuck . if you had tried to get through all the windows and then make one gigantic update so it's actually very inefficient and much more likely to get you stuck . and then the other observation that it's just slowly coming through some of the theory that we couldn't get into this class . is that it turns out a lot of the local optima are actually pretty good . and in many cases not even that far away from what you might think the global optima would be . also you'll observe a lot of times and we'll go through this in some of the project advice in many cases you can actually perfectly fit . we have a powerful enough neural network model . you can often perfectly fit your input and your training dataset . and you'll actually eventually spend most of your time thinking about how to regularize your models better and often at least even more stochasticity . yeah in the end we just have all these updates and it's all very simple . congrats again this was our super useful basic components lecture . and now this window model is actually might observe and practice and you might actually want to implement . so to recap we've learned word vector training we learned how to combine windows . we have the softmax and the cross entropy error and we went through some of the details there . have the scores and the max margin loss and we have the neural network and it's really these two steps here that you have to combine differently for problem set . number one and especially number two in that . so we just have one more math heavy lecture and after that we can have fun and combine all these things together . and i promise you as the last bit of super heavy math- after that you can have a warm fuzzy feeling around most of the state of the art deep learning techniques . both for natural language processing and even a little bit for computer vision a lot of other places . 
so i know this can be a lot if you're not super familiar with multivariate calculus . and so i'll actually describe backprop in four different ways today . and hopefully bring most of you into the backprop the group of people who know so 4 different descriptions of essentially the same thing . but hopefully some will resonate more with some folks than others . and so to show you that afterwards we can have a lot more fun . well actually then the second sort of well not quite half but maybe the last third talk about the projects and encourage you to get started on the project . give you some advice on what the projects will likely entail if you choose to do a project instead of the last problem set . maybe one small hint for problem set one . again it's super important to understand the math and dimensionality and if you do that on paper and then you still have some trouble in the implementation . it can be very helpful to essentially set break points and then print out the shape of all the various derivatives you may be computing debugging process a little bit . all right are there any questions around organization problem sets one no . all right my project my office hours going to be a half hour after the class ends today . so if you have project questions i'll be there after the class . all right let's go to explanation number 1 for back propagation . and again just to motivate you of why we want to go through this . why do i torture some of you with all these derivatives . it is really important to have an actual understanding of the math behind most of deep learning . and in many cases in the future you will kind of abstract the way backpropagation . works based on a framework software package that you might use . but that sometimes leads you to not understand why your model might not be working right . 
in theory you say it's just abstracted away i don't have to worry about it anymore . but really in practice in the optimization you might run into problems . and if you don't understand the actual back propagation you don't know why you will have these problems . and so in addition to that we kinda wanna prepare you to not just be a user of deep learning but maybe even eventually do research . in this field and maybe think of and implement and be very very good at debugging completely new kinds of models . and you'll observe that depending on which software package you'll use in the future not everything is de facto supported in some of these frameworks . so if you want to create a completely new model that's sort of outside the convex known things you will need to implement the forward and the backward propagation for a new sub-module that you might have invented . a little bit in why it's useful . so last time we ended with this kind of neural network where we had a single hidden layer . and we derive all the gradients for all the different parameters of this model namely the word vectors here . the w the weight matrix for our single hidden layer and the u for the simple linear layer here . and we defined this objective function and we ended up writing for instance one such derivative here fully out where we have the indicator function whether we're in this regime or if it's zero . and if it's above zero in here i just rewrote the same derivative twice essentially showing you that instead of having to recompute this if you had basically stored during forward propagation the activations of this this is exactly the same thing so f(wx + b) we defined as our hidden activation a then you could reuse those to compute derivatives . alright now we're going to take it up a notch and add an additional hidden layer to that exact same model and it's the same kind of layer but in out that we have 2 we have to be very careful here about our superscript which will indicate . so it's the same kind of window definition we'll go over corpus we'll select samples for our positive class and everything that doesn't for instance have an entity will be our negative class . everything else is the same but we're adding one hidden layer to this . and so let's go through the definition . we'll define x here our windows and our word vectors that we concatenated as our first activations our first hidden layer . and now to compute to intermediate representation for our second layer just a linear part of that we basically have here w1 . a superscript matrix times x plus b1 . 
and then to compute the activations a superscript two of that will apply the element-wise nonlinearities all right and then we'll define this here as z3 same idea but this could potentially have different dimensionalities w1 w2 for instance don't have to have exactly the same dimensionality . do the same thing again we have the same linear layer at the top . all right are there any questions around the definition of this here . wise functions here have to be the same . and the answer is they do not . that you could cross-validate and try as different hyper-parameters of the model . we so far have only introduced to you the sigmoid . so let's assume for now it's the same . but in a later lecture i think next week kinds of non-linearities . how do you choose which of these functions . we'll go into all of that once what the options are . the best answer usually is you let your data speak for yourself and you run experiments with a lot of different options . once you do that after a while you gain again certain intuitions . and you don't have to redo it every time . especially if you have ten layers you don't wanna go through the cross-product of five different nonlinearities . usually you get diminishing returns for some of those type of parameters . so the question iscould we put the b into the w . and if that's confusing you could essentially assume that b is this biased term here . is another element of this w matrix if we add a single one to every so if a two frame since we just added one here then we could get rid of this bias term and we'd have an additional row or column depending on what we have in w . so yes we could fold b into w then as we're taking derivatives we want to keep everything separate and clear . 
and you'll usually back propagate through these activations whereas you don't back propagate through b . so for this math it's better to keep them separate . so u transpose is our last the question is what's u transposed . and u transpose is our last layer if you will . but there's no non-linearity with it and it's just a single vector . and so because by default here in the notation of the class we assume these are column vectors . we transpose it so that we have a simple inner product . so it's just another set of parameters that will score the final activations to be high if they're a named entity if there's a named entity of the center of this window . it is just a score that we're trying to maximize and we compute that final score with this inner product . and so these activations are now something that we compute in this pretty complex neural network function yeah . here everything is a column vector that's correct . so the question is is there a particular reason of why we chose a linear layer as the last layer . and the answer here is to simplify the math a little bit . and because and to introduce to you another kind of objective function not everything has to be normalized and basically summed to 1 as probabilities . if you just care about finding one thing versus a lot of other things like i just want to find named entities that are locations as center words . and if it's high then that's likely one . and if it's low then it's likely not a center location . then that's all you need to do . and in some sense you could add here a sigmoid after this and then call it a probability and then use standard cross entropy loss to in your model . there's no reason of why you shouldn't do it . 
that's something you have to do in the problem sets trying to combine we derive that we help you derive the softmax and cross entropy pair optimization . and then we going through this and hopefully you can combine the two and you'll see how both work . but it's essentially a modeling decision and it's not wrong to apply a sigmoid here . and then call this a probability instead of a score . all right so now we have this two layer neural network and we essentially did most of the work already to derive the final things here . we already knew how to derive our u gradients . and what used to be just w is not w superscript 2 but just because we add the superscript all the math is the same . so here same derivation that we did for w it's just now sitting on a2 instead of on just a . and so what we did here basically follows directly to what we now have . it's the same thing but we now have to be careful to add these superscripts depending on where we are in the neural network . and we'll have the same definition here when we multiply ui and f prime of zi superscript 3 we'll just call that delta superscript 3 and subscript i for the ith element . and this is going to give us the partial derivative with respect to wij the i jth element of the w matrix . so this one we've already derived in all its glory . i'm just putting here again with the right superscripts . now the total function that we have is this one . and again we have here this same derivative i just copied it over . and in matrix notation we have to find this as the outer product here . that would give us the cross product all the pairs of i and j to have the full gradient of the w2 matrix . so this one was exactly as before except that we now add the superscript a2 here . now in terms of the notation we defined this delta i in terms of all these elements . 
and these are basically if you think about it two vectors . this ui we could write as the full u . all the elements of the u vector . and f prime of zi we could write as f prime of z3 where we basically drop the index and assume this is just one vector of a bunch of element wise applications of this gradient function of this derivative here . so we'll introduce now this notation which will come in very handy . and we call the hadamard product or element-wise product . sometimes you'll see it as little circles . sometimes it's a circle with a cross or with a dot inside . whenever you see these in backprop derivatives it's usually means the same thing . which we just element-wise multiply all the elements of the two vectors with one another . so this is how we'll define from now on this delta the air signal that's coming in at this layer . back propagation and to understand it is essentially the gradient with respect to w1 the second layer now that we're moving through . any questions around the hadamard product it is no longer what . so the question is once you use the hadamard product how is this related to the matrix multiplication here or the vector outer product . and so you basically first have to compute this . and then you have the full delta definition . and then you can multiply these and outer product to get the gradient . sure so the question is could you assume that these are diagonal matrices . and yes it's this kind of the same thing . but in terms of the multiplication you have to then make sure your diagonal matrix is efficiently implemented when it's multiplied with another vector . 
and as you write this out if this is confusing write out what it means to have a matrix times this . and what if this is just a diagonal matrix . and what do you get versus just multiplying each of these elements with one another . so just write out the definitions of the matrix product and then you'll observe that you could think of it this way . but then really this f prime here is just as a single vector why apply the derivative in this case zero two so . all right so the last missing piece w1 the gradient for it . and so the main thing we have to figure out now is what's the bottom layer's error message delta 2 that's coming in . and i'm not going to go through all the indices again . it would take a while and it's kind of repetitive . and it's very very similar to what we've done in the last lecture . but essentially we had already arrived at this expression . and in our previous model we would just arrive arrives at the word vectors . so our final word vector update was defined this way . and what we now basically have to do is once more just apply the chain rule because instead of having coming up at the word vectors . instead we're actually coming up at another layer . so basically you can kind of call it's when you multiply whatever error signal comes from the top layer you multiply that with your local error signal in this case f prime here . then together you'll get the update for either the weights that are at that layer or the intermediate term for the gradient for lower layers . so that's what we mean by our signal . and it might help in the next definition it might give you a better explanation of this in backprop number two . basically we apply the chain rule again . 
and if the chain rule for such a complex function is maybe less intuitive so one thing that helped me many years ago is to essentially assume all of these are scalars just single variable . and then derive all this assuming it just u w and x are all just single numbers . and then derive it that will help you gain some intuition . and then you'll observe in the end that the final delta 2 is essentially similar to what we had derived in a very detailed way which is w2 transposed times delta 3 and then hadamard product times f prime of z2 which is that layer here . and this is basically it if you understand these two equations and you feel you can derive them now then you will know all the updates for all standard multilayer neural networks . you will in the end always arrive at these two equations . and that is if you wanna compute the error signal that's coming into a new layer then you'll have some form of w of the high layer transposed times the error signal that's coming in there . hadamard product with element y's derivatives here of f in the f prime . and in the final update for each w will always be this outer product of delta error signal times and here i include also our and you can even describe the top and bottom layers this way . and then lead to word vectors and the linear layer but they just have a very simple delta . all right now for some of you just like all right now i understand everything and it's great that i fully understand back propagation . but judging from piazza and just from previous years it's also quite a lot to wrap your head around . and so i will go through three additional explanations now and there we're going through much simpler functions not full neural networks but much simpler kinds of functions . but maybe for some it will help to wrap their heads around sort of the general idea of these error signals through these simpler kinds of functions . so instead of having a crazy neural network with lots of matrices and hidden layers . we'll just kinda look at a simple function like this and we'll arrive at a similar kind of idea . namely recursively applying and computing these error signals or local gradients as we move through a network . now the networks in this idea seen function as circuits are going to be much much simpler . and these are examples from another lecture on git learning for convolutional neural networks and computer vision and we're basically copying here some of their slides . and so let's take for example this very simple function f of three variables . 
and this simple function is just x plus y times z . and let's assume we start with some random initial values for x y and z from which we start and wanna compute derivatives . now just as before with a complex neural network we can define intermediate terms but now the intermediate terms are very very simple . so we'll just take q for instance and we define q as x plus y this local computation . and now we can look at the partial derivatives here of q with respect to x and with respect to y . they're very simple it's just addition right just one . in terms of q times z where we use our intermediately defined function here . and here we're kind of simplifying q it should be q is a function of x and y but we just drop that . and we can also define our partials of f our overall function with respect to q . now again to connect that to what we looked at before . f could be our lost function x y z could be parameters of this and we wanna for instance minimize our lost function . so now what we want is we want the final updates to update these variables . so we'll start with at the very top . just a df by df which is just 1 so it's not much there . and now we want to update and learn how do we update our z vectors . so we look at dfdz and what is that . well we wrote down here all our different derivatives so df by dz is just q . and we define q as x + y and x and y is minus 2 and 5 . and so the gradient or the partial derivative here is just 3 . all right so far we're just very simple q times derivative z that's it . 
all right now we can move also through this circuit . and are there questions around just the description of this circuit of this function in terms of the circuit . all right so now let's look at the dfdq which is the element here of the circuit this node in the circuit now the dfdq is again quite simple we already wrote it right here . it's just z and z is just minus 4 . but now the chain rule we have to multiply and this is essentially a delta kind of error message . the higher node in the circuit but that's in this case is just 1 . and so the overall is just z times 1 and z is minus 4 z is minus 4 . and now we're going to move through this plus node to compute the next lower derivatives here . and this is we end up at the final nodes here the final leaf nodes if you will of this tree structure and we wanna compute the dfdy . now dfdy we basically wanna use the chain rule and we're going to multiply what we have in the previous one dfdq which is the error signal coming from here times dqdy which is the local error the local gradient it's not really the full gradient right this is the local part of the gradient dqdy . so we multiply these two terms the dfdq we wrote down here that says z minus 4 times dqdy as you wrote down here . it's just one so minus 4 times 1 we got minus 4 . and we can do the same thing for x again apply the chain rule . all right so in general in this way of seeing all these functions as circuits we basically always have some kind of input so each node in the circuit and we compute some kind of output . and what's important is we can compute our local gradients here directly during the forward propagation . we don't need to know this local part of the gradient . we don't need to know what's up before . but in general we will run this forward . we get the gradient signals from any these nodes in the circuits . and essentially then use the chain rule and multiply all of these to compute the updates . 
all right any questions around the definition of the circuits for simple functions . it's very hard to take this kind of abstraction and then get all the way to this full update . therefore a full near layer neural network but it's very good to gain intuition of what's really going on on a high level . let's go through a little more complex example of what this looks like . and i think of at the end of that you kind of gain some good intuition of how we basically do forward propagation and recursively call these kinds of circuits to compute the full update . so here we have a little bit more of a complex function namely actually our sigmoid function that we had before . usually when we have our sigmoid function this was one activation of one hidden layer . in most cases x was our input and w were the weights . so we defined this already and now let's assume we just want to compute the partial derivatives with respect and let's assume x and w are just x is two-dimensional and w is three-dimensional . and we have here the bias term as just an extra element of w . so now if you take this whole function we're gonna now compute or define this as a circuit . that one description that's the most detailed description of this function as a circuit would look like this where you basically recursively the separate actions that you might take . and you can compute gradients and the local gradients at each so the last operation to compute the final output f here of this function is 1 over whatever is in here . and so that's our last element of the circuit and from the bottom it starts with multiplying these two numbers multiplying these two numbers and then adding to their summation this w2 install all right . are there any questions around the description of the circuit . all right so now let's assume we start with these simple numbers here so w2 w0 starts at 2 x0 starts at minus 1 minus 3 minus 2 and minus 3 here . so we just move forward through the circuit to compute our forward propagation right . so this is a relatively simple concatenation of functions . and now we wanna compute all our partial derivatives with respect to all these different elements here . so we'll now go backwards and recursively backwards through this circuit and apply the chain rule every time . 
so let's start the final value to the forward propagation numbers here in green at the top the final value of this is 0.73 . and again the first delta derivative of just the function with itself is just 1 . and now we hit this node in a circuit and we want to now compute the derivative of this function and the function's 1 over x . and so the derivative is just minus 1 over x squared . x is 1.73 and so we basically compute minus 1 divided by 1.37 sorry 1.37 squared . and then we multiply using a chain rule the top that goes into this node . so now you multiple these two and you get the number minus 0.53 . now we're moved to the next node so this node here we just sum up a constant with the value x and so the derivative of that is just 1 . so we multiply use the chain rule multiply these two elements the error signal or gradient signal from the top as it moves through this element of the circuit which is just minus 0.3 times 1 so we get again minus 0.53 sorry . so here derivative of e to the x is just e to the x . and we have the incoming value which is minus 1 so that's our x . so we have e to the minus 1 times minus 0.53 the gradient signal from the higher node in this circuit . and we basically continue like this for a while and compute the same for plus similar to this plus and so on . and that the end we arrive right here . and our error signal is 0.2 and we have this multiplication here . and we know in multiplication the partial of w0 times x0 partial with respect to x0 is just w0 . and so we multiply 0.2 times the value here which is 2 and we get 0.4 . and now we have an update for this parameter after we've moved recursively through the circuit all the way to where it was used . and this is essentially the same thing that we've done for the very complex neural network but sort of one step at a time for a very simple function . any questions around this sort of circuit description of the same back propagation at year . 
namely reusing the derivatives multiplying local error signals with the global error signals from higher layers where here the layer definition's a bit stretched it's very very simple kinds of operations . that's right so here each time the sort of gradient the local gradient times the global or above higher layer gradient signal . when you multiply them you get an actual gradient . so they're not really gradients right they're sort of intermediate values of a gradient . so the question is we're using this kind of circuit interpretation to compute derivatives and that's correct . if you were to just do standard math on this equation you would end up with something and you would also have similar kinds of numbers . but we're making it a little more complicated in some ways to compute the derivatives here of each of the elements of this function . we're kind of push the chain rule to its maximum by defining every single operation as a separate function . and then computing gradients at every single separate function . and when you do that even for this kind of simple function you usually wouldn't write out this complex thing and take a derivative with respect to this node which is just plus cuz we all know how to do that . and usually we just move the circuit definitions can help you understand the idea that at each node what you end up getting is the local gradient times the gradient signal from the top . so in the end you get the exact same updates as if you had just taken the derivatives using the chain rule like this . and in fact the definition of the circuit can be arbitrary too and sometimes it's a lot more work to write out all the different sub components of a function . so for instance we know if we just described sigma of x as our sigmoid function we could kind of combine all these different elements of the circuit as and we know with this one little trick here the derivative of sigma x with respect to x can actually be described in terms of sigma x . so we don't need to do any extra computation like we did internally here take another exponent and so on . we actually can just know well if that was our value here of sigma x then the derivative that will come out here is just 1- sigma x times sigma x . and so we could in theory also define our circuit differently and in fact the circuits we eventually define are this whole thing is one neural network layer . and internally we know exactly the kinds of messages that pass through such a layer or the error signals or again elements of the final gradients . so the question is we're talking about back propagation here and what is forward propagation . yeah forward propagation just means computing the value of your overall function . 
the relationship between the two is forward propagation is what you compute what you do at test time to compute the final output of your function . so you want the probability for this node to be a location or for this word to be a location you'd do forward propagation to compute that probability . and the you do backward propagation to compute the gradients if you wanna train and update your model if you have a training data set and so on . that's right the red numbers here at the bottom are all the partial derivatives with respect to each of these parameters . and here all the intermediate values that we use as that gradient flows through the circuit to the parameters that we might wanna update great question . all right so essentially we recursively applied the chain rule as we moved through this graph . and we end up with a similar kind of intuition as we did with the same with just using math and multivariate calculus to arrive at these final gradients to update our parameters . all right any questions around the circuit . so here w2 is our bias term it doesn't depend on the values of x we just add it and w2 down here in the circuit . so that is the last element we add after adding these two multiplications . all right so now if that was too simple and you wanna get a little bit high level again you can essentially think of these and circuit is the terminology that andrej karpathy used in 231 and yoshua bengio for instance another very famous researcher in deep learning uses the terminology of flow graphs but again we have the very similar kind of idea . you start with some input x compute some kind of value . you go through some intermediate variables y and then in back propagation you compute your gradients going backwards in the reverse order to what you've done during forward propagation . and so this is if you just have one intermediate value now if x and this is something else important to know it for the circuits it's the same if x modifies two paths in your flow graph you end up based on the multiple variable chain rule . you have to sum up the local air signals for both from both of the paths . and in general again you move backwards through them . so usually as long as you have some kind of directed basically graph or tree structure you can always compute these flows and these elements of your gradient . and in general if x goes through multiple different elements in your flow graph you just sum up all the partials this way . and so this is another interpretation much more high level without defining exactly what kinds of computation you have here at each node . each node is some kind of computational result . 
and each arc here is some kind of dependency so you need in order to compute this you needed this . and you can define more complex things where you have so called short circuit connections we'll define those much later in the class but in general you move forward through your node . so this is a more realistic example where we may have some input x we have some probability or sorry some class y for our train data set . and in forward propagation we'll move these through a sigmoid neural network layer here such as h is just sigma of vx . and so you can also describe your you move through a next layer and then you may have a softmax layer here similar to the one that you derived in problem set one . and then you have your negative log likelihood and you compute that final cost function for this pair xy for this training element . and then back propagation again you move backwards through the flow graph . and you update your parameters as you move through the flow graph . now before i go through the last and final explanation the good news is you won't actually have to do that for it would be close to impossible for the kinds of large complex neural networks to do this by hand . many years ago when i had started my phd there weren't any software packages with automatic differentiation . so you did have to do that . and it slowed us down a little bit . but nowadays you can essentially automatically infer your back propagation updates based on the forward propagation . it's a completely deterministic process so can use symbolic expressions for your forward prop . and then have algorithms automatically determine your gradient right . the gradients always exist for these kinds of functions . and so that will allow us to much faster prototyping . and you'll get introduced next week to a tensor flow which is one such package that essentially takes all these headaches away from you . but with this knowledge you'll actually know what's going on under the hood of these packages . all right any question around the flow graph interpretation of back propagation . 
so sorry the question was the automatic differentiation is it numeric or symbolic . all right now for the last and final explanation of the same idea . but combining the idea of the flow graph with the math that you've seen before so let's bring back this complex two layer neural network . at a much simplified kind of flow graph or circuit where we can combine in a lot of different elements instead of writing every multiplication summation exponent negation and so and out . this is the kind of flow graph that kind of yeah kind of combines these two worlds . so we assumed here we had our delta error signal coming from the simple score that we have . and let's say that our final we want all the updates essentially to w(2) and now w(2) as we move through this linear score the delta doesn't change . and so the update that we get for w(2) here is just this outer product again . and that's kind of as we move through this very high level flow graph we basically now update w(2) once we get the error message from the layer above . this kind of circuit will essentially just multiply the affine like as we move through this simple affine transformation this matrix vector product we're just required to transpose the forward propagation matrix . and we arrived why this is before but this is kind of the interpretation of this flow graph in terms of a complex and large realistic neural network . and so notice also that the dimensions here line up perfectly . so the output here we multiply this delta that has the dimensionality of the output . with the transpose we get exactly the dimensionality of the input of this w . you have the linear transformation affine transformation through this w as you move backwards to this w you just multiply it with its transpose . and now we are hitting this element wise nonlinearity . and so as we update the next delta we essentially have also an element wise derivative here of each of the elements of this activation . so as we're moving our error vector error signal or global parts of the gradient through these point-wise nonlinearities we need to apply point-wise multiplications with the local gradients of the non-linearity . and now we have this delta that's arrived at w(1) . and so w1 we can now compute the final gradient with respect to w(1) as just the delta again times the activation of the previous layer which is a(1) and we have this outer product . 
so this is combining the different interpretations that we've learned . we arrived through this through just multivariate calculus . and now this is the flow graph or circuit interpretation of what's going on . i mean coordinate wise yes they are the same . so whenever we write f(z) here and z was a vector of z1 z2 for instance then we meant f(z1) and f(z2) . and the same is true if we write it like this . it is yes so the question is the delta here the same as in the definition of and it is yeah . so this delta here is this and you notice here that it's the same thing that we wrote before . and then you have the hadamard product with the element-wise derivatives here . now understand the inner workings of most deep learning models out there . and this was literally the hardest part of the class . i think it's gonna go all uphill from here for many of you . and everything from now on is really just more matrix multiplications and this kind of back propagation . it's really 90% of the state of the art models out there right now and top new papers that are coming out this year . you now can have a warm fuzzy feeling as you look through the forward propagation definitions . all right with that let's have a little intermission and look at a paper . so yeah so let's take a break from neural networks and let's talk about this paper which came out from facebook arv so text classification is a really important topic in nlp . given a piece of text we may wanna say is this a positive sentiment or does it have negative sentiment . is this spam or ham or did jk rowling actually write this . and so this one's particular from a website and it's basing [cough] an example of sentiment analysis . 
and so if you recall from your problem set in problem four . an easy way to featurize a sentence is to just average out all the word vectors in a sentence . and that's basically what the model from this paper does . and so they use really low dimensional word vectors . take the average of them kind of you know you lose the ordering of it and then you get this low dimensional text vector which represents the sentence . in order to kind of get some of the ordering back they also use n-grams . and so now that we have the text vector that's kind of like in the hidden layer . we then feed it through a linear classifier which uses softmax compute the probability over all the predictive classes . the hidden representation is also shared by all the classifiers for which helps the classifier use information about words learned from one category for another category . and so will look a little bit more familiar to you now that you guys have gone through all the costs and whatnot . so we minimize the negative flaws likelihood over all the classes and the model's trying to using stochastic gradient descent and a linear decaying learning rate . another thing that makes it really fast is the use of the hierarchical softmax . and so by using this the classes are organized in like this tree kind of fashion instead of just like in a list . and so this also helps with the timing so we go from linear time to logarithmic time . because also the costs are organized in terms of how frequent they are . so in case we have maybe like a lot of class but less of one class . this helps kind of balance that out so nlp is really hot right now . so in here the depth is much smaller so we can access that cost a lot faster . but maybe for some less popular topics i just made some up here that's not actually my opinion . but they have a much deeper depth because they are much more infrequent . 
and so especially in this day and age when we're really crazy about neural networks does this stack up against them . because it uses a linear classifier it doesn't really have all those layers for neural network . and as it turns out this actually performs really well . it's not only really fast but it performs just as well if not sometimes better than neural networks which is pretty crazy . fasttext which is what they call their model is often on par with deep learning classifiers . it takes seconds to train instead of days thanks to their use of low dimensional word vectors in the hierarchical softmax . and another side bit is that it can also learn vector representations of words in different languages with performs even better than word2vec . like this kind of equation you could totally derive all the gradients now too . this is for many the most lasting and fun part of the class . but some people also don't have a research agenda or some kind of interesting data set so you don't have to do the project . if you do a project we want you to have a mandatory mentor . the mentors that are pre-approved are all the phd students and chris and me . so we wanna really give you good advice and we want you to meet your mentors frequently . so think i'll have 25 chris has 25 and then i guess each of the phd tas also has at most 25 groups . but yeah so basically your class projects if you do decide to do it is 30% of your final grade . and sometimes real paper submissions come out from these . it's really exciting you get to travel . you get probably paid depending on who you're working with . if you're a grad student and you write a paper to go to some fun places in the world . and something that's really helpful for people's careers . 
sometimes these papers people get contacted from various companies once we put these papers up . if you do a really good job it can have really lasting impact on the kinda work that you do . so on the choice of doing assignment four the final project . we don't wanna force you to do the final project cuz some people just wanna learn the concepts and then move on with life . and it can be a little painful to try to come up with something . so there is a final project and we will ask you to sort of define your project with your mentor . and then we might encourage you or discourage you from moving forward with that project . some projects might be too large in scope or too small in scope and so on . and so do check with the tas of whether the project is the right thing for you . if you do a project and if you decide to do it you really have to start early . ideally you will start meeting me today latest like next week or two weeks and or the other tas . we write out a lot of the sort of organizational things on the website . so let's look at the website really quick . it's now linked from our main page . so you can get a couple of different ideas from these top conferences . so one project idea and we'll go into that a little bit later is to take one of these newest papers from the various groups or various conferences and just try to replicate the results . you will notice that despite having in theory everything written in the paper if it's a nontrivial model there's a lot of subtle detail . and it's hard to squeeze all of so usually the maximum page them in so replicating sometimes this paper is sufficient enough for so here here's some very concrete papers that you can look at and to get adheres from others . and what's kind of interesting and new these days this is by no means and exclusive list there a lot more other interesting papers . so again here there sort of pre you'll have to contact us through office hours . 
and if you do a project in your project proposal you have to write out who the mentor is . a list of potential projects that are coming from people who spend all their time thinking about deep learning and nlp . so if you don't have an idea but you really do wanna do some interesting novel research project we'll post that link internally . so that not the whole world sees it but only the students in this class . cuz sometimes the phd students have some interesting novel idea . they don't want it to get scooped and have some other researchers do that idea with students and youths . so we'll keep those ideas under wraps here . so yeah this is your project proposal . you have to define all these things and we'll go through that now in some of the details here . and then you have a final submission you have to write a report . and then we'll also have a poster presentation where all the projects are basically being described . you'll have to print a little poster and we'll walk around . maybe we'll even come up with a prize for best poster and best paper and so on . all right so these are the organizational tips . posters and projects by the way i have maximum of three people . if you have some insanely well thought out plan we may make an exception and go to four . so the exception kind of has to be mailed to the tas or aston piazza . any questions around the organizational aspects of the project . you can do groups of one two or three . so it doesn't have to be three . 
the bigger your group the more we expect from the project . and you have to also write out exactly what each person in the project has done . you can actually use any kind of open source library and code that you want . but if you just take kaldi which is a speech recognition system and you say i did speech recognition . and then really all you did was download the package and run it then that's not very impressive . so the more you use the more you also have to be careful and say exactly what parts you actually implemented . and in the code you also have to submit your code so that we understand what you've done and the results are real . so this year we do want some language in there . last year i was a little more open . it could be the language of music and so on now . so we've got to have some natural language in there yeah . but other than that that can be done quite easily so projects you might want to do . and if you have a more theoretically inclined project where you really are just faking out some clever way of doing a sarcastic ready to sent or using different kinds of optimization functions . about leading the class to then as long as you at least applied it in one experiment to a natural language processing data set that would still be a pretty cool project . so you can also apply it to genomics data and to text data if you wanna have a little bit of that flavor . but there is gonna be at least one experiment where you apply it to a text data set . all right so now let's walk through the different kinds of projects that you might wanna consider and what might be entailed in such project to give you an idea . unless there are any other questions around the organization of the projects deadlines and so on . so let's start with the kind of simplest and all the other ones are sort of bonuses on top of that simple kind of project . and this is actually i think generally good advice not just for a class project but in general how to apply a deep learning algorithm to any kind of problem whether in academia or in industry or elsewhere . 
so let's assume you want to apply an existing neural network to an existing task . so you want to be able to take a long document and summarize into a short paragraph . now step one after you define your task is you have to define your dataset . and that is actually sadly in many cases in both industry and in academia an incredibly time intensive problem . to that is you just search for there's some people who've worked in summarization before . the nice thing is if you use an existing data set for instance from the document understanding conference duc here then other people have already applied some algorithms to it you'll have some base lines you know what kind of metric or evaluation is reasonable versus close to random . and so on cuz sometimes that's not always obvious . we don't always us just accuracy for instance . so in that case using an existing academic data set gets rid of a lot of complexity . however it is really fun if you actually come up with your own kind of dataset too . so maybe you're really excited about food and you want to prowl yelp or use a yelp dataset for restaurant review or something like that . so however when you do decide to do that you definitely have to check in with your mentor or with chris and me and others . because i sadly have seen several projects in the last couple of years where people have this amazing idea . and then they spent 80% of the time on their project on a web crawler getting not blocked from ip addresses writing multiple ip addresses having multiple machines and crawling . sometimes it's just the document they were hoping to get and crawl it's just a 404 page . and then they realize html and they filter that . and before you know it it's like they have like three more days left to do any deep learning for nlp . and so it has happened before so don't fall into that trap . if you do decide to do that check with us and try to before the milestone deadline . for sure have the data set ready so you can actually do deep learning for nlp cuz sadly we just can't give you a good grade for a deep learning for nlp class if you spend 95% of your time writing a web crawler and explaining your data set . 
so in this case for instance you might say all right i want to use wikipedia . you can actually download sort of already pre-crawled versions of it . maybe you want to say my intro paragraph is the summary of the whole rest of the article . not completely crazy to make that assumption but really you can be creative in this part . you can try to connect it to your own research or your own job if your a [inaudible] student or just any kind of interest that you have . to time it's really fun nlp combine with language of music with natural language and so on . so you can be creative here and we kind of value a little bit of the creativity this is like a task of data set we had never seen before and you actually gain some interesting linguistic insights or something . that is the cool part of the project right . any questions around defining a data set . all right so then you wanna define your metric . for instance you have maybe let's say you did something simpler like restaurant star rating classification . this is a review and i want to classify if this a four star review or a one star review or a two or three . and now you may have a class distribution where this is one star this is two stars three and four and now the majority are three . maybe that you troll kind of funny and are three star reviews . so this is just like number and maybe 90% of the things you called are in the third class . and then you write your report you're super excited it was a new data set you did well you crawled it quickly . and then all you give us is an accuracy metric so accuracy is total correct divided by total . and now let's say your accuracy is 90% . it's 90% accurate 90% of the cases gives you the ride star rating . you're essentially overfit to your dataset and your evaluation metric was completely bogus . 
it's hard to know whether they basically could have implemented a one line algorithm that's just as accurate as yours which is just no matter what the input return three . so hard to give a good grade on that and it's a very tricky trap to fall into . i see it all the time in industry and for young researchers and so on . so in this case you should've used does anybody know what kind f1 that's right . so and we'll go through some of these as we go through the class but it's very important to define your metric well . now for something as tricky as summarization this isn't where you're really just like this is the class this is the final answer . you have to actually either extract or generate a longer sequence . and there are a lot of different bleu's n-gram overlap or rouge share which is a recall-oriented understudy for gisting evaluation which essentially is just a metric to weigh differently how many n-grams are correctly overlapping between a human generated summary . for instance your wikipedia paragraph number one and whatever output your algorithm gives . so rouge is the official metric for summarization in different sub-communities and nop have their own metrics and it's important that you know what you're optimizing . so the machine translation for instance you might use bleu scores bleu scores are essentially also a type of n-gram overlap metric . if you have a skewed data set you wanna use f1 . and in some cases you can just use accuracy . and this is generally useful even if you're in industry and later in life you always wanna know what metric you're optimizing . it's hard to do well if you don't know the metric that you're optimizing for both in life and deep learning projects . all right so let's say you defined your metric now you need to split your dataset . and it's also very important step and it's also something that you can easily make sort of honest mistakes . again in advantage of taking pre-existing academic dataset is that in many cases it's already pre-split but not always . and you don't wanna look at your 1 week before the deadline . so let's say you have downloaded a lot of different articles and now you basically have 100% of some articles you wanna summarize . 
and normal split would be take 80% for training you take 10% for your validation and your development . the validation split or the development split or dev split or various other terms . and 10% for your final test split . and so the final one you ideally get a sense of how your algorithm would work in real life on data you've never seen before you didn't try to chew on your model like how many layers should i use how wide should each layer be . you'll try a lot of these things we'll describe these in the future . but it's very important to correctly split and why do i make such a fuss about that . well there too you might make mistakes . so let's say you have unused text and let's say you crawled it in such a way there's a lot of mistakes that you can make if you try to predict the soft market for instance don't do that it doesn't work . but in many cases you might say or there some temporal sequence . and now you basically have all your dataset and the perfect thing to do is actually do it like this you take 80% of let's say month january to may or something and then your final test split is from november . that way you know there's no overlap . but maybe you made a mistake and you said well i crawled it this way but so as sample an article from here and one from here and one from here . and then the random sample goes to the 80% of my training data . and now the test data and the development data might actually have some overlap . cuz if you're depending on how you chose your dataset maybe the another article which just like a slight addition like some update to an emerging story . and now the summary is almost exact same but the input document just changed a tiny bit . and you have one article in your training set and another one in your test set . but the test set article is really only one extra paragraph on an emerging story and the rest is exactly the same . so now you have an overlap of your training and your testing data . and so in general if this is your training data and this should be your test data . 
it should be not overlapping at all . and whenever you do really well you run your first experiment and you get 90 f1 . and things look just too good to be true sadly in many cases they are and you made some mistake where maybe your test set had some overlap for instance with your training data . it's very important to be a little paranoid about that when your first couple of experiments turn out just to be too good to be true . that can mean either your training your task is too simple or you made a mistake in splitting and defining your dataset . all right any questions around defining a metric or your dataset yeah . so if we split it temporally wouldn't we learn a different distribution . that is correct we would learn a different distribution these are non-stationary . and that is kinda true for a lot of texts but if you ideally when you built a deep learning system for an lp you want it to built it so that it's robust . it's robust to sum such changes over time . and you wanna make sure that when you run it in a real world setting on something you've never seen before it's doing something it will still work . and this was the most realistic way to capture how well it would work in real life . would it be appropriate to run both experiments as in both where you subsample randomly and then you subsample temporally for your . you could do that and the intuitive thing that is likely going to happen is if you sample randomly from all over the place then you will probably do better than if you have this sort of more strict kind of split . but running an additional experiment will rarely ever get you points subtracted . you can always run more experiments and we're trying really hard to help you get computing infrastructure and cloud compute . so you don't feel restricted with the number of experiments you run . all right now number 5 establish a baseline . so you basically wanna implement the simplest model first . this could just be a very simple logistic regression on unigrams or bigrams . 
then compute your metrics on your train data and your development data so overfitting or underfitting . let's say your loss is very very low on training . you do very well on training but you don't do very well on testing then you're in an over fitting regime . if you do very well on training and well on testing you're done you're happy . but if your training loss can't be lower so you're not even doing well on your training that often means your so it's very important to compute both the metrics on your training and your development split . and then and this is something we value a lot in this class too . and it's something very important for you in both research and industries like you wanna analyze your errors carefully for that baseline . and if the metrics are amazing and there are no errors you're done . probably a problem was too easy and you may wanna restart unless it's really a valuable problem for the world . and then maybe you can just really describe it carefully and you're done too . it is very important to not just go in and add lots of bells and whistles that you'll learn about in the next couple of weeks in this class and create this monster of a model . you want to start with something simple sanity check make sure you didn't make mistakes in splitting your data . you have the right kind of metric . and in many cases it's a good indicator for how successful your final project is if you can get this baseline in the first half of the quarter . cuz that means you figured out a lot of these potential issues here . and you kind of have your right data set . you know what the metric is you know what you're optimizing and everything is good . so try to get to this point as quickly as possible . cuz that is also not as interesting and you can't really use that much knowledge from the class . and now you can implement some existing neural network model that we taught you in class . 
for instance this window-based model if your task is named entity recognition . you can compute your metric again on your train and dev set . hopefully you'll see some interesting patterns such as usually train neural nets is quite easy in a sense that we lower the loss very well . and then we might not generalize as well in the development set . and then you'll play around with regularization techniques . and don't worry if some of the stuff i'm saying now is kind of confusing . if you want to do this we'll walk you through that as we're mentoring you through the project . and that's why each project has to have an assigned mentor that we trust . all right then you analyze your very important be close to your data . you can't give too many examples usually ever . and this is kind of the minimum bar for this class . so if you've done this well and there's an interesting dataset then your project is kind of in a safe haven . once you have a metric and everything looks good we still want you to visualize the kind of data even if it's a known data set . we wanted you to visualize it collect summary statistics . it's always good to know the distribution if you have different kinds of classes . you want to again very important look at the errors that your model is making . cuz that can also give you intuitions of what kinds of patterns can your deep learning algorithm not capture . maybe you need to add a memory component or maybe you need to have longer temporal kind of dependencies and so on . those things you can only figure out if you're close to your data and you look at the errors that your baseline models are making . and then we want you to analyze also different hyperparameters . 
a lot of these models have lots of choices . did we add the sigmoid to that score or is the second layer 100 dimensional or 200 dimensional . should we use 50 dimensional word vectors or 1000 dimensional word vectors . there are a lot of choices that you make . and it's really good in your first couple projects to try more and and sometimes if you're running out of time and only so much so many experiments you can run we can help you and use our intuition to guide you . but it's best if you do that a little bit yourself . and once you've done all of that now you can try different model variants and you'll soon see a lot of these kinds of options . we'll talk through all of them in the class . so now another kind of class project is you actually wanna implement a new fancy model . those are the kinds of things that will put you into potentially writing an academic paper peer review and at a conference and so on . the tricky bit of that is you kinda have to do all the other steps that i just described first . and then on top of that you know the errors that you're making . and now you can gain some intuition of why the existing models are flawed . and you come up with your own new model . if you do that you really wanna be in close contact with your mentor and some researchers unless you're a researcher yourself and you earned your phd . but even then you should chat with us from the class . you want to basically try to set up an infrastructure such that you can iterate quickly . you're like maybe i should add this new layer type to this part of my model . you want to be able to quickly iterate and see if that helps or not . so it's important and actually require a fair amount of software engineering skills to set up efficient experimental frameworks that and again you want to start with simple models and then go to more and more complex ones . 
so for instance in summarization you might start with something super simple like just average all your word vectors in the paragraph . and then do a greedy search of generating one word at a time . or even greedily searching for just snippets from the existing article in wikipedia and you're just copying certain snippets over . and then stretch goal is something more advanced would be lets you actually generate that whole summary . and so here are a couple of project ideas . but again we'll post the whole list of them with potential mentors from the nop group and the vision group and various other groups inside stanford . sentiment is also a fun data set . you can look at this url here for one of the preexisting data sets that a lot of people have worked on . all right so next week we'll look at some fun and fundamental linguistic tasks like syntactic parsing . and then you'll learn tensorflow and have some great tools under your belt . again with cs224n natural language processing with deep learning . so you're in for a respite or a change of pace today . so for today's lecture what we're principally going to look at is syntax grammar and dependency parsing . so my hope today is to teach you in one lecture enough about dependency grammars and parsing that you'll all be able to do the main part of assignment 2 successfully . so quite a bit of the early part of the lecture is giving a bit of background about syntax and dependency grammar . and then it's time to talk about a particular kind of dependency grammar transition-based also dependency parsing transition-based dependency parsing . and then it's probably only in the last kind of 15 minutes or so of the lecture that we'll then get back into specifically neural network content . talking about a dependency parser that danqi and i wrote a couple of years ago . okay so for general reminders i hope you're all really aware that assignment 1 is due today . and i guess by this stage you've either made good progress or you haven't . 
but to give my good housekeeping reminders i mean it seems like every year there are people that sort of blow lots of late days on the first assignment for no really good reason . and that isn't such a clever strategy [laugh] . so hopefully [laugh] you are well along with the assignment and can aim to hand it in before it gets to the weekend . okay then secondly today is also the day that the new assignment comes out . till the start of next week but we've got it up ready to go . and so that'll involve a couple of new things and in some respects probably for much of it you might not want to start it until after next tuesday's lecture . so two big things will be different for that assignment . big thing number one is we're gonna do assignment number two using tensorflow . and that's the reason why quite apart from exhaustion from assignment one why you probably you don't wanna start it on the weekend is because on tuesday tuesday's lecture's gonna be an introduction to tensorflow . so you'll really be more qualified then to start it after that . and then the other big different thing in assignment two is we get into natural language processing content . in particular you guys are going to build neural dependency parsers and the hope is that you can learn about everything that you need to know to do that today . the readings on the website if you don't get quite everything straight from me . we're going to sort of post hopefully tomorrow or on the weekend a kind of an outline of what's in assignment four so you can have sort of a more informed meaningful choice between whether you want to do assignment four or the area of assignment four if you do it is going to be question answering over the squad dataset . but we've got kind of a page and a half description to explain what that means so you can look out for that . but if you are interested in we'll encourage people to come and meet with one of the final project mentors or find some other well qualified person around here to be a final project mentor . so what we're wanting is that sort of everybody has met with their final project mentor before putting in an abstract . and that means it'd be really great for people to get started doing that as soon as possible . i know some of you have already talked to various of us . for me personally i've got final from 1 to 3 pm so i hope some people will come by for those . 
and again sort of as richard mentioned not everybody can possible have richard or me as the final project mentor . and besides there's some really big advantages of having some of the phd student tas as final project mentors . cuz really for things like spending time hacking on tensorflow they get to do it much more than i do . and so danqi kevin ignacio arun that they've had tons of experience doing nlp research using deep learning . and so that they'd also be great mentors and look them up for their final project advice . the final thing i just want to touch on is we clearly had a lot of problems i realize at keeping up and coping with people in office hours and queue status has just i'm sorry that that's been kind of difficult . i mean honestly we are trying to work and work out ways that we can do this better and we're thinking of sort of unveiling a few changes for doing things for the second assignment . if any of you peoples have any better advice as to how things could be organized so that they could work better feel free to send a message on piazza with suggestions of ways of doing it . i guess yesterday i ran down percy liang and said percy percy how do you do it for cs221 . do you have some big secrets to do this better . but unfortunately i seem to come away with no big secrets cuz he sort of said: "we use queue status and we use the huang basement" what else are you meant to do . so i'm still looking for that divine insight [laugh] that will tell me how to get this so if you've got any good ideas feel free to share . but we'll try to get this as much better under control as we can for the following weeks . okay any questions or should i just go into the meat of things . all right so what we're going to want to do today is work out how to put structures over sentences in some human language . all the examples i'm going to show is for english but in principle the same techniques you can apply for any language where these structures are going to sort of reveal how the sentence is made up . so that the idea is that sentences and parts of sentences have some kind of structure and there are sort of regular ways that people put sentences together . so we can sort of start off with very simple things that aren't yet sentences like "the cat" and "a dog" and they seem to kind of have a bit of structure . we have an article or what linguists often call a determiner that's followed by a noun . and then well for those kind of phrases which get called noun you can kind of make them bigger and there are sort of rules for how you can do that . 
so you can put adjectives in between the article and the noun . you can say the large dog or a barking dog or a cuddly dog and things like that . and well you can put things like what i call prepositional phrases after the noun so you can get things like "a large dog in a crate" or something like that . and so traditionally what linguists and natural language processors have wanted to do is describe the structure of human languages . and they're effectively two key tools that people have used to do this and one of these key tools and i think in general the only one you have seen a fraction of is to use what in computer science terms what is most commonly referred to as context free grammars which are often referred to by linguists as phrase structure grammars . and is then referred to as the notion of constituency and so for that what we are doing is writing these context free grammar rules and the least if you are standford undergrad or something like that . i know that way back in 103 you spent a whole lecture learning about context-free grammars and their rules . so i could start writing some rules that might start off saying a noun phrase and go to a determiner or a noun . then i realized that noun phrases would get a bit more complicated . and so i came up with this new rule that says- noun phrase goes to terminal optional adject of noun and then optional prepositional phrase wherefore prepositional phrase that's a preposition followed by another noun phrase . because i can say a crate or a large crate . or a large crate by the door . even further and i could say you know a large barking dog by the door in a crate . so then i noticed wow i can put in multiple adjectives there and i can stick on multiple prepositional phrases so i'm using that star the kinda clingy star that you also see see in regular expressions to say that you can have zero or any number of these . and then i can start making a bigger thing like talk to the cuddly dog . and well now i've got a verb followed by a prepositional phrase . and so i can sort of build up a constituency grammar . so that's one way of organizing the structure of sentences and you know in 20th dragging into 21st century america this has been i mean it's what you see mainly in your intro cs class when you get taught about regular languages and context free languages and context sensitive languages . hierarchy where noam chomsky did not actually invent the chomsky hierarchy to torture cs under grads with formal content to fill the scs 103 class . the original purpose of the chomsky hierarchy was actually to understand the complexity of human languages and to make arguments about their complexity . 
sorry it's also dominated sorta linguistics in america in the last 50 years through the work of noam chomsky . but if you look more broadly than that this isn't actually the dominate form of syntactic description that is being used for understanding of the structure of sentences . so there is this other alternative view of linguistic structure which is referred to as dependency structure and what your doing with dependency structure . is that you're describing the structure of a sentence by taking each word and saying what it's a dependent on . so if it's a word that kind of modifies or is an argument of another word that you're saying it's a dependent of that word . so barking dog barking is a dependent of dog because it's of a modifier of it . large barking dog large is a modifier of dog as well so it's a dependent of it . and dog by the door so the by the door is somehow a dependent of dog . and we're putting a dependency between words and we normally indicate those dependencies with arrows . structures over sentences that say how they're represented as well . and when right in the first class i gave examples of ambiguous sentences . a lot of those ambiguous sentences we can think about in terms of dependencies . so do you remember this one scientists study whales from space . and well why is it an ambiguous headline . well it's ambiguous because there's sort of two possibilities . so in either case there's the main verb study . and it's the scientist that's studying that's an argument of study the subject . and it's the whales that are being studied so that's an argument of study . but the big difference is then what are you doing with the from space . you saying that it's modifying study or are you saying it's modifying whales . 
quickly read the headline it sounds like it's the bottom one right . but [laugh] what the article was meant to be about was really that they were being able to use satellites to track the movements of whales . and so it's the first one where the from space is modifying . and so thinking about ambiguities of sentences can then be thought about many of them in terms of these dependency structures as to what's modifying what . and this is just a really common thing in natural language because these kind of questions of what modifies what really dominate a lot of questions of interpretation . so here's the kind of sentence you find when you're reading the wall street journal every morning . the board approved its acquisition by royal trustco limited of toronto for $27 a share at its monthly meeting . and as i've hopefully indicated by the square brackets if you look at the structure of this sentence it sort of starts off as subject verb object . the board approved its acquisition and then everything after that is a whole sequence of prepositional phrases . by royal trustco ltd of toronto for $27 a share at its monthly meeting . and well so then there's the question of what's everyone modifying . so the acquisition is by by royal trustco ltd is modifying the thing that immediately precedes that . and of toronto is modifying the company royal trustco limited so that's modifying the thing that comes immediately preceeding it . so you might think this is easy everything just modifies the thing that's coming immediately before it . so what's for $27 a share modifying . yeah so that's modifying the acquisition so then we're jumping back a few candidates and saying is modifying acquisition and then actually at it's monthly meeting . that wasn't the toronto the royal trustco ltd or the acquisition that that was when the approval was happening so that jumps all the way back up to the top . so in general the situation is that if you've got some stuff like a verb and getting these prepositional phrases . well the prepositional phrase can be modifying either this noun phrase or the verb . but then when you get to the second prepositional phrase . 
well there was another noun phrase inside this prepositional phrase . it can be modifying this noun phrase that noun phrase or the verb phrase . and then we get to another one . and you don't get sort of a completely free choice cuz you do get a nesting constraint . so once i've had for $27 a share referring back to the acquisition the next prepositional phrase has to in general refer to either the acquisition or approved . i say in general because and i'll actually talk about that later . but most of the time in english it's true . you have to sort of refer to the same one or further back so you get a nesting relationship . but i mean even if you obey that nesting relationship the result is that you get an exponential number of ambiguities in a sentence based on in the number of prepositional phrases you stick on the end of the sentence . and so the series of the exponential series you get of these catalan numbers . and so catalan numbers actually show up in a lot of places in that is somehow sort of similar if you're putting these constraints in you get catalan series . so are any of you doing cs228 . yeah so another place the catalan series turns up is that when you've got a vector graph and you're triangulating it the number of ways that you can triangulate your vector graph is also giving you catalan numbers . okay so human languages get very ambiguous . and we can hope to describe them on the basis of sort of looking at these dependencies . the other important concept i wanted to introduce at this point is this idea of full linguistics having annotated data in the form of treebanks . this is probably a little bit small to see exactly . but what this is is we've got sentences . these are actually sentences that come off yahoo answers . and what's happened is human beings have sat around and drawn in the syntactic structures of these sentences as dependency graphs and those things we refer to as treebanks . 
and so a really interesting thing that's happened starting around 1990 is that people have devoted a lot of resources to building up these kind of annotated treebanks and various other kinds of annotated linguistic resources that we'll talk about later . now in some sense from the viewpoint of sort of modern machine learning in 2017 that's completely unsurprising because all the time what we do is say we want labelled data so we can take our supervised classifier and chug on it and get good results . but in many ways it was kind of a surprising thing that happened which is sort of different to the whole of the rest of history right . cuz for the whole of the rest of the history it was back in this space of well to describe linguistic structure what we should be doing is writing grammar rules that describe what happens in linguistic structure . where here we're no longer even attempting to write grammar rules . we're just saying give us some sentences . and i'm gonna diagram these sentences and show you what their structure is . and tomorrow give me a bunch more and i'll diagram them for you as well . and if you think about it in a way that initially seems kind of a crazy thing to do cuz it seems like just putting structures over sentences one by one seems really really inefficient and slow . whereas if you're writing a grammar you're writing this thing that generalizes right . the whole point of grammar is that you're gonna write this one small finite grammar . and it describes an infinite number of sentences . and so surely that's a big labor saving effort . but slightly surprisingly but maybe it makes sense in terms of what's happened in machine learning that it's just turned out to be kind of super successful this building of explicit annotated treebanks . and it ends up giving us a lot of things . and i sort of mention a few of their advantages here . first it gives you a reusability of labor . but the problem of human beings handwriting grammars is that they tend to in practice be almost unreusable because everybody does it differently and has their idea of the grammar . and people spend years working on one and no one else ever uses it . where effectively these treebanks have been a really reusable tool that lots of people have then built on top of to build all kinds of natural language processing tools of part of speech taggers and parsers and things like that . 
they've also turned out to be a really useful resource actually for linguists because they give a kind of real languages are spoken complete with syntactic analyses that you can do all kinds of quantitative linguistics on top of . it's genuine data that's broad coverage when people just work with their intuitions as to what are the grammar rules of english . and so this is actually a better way to find out all of the things that actually happened . for anything that's sort of probabilistic or machine learning it gives some sort of not only what's possible but how frequent it is and what other things it tends to co-occur with and all that kind of distributional information that's super important . and crucially crucially crucially and we'll use this for assignment two it's also great because it gives you a way to evaluate any system that you built because this gives us what we treat as ground truth gold standard data . and then we can evaluate any tool on how good it is at reproducing those . and what i wanted to do now is sort of go through a bit more carefully for sort of 15 minutes what are dependency grammars and dependency structure . so we've sort of got that straight . i guess i've maybe failed to say yeah . i mentioned there was this sort of constituency context-free grammar viewpoint and the dependency grammar viewpoint . and what we're doing for assignment two is all dependencies . we will get back to some notions of constituency and phrase structure . you'll see those coming back in later classes in a few weeks' time . but this is what we're going to be doing today . and that's not a completely random choice . it's turned out that unlike what's happened in linguistics in most of the last 50 years in the last decade in natural language processing it's essentially been swept by the use of dependency grammars that people have found dependency grammars just a really suitable framework on which to build semantic representations to get out the kind of understanding of language that they'd like to get out easily . they enable the building of very fast efficient parsers as i'll explain later today . and so in the last sort of ten years you've just sort of seen this huge sea change in natural language processing . whereas if you pick up a conference volume around the 1990s it was basically all phrase structure grammars and one or two papers on dependency grammars . and if you pick up a volume now what you'll find out is that of the papers they're using syntactic representations dependency representations . 
phrase structure what's the phrase structure grammar that's exactly the same as the context-free grammar when a linguist is speaking . okay so what does a dependency syntax say . so the idea of dependency syntax is to say that the sort of model of syntax is we have relationships between lexical items words and only between lexical items . they're binary asymmetric relations which means we draw arrows . so the whole there is a dependency analysis of bills on ports and senator brownback republican of kansas . okay so that's a start normally hen we do dependency parsing we do a little bit more than that . so typically we type the dependencies by giving them a name for some grammatical relationship . so i'm calling this the subject and it's actually a passive subject . and then this is an auxiliary modifier republican of kansas is an appositional phrase that's coming off of brownback . and so we use this kind of typed dependency grammars . and interestingly i'm not going to go through it but there's sort of some interesting math that if you just have this although it's notationally very different from context-free grammar to a restricted kind of context-free grammar with one addition . but things become sort of a bit more different once you put in a typing of the dependency labels where i wont go into that in great detail right . so a substantive theory of dependency grammar for a language we're then having to make some decisions . so what we're gonna do is when we between two things and i'll just mention a bit more terminology . so we have an arrow and its got what we called the tail end of the arrow i guess . and the word up here is sort of the head . so bills is an argument of submitted were is an auxiliary modifier of submitted . and so this word here is normally referred to as the head or the governor or the superior or sometimes even the regent . and then the word at the other end of the arrow the pointy bit i'll refer to as the dependent but other words that you can sometimes see are modifier inferior subordinate . some people who do dependency grammar really get into these classist notions of superiors and inferiors but i'll go with heads and dependents . 
okay so the idea is you have a head of a clause and then the arguments of the dependence . and then when you have a phrase like by senator brownback republican of texas . being taken as brownback and then it's got words beneath it . and so one of the main parts of dependency grammars at the end of the day as to which words are heads and which words are then the dependents of the heads of any particular structure . so in these diagrams i'm showing you here the ones i showed you back a few pages here is analysis according to universal dependencies . so universal dependencies is a new tree banking effort which i've actually been very strongly involved in . that sort of started a couple of years ago and there are pointers in both earlier in the slides and on the website if you wanna go off and learn a lot about universal dependencies . i mean it's sort of an ambitious attempt to try and have a common dependency representation that works over a ton of languages . i could prattle on about it for ages and if by some off chance there's time at the end of the class i could . but probably there won't be so i won't actually tell you a lot about that now . but i will just mention one thing that probably you'll notice very quickly . and we're also going to be using this representation in the assignment that's being given out today the analysis of universal dependencies treats prepositions sort of differently to what you might have seen else where . if you've seen any many accounts of english grammar or heard references in some english classroom to have prepositions having objects . in universal dependencies prepositions don't have any dependents . of like they were case markers if you know any language like german or latin or hindi or something that has cases . so that the by is sort of treated as if it were a case marker of brownback . so this sort of a bleak modifier of by senator brownback . brownback here as the head with the preposition as sort of like a case marking dependent of by . and that was sort of done to get more parallelism across different languages of the world . other properties of old dependencies normally dependencies form a tree . 
so there are formal properties that goes along with that . that means that they've got a single-head they're acyclic and they're connected . so there is a sort of graph theoretic properties . yeah i sort of mentioned that really dependencies have dominated most of the world . the famous first linguist was panini who wrote his grammar of sanskrit really most of the work that panini did was kind of on sound systems and make ups of words phonology and morphology when we mentioned linguistic levels in the first class . and he only did a little bit of work on the structure of sentences . but the notation that he used for structure of sentences was essentially a dependency grammar of having word relationships being marked as dependencies . yeah so the question is well compare cfgs and pcfgs and do they dependency grammars look strongly lexicalized they're between words and does that makes it harder to generalize . i honestly feel i just can't do justice to that question right now if i'm gonna get through the rest of the lecture . but i will make two comments so i mean there's certainly the natural way to think of dependency grammars they're strongly lexicalized you're drawing relationships between words . whereas the simplest way of thinking of context-free grammars is you've got these rules in terms of categories like . noun phrase goes to determiner noun optional prepositional phrase . and so that is a big difference . but it kind of goes both ways . so normally when actually natural language processing people wanna work with context-free grammars they frequently lexicalize them so they can do more precise probabilistic prediction and vice versa . if you want to do generalization and dependency grammar you can still use at least notions of parts of speech to give you a level of generalization as more like categories . but nevertheless the kind of natural ways of sort of turning them into probabilities and machine learning models are quite different . though on the other hand there's sort of some results or sort of relationships between them . but i would think i'd better not go on a huge digression . that means to rather than just have categories like noun phrase to have categories like a noun phrase headed by dog and so it's lexicalized . 
let's leave this for the moment though please okay . okay so that's panini and there's a whole big history right . so essentially for latin grammarians what they did for the syntax of latin again not very developed . they mainly did morphology but it was essentially a dependency kind of analysis that was given . there was sort of a flowering of arabic grammarians in the first millennium and they essentially had a dependency grammar . i mean by contrast i mean really kind of context free grammars and constituency grammar only got invented almost in the second half of the 20th century . i mean it wasn't actually chomsky that originally invented them there was a little bit of earlier work in britain but only kind of a decade before . so there was this french linguist lucien tesniere he is often referred to as the father he's got a book from 1959 . dependency grammars have been very popular and more sorta free word order languages cuz notions sort of like context-free languages like english that have very fixed word order but a lot of other languages of the world have much freer word order . and that's often more naturally described with dependency grammars . interestingly one of the very first natural language parsers developed in the us was also a dependency parser . so david hays was one of the first us computational linguists . and one of the founders of the association for computational linguistics which is our main kind of academic association where we publish our conference papers etc . and he actually built in 1962 a dependency parser for english . okay so a lot of history of dependency grammar . so couple of other fine points to note about the notation . people aren't always consistent in which way they draw the arrows . i'm always gonna draw the arrows so they point go from a head to a dependent which is the direction which tesniere drew them . but there are some other people who draw the arrows the other way around . so they point from the dependent to the head . 
and so you just need to look and see what people are doing . the other thing that's very commonly done is you stick this pseudo-word wall or some other name like that and that kind of makes the math and formalism easy because then every sentence starts with root and something is a dependent of root . or turned around the other way if you think of what parsing a dependency grammar means is for every word in the sentence you're going to say what is it a dependent of because if you do that you're done . you've got the dependency structure of the sentence . and what you're gonna want to say is well it's either gonna be a dependent of some other word in the sentence or it's gonna be a dependent of the pseudo-word root which is meaning it's the head of the entire sentence . specifics of dependency parsing but the kind of thing that you should think about is well how could we decide which words are dependent on what . and there are certain various information sources that we can think about . so yeah it's sort of totally natural with the dependency representation to just think about word relationships . and that's great cuz that'll fit super well with what we've done already in distributed word representations . so actually doing things this way just fits well with a couple of tools we already know how to use . we'll want to say well discussion of issues is that a reasonable attachment as lexical dependency . and that's a lot of the information that we'll actually use but information that we'd also like to use . dependency distance so sometimes there are dependency relationships and sentences between words that is 20 words apart when you got some big long sentence and you're referring that back to some previous clause but it's kind of uncommon . most of dependencies are pretty short distance so you want to prefer that . many dependencies don't sort of so if you have the kind of dependencies that occur inside noun phrases like adjective modifier they're not gonna cross over a verb . it's unusual for many kinds of dependencies to cross over a punctuation so it's very rare to have a punctuation between a verb and a subject and things like that . so looking at the intervening material gives you some clues . and the final source of information is sort of thinking about heads and thinking how likely they are to have to dependence in what number and on what sides . so the kind of information there is right a word like the is basically not likely to have any dependents at all anywhere . so you'd be surprised if it did . 
words like nouns can have dependents and they can have quite a few dependents but they're likely to have some kinds like determiners and adjectives on the left other kinds like prepositional phrases on the right verbs tend to have a lot of dependence . so different kinds of words have different kinds of patterns of dependence and so there's some information there we could hope to gather . okay yeah i guess i've already said the first point . in principle it's kind of really easy . so we're just gonna take every make a decision as to what word or root this word is a dependent of . and we do that with a few constraints . so normally we require that only one word can be a dependent of root and we're not going to allow any cycles . and if we do both of those things we're guaranteeing that we make the dependencies of a tree . and normally we want to make out dependencies a tree . and there's one other property i then wanted to mention that if you draw your dependencies as i have here so all the dependencies been drawn as loops above the words . it's different if you're allowed to put some of them below the words . whether you can draw them like this . so that they have that kind of nice none of them cross each other . or whether like these two that i've got here where they necessarily cross each other and i couldn't avoid them crossing each other . and what you'll find is in most languages certainly english the vast majority of dependency relationships have a nesting structure relative to the linear order . and if a dependency tree is fully nesting it's referred to as that you can lay it out in this plane and have sort of a nesting relationship . but there are few structures in english where you'd get things that aren't nested and yet crossing . and this sentence is a natural example of one . so i'll give a talk tomorrow on bootstrapping . so something that you can do with noun modifiers especially if they're kind of long words like bootstrapping or techniques of bootstrapping is you can sort of move them towards the end of the sentence right . 
i could have said i'll give a talk on bootstrapping tomorrow . but it sounds pretty natural to say i'll give a talk tomorrow on bootstrapping . but this on bootstrapping is still modifying the talk . and so that's referred to by linguists as right extraposition . and so when you get that kind of rightward movement of phrases you then end up with these crossing lines . and that gives you what's referred to as a non-projective dependency tree . so importantly it is still a tree if you sort of ignore the constraints of linear order and you're just drawing it out . there's a graph in theoretical computer science right it's still a tree . it's only when you consider this extra thing of the linear order of the words that you're then forced to have the lines across . and so that property which you don't actually normally see mentioned in theoretical computer science discussions of graphs is then this property that's referred to projectivity . recover the order of the words from a dependency tree . so given how i've defined dependency trees the strict answer is no . they aren't giving you the order at all . now in practice people write down the words of a sentence in order and have these crossing brackets right crossing arrows when they're non-projective . and obviously it's a real thing about languages that they have linear order . but as i've defined dependency structures you can't actually recover okay one more slide before we get to the intermission . yeah so in the second half of the class i'm gonna tell you about a method of dependency parsing . i just wanted to say very quickly there are a whole bunch about doing dependency parsing . so one very prominent way of doing dependency parsing is using dynamic programming methods which is normally what people have used for constituency grammars . a second way of doing it is to use graph algorithms . 
so a common way of doing dependency parsing you're using mst algorithms minimum spanning tree algorithms . and that's actually a very successful way of doing it . you can view it as kind of a constraint satisfaction problem . but the way we're gonna look at it is this fourth way which is these days most commonly called transition based-parsing though when it was first introduced it was quite often called deterministic dependency parsing . and the idea of this is that we're kind of greedily going to decide which word each word is a dependent of guided by having a machine learning classifier . so one way of thinking about this is so far in this class we only have two hammers . one hammer we have is word vectors and you can do a lot with word vectors . and the other hammer we have is how to build a classifier as a feedforward neural network with a softmax on top so it classifies between two various classes . and it turns out that if those are your two hammers you can do dependency parsing this way and it works really well . and so therefore that's a great approach for using in assignment two . and it's not just a great approach for assignment two . actually method four is the dominant way these days of doing dependency parsing because it has extremely good properties of scalability . that greedy word there is a way of saying this is a linear time algorithm which none of the other methods are . so in the modern world of web-scale parsing it's sort of become most people's favorite method . so i'll say more about that very soon . but before we get to that we have ajay doing our research spotlight with one last look back at word vectors . okay awesome so let's take a break from dependency parsing and talk about something we should know a lot about word embeddings . so for today's research highlight we're gonna be talking about a paper titled improving distributional similarity with lessons learned from word embeddings . and it's authored by levy et al . so in class we've learned two major paradigms for generating word vectors . 
we've learned count-based distributional models which essentially utilize a co-occurrence matrix to produce your word vectors . and we've learned svd which is singular value decomposition . and we haven't really talked about ppmi . but in effect it still uses that co-occurrence matrix to produce sparse vector encodings for words . we've also learned neural network-based models which you all should have lots of experience with now . and specifically we've talked about skip-gram negative sampling as well as cbow methods . and glove is also a neural network-based model . and the conventional wisdom is that neural network-based models are superior to count-based models . however levy et al proposed that hyperparameters and system design choices are more important not the embedding algorithms themselves . and so essentially what they do in their paper is propose a slew of hyperparameters that when implemented and tuned over the count-based distributional models pretty much approach the performance of neural network-based models to the point where there's no consistent better choice across the different tasks that they tried . and a lot of these inspired by these neural network-based models such as skip-gram . so if you recall which you all should be very familiar with this we have two hyperparameters in skip-gram . we have the number of negative samples that we're sampling as well as the unigram distributions smoothing exponent which we fixed at 3 over 4 . but it can be thought of as more of a system design choice . and these can also be transferred over to the account based variants . and i'll go over those very quickly . so the single hyper parameter that levy et al . impact in performance was context distribution smoothing which is analogous to the unigram distribution smoothing constant 3 over 4 here . and in effect they both achieved the same goal which is to sort of smooth out your distribution such that you're penalizing rare words . which interestingly enough the optimal alpha they found was exactly 3 over 4 which is the same as the skip-gram unigram smoothing exponent . 
they were able to increase performance by an average of three points across tasks on average which is pretty interesting . and they also propose shifted pmi which i'm not gonna get into the details of this . but this is analogous to the negative sampling choosing the number of negative samples in skip-gram . and they've also proposed a total of eight hyperparameters in total . and we've described one of them which is the context distribution smoothing . and this is a lot of data and if you're confused that's actually the conclusion that i want you to arrive at because clearly there's no trend here . so what the authors did was take all four methods tried three different windows and then test all the models across a different task . and those are split up into word similarity and analogy task . and all of these methods are tuned to find the best hyperparameters to optimize for the performance . and the best models are bolded and as you can see there's no consistent best model . so in effect they're challenging the popular convention that neural network-based models are superior to the count-based models . however there's a few things to note here . number one adding hyperparameters is never a great thing because now you have to train those hyperparameters which takes time . number two we still have the issues with count-based distributional models specifically with respect to the computational as well as performing svd . so the key takeaways here is that the paper challenges the conventional wisdom that neutral network-based models are in fact superior to count-based models . number two while model design is important hyperparameters are also key for achieving good results . so this implies specifically to doing a project instead of assignment four . you might implement the model but that might only take you half way there . some models to find your optimal hyperparameters might take days or even weeks to find . and finally my personal interest within ml is in deep representation learning . 
and this paper specifically excites displays that there's still lots of work to be done in the field . and so the final takeaway is challenge the status quo . okay and so now we're back to learning about how to build a transition based dependency parser . so maybe in 103 or compilers class formal languages class there's this notion of shift reduced parsing . how many of you have seen shift reduced parsing somewhere . they just don't teach formal languages the way they used to in the 1960s in computer science anymore . okay well i won't assume that you've all seen that before . okay essentially what we're going to have is i'll just skip these two slides and go straight to the pictures . because they will be much more understandable . mention the picture on this page that's a picture of joakim nivre . so joakim nivre is a computational linguist in uppsala sweden who pioneered this approach of transition based dependency parsing . he's one of my favorite computational linguists . i mean he was also an example going along with what ajay said of sort of doing something unpopular and out of the mainstream and proving that you can get it to work well . so at an age when everyone else was trying to build sort of fancy dynamic program parsers joakim said nono what i'm gonna do is i'm just gonna take each successive word and have a straight classifier that says what to do with that . and go onto the next word completely greedy cuz maybe that's kinda like what humans do with incremental sentence processing and i'm gonna see how well i can make that work . and it turned out you can make it work really well . so and then sort of transition based parsing has grown to this sort of really widespread dominant way of doing parsing . so it's good to find something different to do if everyone else is doing something it's good to think of something else that might be promising that you got an idea from . and i also like joakim because he's actually another person that's really interested in human languages and linguistics which actually seems to be a minority of the field of natural language processing when it comes down to it . okay so here's some more formalism but i'll skip that as well and i'll give you the idea of what an arc-standard transition-based dependency parser does . 
so what we're gonna do is were going to have a sentence we want to parse i ate fish and so we've got some rules for parsing which is the transition scheme which is written so small you can't possibly read it . so we have two things we have a stack and cartouche around that . and we start off parsing any sentence by putting it on the stack one thing which is our root symbol . okay and the stack has its top towards the right . and then we have this other thing which gets referred to as the buffer . and the buffer is the orange cartouche and the buffer is the sentence that we've got to deal with . and so the thing that we regard as the top of the buffer is the thing to the left off excessive words right . so the top of both of them is sort of at that intersection point between them . okay and so to do parsing under this transition-based scheme there are three operations that we can perform . we can perform they're called shift left-arc and right-arc . so the first one that we're gonna do is shift operation . all we do when we do a shift is we take the word that's on the top of the buffer and put it on the top of the stack . and then we can shift again and we take the word that's on the top of the buffer and put it on the top of the stack . remember the stack the top is to the right . the buffer the top is to the left . okay so there are two other operations left in this arc-standard transition scheme which were left arc and right arc . so what left arc and right arc are gonna do is we're going to make attachment decisions by adding a word as the dependent either to the left or to the right . okay so what we do for left arc is on the stack we say that the second to the top of the stack is a dependent of the thing that's the top of the stack . so i is a dependent of ate and we remove that second top thing from the stack . and so now we've got a stack with just [root] ate on it . 
but we collect up our decisions so we've made a decision that i is a dependent of ate and that's that said a that i am writing in small print off to the right . okay so we still had our buffer with fish on it . so the next thing we're gonna do is shift again and put fish on the stack . and so at that point our buffer is empty we've moved every word on to the stack in our sentence . and we have on it root ate fish okay . so then the third operation we have is right arc and right arc is just the opposite of left arc . so for the right arc operation we say the thing that's on the top of the stack should be made a dependent of the thing that's second to top on the stack . we remove it from the stack and we add an arc saying that . so we right arc so we say fish is a dependent of ate and we remove fish from the stack . we add a new dependency saying that fish is a dependent of ate . and then we right arc one more time so the dependent of the root . so we pop it off the stack and we're just left with root on the stack and we've got one new dependency saying that ate is a dependent of root . so at this point and i'll just mention right in reality there's i left out writing the buffer in a few of those examples there just because it was getting pretty crowded on the slide . but really the buffer is always there right it's not that the buffer disappeared and came back again it's just i didn't always draw it . so but in our end state we've got one thing on the stack and we've got nothing in the buffer . and that's the good state that we want to be in if we finish parsing our sentence correctly . and so we say okay we're in the finished state and we stop . and so that is almost all there is to arc-standard transition based parsing . right so we have a stack and our buffer and then on the side we have a set of dependency arcs a which starts off empty and we add things to . and we have this sort of set of actions which are kind of legal moves that we can make for parsing and so this was how things are . 
so we have a start condition root on the stack buffer is the sentence no arcs . we have the three operations that we can perform . here i've tried to write them out formally so the sort of vertical bar is sort of appends an element to a list operation . so this is sort of having wi as the first word on the buffer it's written the opposite way around for the stack because the head's on the other side . and so we can sort of do this shift operation of moving a word onto the stack and these two arc operations add a new dependency . and then removing one word from the stack and our ending condition is one be the root and an empty buffer . and so that's sort of the formal operations . so the idea of transition based parsing is that you have this sort of set of legal moves to parse a sentence in sort of a shift reduced way . i mean this one i referred to as arc-standard cuz it turns out there are different ways you can define your sets of dependencies . the one we'll use for the assignment and one that works pretty well . so i've told you the whole thing except for one thing which is this just gives you a set of possible moves . it doesn't say which move you should do when . and so that's the remaining thing that's left . and i have a slide on that . okay so the only thing that's left is to say gee at any point in time like we were here at any point in time you're in some configuration right . you've got certain things on there certain things in your buffer you have some set of arcs that you've already made . and which one of these operations do i do next . that nivre proposed is well what we should do is just build a machine learning classifier . since we have a tree bank with parses of sentences we can use those parses of sentences to see which sequence of operations would give the correct parse of a sentence . i am not actually gonna go through that right now . 
you can sort of work out deterministically the sequence of shifts and reducers that you need to get that structure . and it's indeed unique right that for each tree structure there's a sequence of shifts and left arcs and right arcs that will give you the right structure . so you take the tree you read off the correct operation sequence and therefore you've got a supervised classification problem . say in this scenario what you should do next is you should shift and so you're then building a classified to try to predict that . so in the early work that started off with nivre and others in the mid 2000s this was being done with conventional so maybe an svm maybe a perceptron a kind of maxent / soft max classifiers various things but sort of some classified that you're gonna use . so if you're just deciding between the operations shift left arc right arc you have got at most three choices . occasionally you have less because if there's nothing left on the buffer you can't shift anymore so then you'd only have two choices left maybe . but something i didn't mention when i was showing this is when i added to the arc set i didn't only say that fish is an object of ate . i said the dependency is the object of ate . and so if you want to include dependency labels the standard way of doing that is you just have sub types of left arc and right arc . if you have a approximately 40 different dependency labels . as we will in assignment two and in universal dependencies . you actually end up with the space of 81 way classification . names like left arc as an object . or left arc as an adjectival modifier . for the assignment you don't have to do that . for the assignment we're just doing un-type dependency trees . which sort of makes it a bit more scalable and easy for you guys . so it's only sort of a three way in most real applications it's really handy to have those dependency labels . and then what do we use as features . 
well in the traditional model you sort of looked at all the words around you . you saw what word was on the top of the stack . what was the part of speech of that word . what was the first word in the buffer . maybe it's good to look at the thing and what word and part of speech it is . so you're looking at a bunch of words . you're looking at some attributes of those words such as their part of speech . and that was giving you a bunch of features . which are the same kind of classic categorical sparse features of and people were building classifiers over that . so yeah the question is are most treebanks annotated with part of speech . we've barely talked about part of speech so far things like living things nouns and verbs . so the simplest way of doing dependency parsing as you're first writing a part of speech tag it or assign parts of speech to words . and then you're doing the syntactic structure of dependency parsing over a sequence of word part of speech tag pairs . though there has been other work part of speech tag prediction at the same time . which actually has some advantages because you can kind of explore . since the two things are associated you can get some advantages from doing it jointly . okay on the simplest possible model which was what nivre started to explore . you just took the next word ran your classifier . and said that's the object of the verb what's the next word . and you went along and just made these decisions . 
now you could obviously think gee maybe if i did some more searching and explore different alternatives i could do a bit better . and the answer is yes you can . so there's a lot of work in dependency parsing . which uses various forms of beam search where you explore different alternatives . and if you do that it gets a ton slower . and gets a teeny bit better in terms of your performance results . okay but especially if you start from the greediest end or you have a small beam . the secret of this type of parsing is it gives you extremely fast linear time parsing . because you're just going through your corpus no matter how big . so when people like prominent search engines in suburbs south of us want to parse the entire content of the web . they use a parser like this because it goes super fast . and so what was shown was these kind of greedy dependencies parses . their accuracy is slightly below the best dependency parses possible . but their performance is and the fact that they're sort of so fast and scalable . more than makes up for their teeny performance decrease . okay so then for the last few minutes i now want to get back to neural nets . okay so where are we at the moment . so at the moment we have a configuration where we have a stack and a buffer and parts of speech or words . and as we start to build some structure . the things that we've taken off we can kind of sort of think of them as starting to build up a tree as we go . 
as i've indicated with that example below . so the classic way of doing that is you could then say okay well we've got all of these features . like top of stack is word good or top of stack is word bad top of stack's part of speech as adjective . when you've got a combination of positions and words and parts of speech . you very quickly find that the number of features you have in your model extremely extremely large . but you know that's precisely how these kinds of parses were standardly made in the 2000s . so you're building these huge machine learning classifiers over sparse features . and commonly you even had features that were conjunctions of things . so you had features like the second word on the stack is has . and its tag is present tense verb . and the top word on the stack is good . and things like that would be one feature . and that's where you easily get into the ten million plus features . so even doing this already worked quite well . but the starting point from going on is saying well it didn't work completely great . that we wanna do better than that . and we'll go on and do that in just a minute . but before i do that i should mention just the evaluation of dependency parsing . evaluation of dependency parsing is actually very easy . cuz since for each word we're saying what is it a dependent of . 
that we're sort of making choices of what each word is a dependent of . which we get from our tree bank which is the gold thing . we're sort of essentially just counting how often we are right . and so there are two ways that that's commonly done . one way is that we just look at the arrows and ignore the labels . and that's often referred to as the uas measure unlabeled accuracy . or we can also pay attention to the labels . and say you're only right if and that's referred to as the las the labelled accuracy score . so the question is don't you have waterfall effects if you get something you do get some of that . because yes one decision will it's typically not so bad . because even if you mis-attach something like a prepositional phrase attachment . you can still get right all of the attachments inside noun phrase that's inside that prepositional phrase . and i mean actually dependency parsing evaluation suffers much less than doing cfg parsing which is worse in that respect . okay i had one slide there which i think i should skip . okay i'll skip on to neural ones . okay so people could build quite good machine learning dependency parsers on these kind of categorical features . but nevertheless there was a problems of doing that . so problem #1 is the features were just super sparse . that if you typically might have a tree bank that is an order about a million words and if you're then trying which are kinda different combinations of configurations . not surprisingly a lot of those configurations you've seen once or twice . 
so you just don't have any accurate model of what happens in different configurations . you just kind of getting these weak feature weights and crossing your fingers and hoping for the best . now it turns out that modern machine learning crossing your fingers works pretty well . but nevertheless you're suffering a lot from sparsity . okay the second problem is you also have an incompleteness problem because lots of configurations you'll see it run time will be different configurations that you just never happened to see the configuration . word on the stack and the top word of the stack speech or something . any kind of word pale i've only seen a small fraction of them . lot's of things you don't have features for . the third one is a little bit surprising . it turned out that when you looked at these symbolic dependency parsers and you ask what made them slow . what made them slow wasn't running your svm or your dot products in your logistic regression or things like that . all of those things were really fast . what these parsers were ending up spending 95% of their time doing is just computing these features and looking up their weights because you had to sort of walk around the stack and the buffer and sort of put together . a feature name and then you had to look it up in some big hash table to get a feature number and a weight for it . and all the time is going on that so even though there are linear time that slowed them down a ton . so in a paper in 2014 danqi and i developed this alternative where we said well let's just replace that all so that way we can have a dense compact feature representation and do classification . we'll have a relatively modest we'll use that to decide our next action . and so i want to spend the last few minutes sort of showing you how that works and this is basically question two of the assignment . okay and basically just to give you the headline this works really well . so this was sort of the outcome the first parser maltparser . 
so it has pretty good uas and las and it had this advantage that it was really fast . when i said that's been the preferred method i give you some contrast in gray . so these are two of the graph base parsers . so the graph based parsers have been somewhat more accurate but they were kind of like two orders in magnitude slower . so if you didn't wanna parse much stuff than you wanted accuracy you'd use them . but if you wanted to parse the web no one use them . and so the cool thing was that by doing this as neural network dependency parser we were able to get much better accuracy . we were able to get accuracy that was virtually as good as the best graph-based parsers at that time . and we were actually about to build a parser that works significantly faster than maltparser because of the fact that it wasn't spending all this time doing feature combination . it did have to do more vector matrix multiplies of course but that's a different story . okay so how did we do it . well so our starting point was distributed representation . so we're gonna use distributed representations of words . so similar words have close by vectors we've seen all of that . we're also going to use part in our pos we use part-of-speech tags and dependency labels . and we also learned distributed representations for those . that's kind of a cool idea cuz it's also the case that parts of speech some are more related than others . so if you have a fine grain part-of-speech set where you have plural nouns and proper names as different parts of speech from nouns singular you want to say so we also had distributed representations for those . so now we have the same kind of configuration . we're gonna run exactly the same transition based dependency parser . 
so the configuration is no different at all . but what we're going to extract from it is the starting point . just like nivre's maltparser but then what we're gonna do is for each of these positions like top of stack second top of stack buffer etc . words as sort of a 50 or 100 dimensional word vector representation of the kind that we've talked about . and so we get those representations for the different words as vectors and then what we're gonna do is just concatenate those into one longer vector . so any configuration of the parser is just being represented as the longest vector . well perhaps not that long our vectors are sort of more around 1000 not 10 million yeah . sorry the dependency of right the question is what's this dependency on feeding as an input . the dependency i'm feeding here as an import is when i previously built some arcs that are in my arc set i'm thinking maybe it'll be useful to use those arcs as well to help predict the next decision . so i'm using previous decisions on arcs as well to predict my follow-up decisions . okay so how do i do this . and this is essentially what you guys are gonna build . from my configuration i take things out of it . i get there embedding representations and i can concatenate them together and that's my input layer . i then run that through a hidden layer is a neural network feedforward neural network i then have from the hidden layer i've run that through a softmax layer and i get an output layer which is a probability distribution of my different actions in the standard softmax . and of course i don't know what any of these numbers are gonna be . so what i'm gonna be doing is i'm going to be using cross-entropy error and then back-propagating down to learn things . and this is the whole model and it learns super well and it produces a great dependency parser . i'm running a tiny bit short of time but let me just i think i'll have to rush this but i'll just say it . so non-linearities we've mentioned we haven't said very much about them and i just want to say a couple more something like a softmax . 
you can say that using a logistic function gives you a probability distribution . and that's kind of what you get in generalized linear models and statistics . in general though you want to say that . having these non-linearities sort of let's us do function approximation by putting together these various neurons that have some non-linearity . we can sorta put together little pieces like little wavelets to do functional approximation . and the crucial thing to notice is you have to use some non-linearity right . deep networks are useless unless you put something in between the layers right . if you just have multiple linear layers they could just be collapsed down into one product of linear transformations affine transformations is just an affine transformation . so deep networks without non-linearities do nothing okay . and so we've talked about logistic non-linearities . a second very commonly used non-linearity is the tanh non-linearity which is tanh is normally written a bit differently . but if you sort of actually do your little bit of math tanh is really the same as a logistic just sort of stretched and moved a little bit . and so tanh has the advantage that it's sort of symmetric around zero . and so that often works a lot better if you're putting it in the middle of a new neural net . but in the example i showed you earlier and for what you guys will be using for the dependency parser the suggestion to use for the first layer is this linear rectifier layer . and linear rectifier non-linearities are kind of freaky . they're not some interesting curve at all . linear rectifiers just map things to zero if they're negative and then linear if they're positive . and when these were first introduced i thought these were kind of crazy . i couldn't really believe that these were gonna work and do anything useful . 
but they've turned out to be super successful so in the middle of neural networks these days often the first thing you try and often what works the best is what's called relu which is rectified linear unit . and they just sort of effectively have these nice properties where if you're on the positive side the slope is just 1 . which means that they transmit error in the back propagation step really well linearly back down through the network . and if they go negative that gives enough of a non-linearity that they're just sort of being turned off in certain configurations . and so these really non-linearities have just been super super successful . and that's what we suggest that you use in the dependency parser . but this kind of putting a neural network into a transition based parser was just a super successful idea . so if any of you heard about the google announcements of parsey mcparseface . and syntaxnet for their kind of open source dependency parser . it's essentially exactly the same idea of this . just done with a bigger scaled up better optimized neural network . barak are going to be giving an introduction to tensorflow . so tensorflow is google's deep learning framework which i hope everyone will be excited to learn . and at any rate you have to learn it because we're gonna be using it so this should really also help out for the second assignment . and so before we get started with that i just want to do a couple of quick announcements . so the first one was on final projects . so this is really the time to be thinking about final projects . and if you've got ideas for final projects and want to do the final project you should be working out how to talk to one of me kevin danqi richard ignacio or arun over the next couple of weeks . and again obviously you've got to find the time so it's hard to fit everybody in . but were are making a real effort to have project advice office hours . 
there were also some ideas for projects that been stuck up on the projects page . so encourage people to look at that . now people have also asked us about assignment four . so we've also stuck up a description of assignment four . and so look at that if you're considering whether to do assignment four . so assignment four is gonna be doing question answering over the squad dataset and you can look in more details about that . so then there are two other things i wanted to mention . and we'll also put up messages on piazza etc about this . i mean the first one is that for assignment three we want people to have experience of doing things on a gpu . and we've arranged with microsoft azure to use their gpus for doing that and for people to use for the final project . and so we're trying to get that all organized at the moment . there's a limit to how many gpus we can have . so what we're gonna be doing for assignment three and for the final project is to allow teams of up to three . and really it's in our interest and the resource limit's interest if many people could be teamed up . so we'd like to encourage people to team up for assignment three . and so we've put up a google form for people to enter their teams . and we need people to be doing that in advance because we need to get that set up at least a week in advance so we can get the microsoft people to set up accounts for people so that people can use azure . so please think about groups for assignment three and then fill in the google form for that . and then the final thing is for next week we're gonna make some attempts of reorganizing the office hours and get some rooms for office hours so they can hopefully run more smoothly in the countdown towards the deadline for assignment two than they did for assignment one . so keep an eye out for that and expect that some of the office hour times and locations will be varying a bit compared to what they've been for the first three weeks . 
and so that's it from me and over to nishith . so today we are gonna be talking about tensorflow which is another deep learning framework from google . first of all much of the research in deep learning and machine learning can be attributed because of these deep learning frameworks . they've allowed researchers to iterate extremely quickly and also have made deep learning and other algorithms in ml much more accessible to practitioners . so if you see your phone a lot smarter than it was three years ago it's probably because one of these deep learning frameworks . so the deep learning frameworks help to scale machine learning code which is why google and facebook can now scale to billions of users . obviously since you all must have finished your first assignment you must know that gradient calculation isn't trivial . and so this takes care of it automatically and we can focus on the high-level math instead . it also standardizes ml across different spaces . so regardless of whether i'm at google or at facebook we still use some form of tensorflow or another deep learning framework . and there's lot of cross-pollination between the frameworks as well . a lot of pre-trained models are also available online so people like us who have limited resources in terms of gpus do not have to start from scratch every time . we can stand on the shoulders of giants and on the data that they have collected and sort of take it up from there . they also allow interfacing with gpus which is a fascinating feature because gpus actually speed up your code a lot faster because of the parallelization . which is why studying tensorflow is sort of almost necessary in order to make progress in deep learning just because it can facilitate your research and your projects . pa two three and also for the final project which also is an added incentive for studying tensorflow today . it's just a deep learning framework an open source software library for numerical computation using flow graphs from google . it was developed by their brain team which specializes in machine learning research . and in their words tensorflow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms . so now i'll allow barak to sort of take over and give a high-level overview of how tensorflow works and the underlying paradigms that so many researchers have spent so much time thinking about . 
i'm gonna be introducing some of the main ideas behind tensorflow some of its main features . the big ideas about tensorflow is that numeric computation is expressed as a computational graph . if there was one lesson that you took out of this presentation today at the back of your mind is that the backbone of any tensorflow program is going to be a graph where the graph nodes are going to be operations shorthand as ops in your code . and they have any number of inputs and a single output . and the edges between our nodes are going to be tensors that flow between them . and the best way of thinking about what tensors are in practice is as n-dimensional arrays . the advantage of using flow graphs as the backbone of your deep learning framework is that it allows you to build complex models in terms of small and simple operations . and this is going to make your gradient calculations extremely simple when we get to that . you're going to be very very grateful for the automatic differentiation when you're coding large models in your final project and in the future . another way of thinking about a tensorflow graph is that each operation is a function that can be evaluated at that point . and hopefully we will see why that is the case later in the presentation . so let us look at an example of a neural network with one hidden layer and what its computational graph in tensorflow might look like . so we have some hidden layer that we are trying to compute as the relu activation of some parameter matrix w times some input x plus a bias term . so if you recall from last lecture the relu is an activation function standing for rectified linear unit in the same way that a sigmoid is an activation function . we are applying some nonlinear function over our linear input networks their expressive function . and the relu takes the max of your input and zero . on the right we see what the graph might look like in tensorflow . we have variables for our b and w we have a placeholder we'll get to that soon with the x and nodes for each of the operations in our graph . so let's actually dissect those node types . variables are going to be stateful nodes which output their current value . 
in our case it's just b and w what we mean by saying that variables are stateful is that they retain their current value over multiple executions and it's easy to restore saved values to variables . so variables have a number of other useful features . they can be saved to your disk during and after training which is what facilitates the use the niche talked about earlier that allows people from different companies and groups to save store and send over their model parameters to other people . and they also make gradient updates by default . will apply over all of the variables and your graph . the variables are the things that you wanna tune to minimize the loss and we will see how to do that soon . it is really important to remember that variables in the graph like b and w are still operations . by definition if there can be such a thing as a definition on this all of your nodes in the graph are operations . so when you evaluate the operation that is these variables in our run time and we will see what run time means very shortly you will get the value of those variables . the next type of nodes are placeholders . so placeholders are nodes whose value is fed in at execution time . if you have inputs into your network that depend on some sort of external data that you don't want build your graph that depends on any real value . so these are place folders for values that we're going to add into our computation during training . so this is going to be our input . so for placeholders we don't give any initial values . we just assign a data type and we assign a shape of a tensor so the graph still knows what to compute even though it doesn't have any stored values yet . the third type of node are mathematical operations . this is going to be your matrix multiplication all of these are nodes in your tensorflow graphs . and it's very important that we're actually calling on tensorflow mathematical operations as opposed to numpy operations . okay so let us actually see how this works in code . 
we're going to create a placeholder variable for our input x and then we're going to build our flow graph . so how does this look in code . we're gonna import our tensorflow package we're gonna build a python variable b that is a tensorflow variable . taking in initial zeros of size 100 . our w is going to be a tensorflow variable taking uniformly distributed values between -1 and 1 of shapes of 184 by 100 . we're going to create a placeholder for our input data that doesn't take in any initial values it just takes in a data type 32 bit floats as well as a shape . now we're in position to actually build our flow graph . we're going to express h as the tensorflow relu of the tensorflow matrix multiplication of x and w and we add b . so you can actually see that the form of that line when we build our h essentially looks exactly the same as how it would look like a numpy except we're calling on our tensorflow mathematical operations . and that is absolutely essential because up to this point we are not actually manipulating any data we're only building symbols inside our graph . no data is actually moving in through our system yet . you can not print off h and actually see the value it expresses . first and foremost because x is just a place holder it doesn't have any real data in it yet . but even if x wasn't you can not print h until we run a tune . we are just building a backbone for our model . but you might wonder now where is the graph . if you look at the slide earlier i didn't build a separate node for this matrix multiplication node and a different node for add and a different node for relu . we've only defined one line but i claim that we have all of these nodes in our graph . so if you're actually try to analyze what's happening in the graph what we're gonna do and there are not too many reasons for you to do this when you're actually programming a tensorflow operation . but if i'm gonna call on my default graph and then i call get_operations on it i see all of the nodes in my graph and there are a lot of things going on here . 
you can see in the top three lines that we have three separate nodes just to define what is this concept of zeroes . there are no values initially assigned yet to our b but the graph is getting ready to take in those values . we see that we have all of these other nodes just to define what the random uniform distribution is . and on the right column we variable_1 that is probably going to be our w . and then at the bottom four lines we actually see the nodes as they appear in our figure the placeholder the matrix multiplication the addition and the relu . so in fact the figure that we're presenting on the board is simple for what tensorflow graphs look like . there are a lot of things going behind the scenes that you don't really need to interface with as a programmer . but it is extremely important to keep in mind that this is the level of abstraction that tensorflow is working with above the python code . this is what is actually going and it is also interesting to see that if you look at the last node relu it is pointing to the same object in memory as the h variable that we defined above . both of them are operations referring to the same thing . so in the code before what this h actually stands for is the last current node in the graph that we built . so the question was about how we're deciding what the values are and the types . this is purely arbitrary choice we're just showing an example it's not related to it's just part of our example . and the next question is how do we actually run it . so the way you run graphs in tensorflow is you deploy it in something called a session . a session is a binding to a particular execution context like a cpu or a gpu . so we're going to take the graph that we built and we're going to deploy it on to a cpu or a gpu . and you might actually be interested to know that google is developing their own integrated circuit called a tensor processing unit just to make tensor computation extremely quickly . it's in fact orders of magnitude more quick then even a gpu and they did use a tender alpha go match against lissdell . so the session is any like hardware environment that supports the execution of all so that's how you deploy a graph great . 
so lets see how this is run in code . we're going to build a session object and we're going to call run on two arguments fetches and feeds . fetches are the list of graph nodes that return the outputs of the nodes . these are the nodes that we're interested in actually computing the values of . the feeds is going to be a dictionary mapping from graph nodes to actual values that we want to run in our model . so this is where we actually fill in the placeholders that we talked about earlier . so this is the code that we have earlier and we're gonna add some new lines . we're first going to build a session object called tf.session . most likely a cpu but you're able to add in as an argument what device you want to run it on . first of all session.run on initialize all the variables . this is concept intensive flow called lazy evaluation . it means that the evaluation of your graph only ever happens at run time and run time now we can add an interpretation to run time in tensorflow so and so means the session . once we build the session we're the tensa flow run time so it is only then that we actually stick or assign the values that we initialize our bmw on to those notes . after those two lines we're finally in a position to call run on the note we're actually interested in the h and we feed in our second argument of dictionary for x it's our placeholder with the values that we're interested . for now just some random values question . initialize all variables will initialize all the things that are formerly called variables in your graph like b and w in this case . what is the difference between variables and place holders and why we might we might want to use which . so place sorry variables are in most cases will be the parameters that we're interested in you can almost think of them as the direct correspondence x are data is not a parameter we're interested in tuning . in the models we are working with . additionally it's important that our parameters have initializations in our model to begin with . 
if we're gonna take our model and export it to somebody else . there's no reason for it to actually the data is arbitrary it's the model parameters that are the foundation of your model . they are what makes your model interesting and computing what it computes . great so what have we covered so far . we first built a graph using variables and placeholders . we then deploy that graph onto a session which is the execution environment . and next we will see how to train the model . so the first question that we might ask in terms of optimization is how do we define the loss . so we're going to use placeholder for labels as data that we feed in only at run time and then we're going to build a loss node using our labels and prediction . the first line in code here is we're going to have this python variable that is the prediction at the end of your neural network . it's going to be the top of some soft max over whatever it is that your neural network is outputting a probability the first sign is where is the end of the feed forward stage of your neural network . it's what your network is trying to predict . we're then going to create a variable called label that is a place holder for the ground truth that our model is trying to train against . now we are ready to create our cross entropy node which is just like in our assignment one . it's going to be the sum of the labels times the tensorflow log of the prediction on our column . so just an interesting point so tensorflow functions but tensorflow will automatically convert addition subtraction and element wise multiplication into tensorflow operations question . the row altogether which is what we want to do since label in the label each row so you wanna multiply that by our prediction . and it's going to multiply it at the point of the target index . and when we sum that it's going to give us the correct result . everything else will be a zero in that row so it's squashing it into a column . 
since zero access is the rows axis 1 is the columns . the question was are the feeds just for the placeholders . the feeds are just used as a dictionary to fill in the values of our placeholders . great all right so we've now defined the loss and we are ready to compute the gradients . so the way this is done in tensorflow is we're first going to create an optimizer object . so there's a general abstract class in tensorflow called optimizer . where each of the subclasses in that class is going to be an optimizer for a particular learning algorithm . so the learning algorithm that we already use in this class is the gradient descent algorithm but there are many other choices that you might want to experiment with in your final project . they have different advantages so that is just the object to create an optimization node in our graph . we're going to call on the method of it it's called minimize and it's gonna take in its argument to the node that we actually want to minimize . so this adds an optimization operation to the top of our computational graph which when we evaluate that node when we evaluate this variable i wrote in the top line called train_step equals the line . when we call session on run on trainstep it is going to actually apply the gradients onto all of the variables in our model . this is because the dot minimize function actually does two things in tensor flow . it first computes the gradient of our argument in this case cross entropy . with respect to all of the things that we defined as variables in our graph in this case the b and w . and then it's actually going to apply the gradient updates to those variables . so i'm sure the question in the back of all your minds now is how do we actually compute the gradients . so the way it works in tensorflow is that every graph node has an attached gradient operation has a prebuilt gradient of the output with respect to the input . when we want to calculate the gradient of our cross entropy with respect to all the parameters it is extremely easy to just backpropagate through the graph using the chain rule . of expressing this machine-learning framework as this computational graph because it is very easy for the application to step backwards to traverse backwards through your graph and at each point multiply the error signal by the predefined gradient of our node . 
and all of this happens automatically and it actually happens behind the programmer's interface . the question was is the gradients are competed with respect to the cross with respect to all of our variables . so the argument into the minimize function is going to be the node that it's computing the gradient of in the numerator with respect to automatically all of the things we defined as variables in our graph . doesn't that you can add as another argument what variables to actually apply gradients to but if you don't it's just going to automatically do it to everything defined as a variable in our graph . which also answers a question earlier about why we wouldn't want to call x a variable because we're not actually we don't actually want to update that . so how does this look like in code . we're just going to add the top line in the previous slide . we're gonna create a python variable called train_step that takes in our gradient descen toptimizer object with learning rate of 0.5 . we're gonna minimize on it over the cross_entropy . so you can kinda see that that all of the important information about doing optimization . it knows what gradient step algorithm and knows what learning rate and knows what node to compute the gradients over and an oath to minimize it of course . so let's actually see how to run this in code the last thing we have to do . the question was how does session know what variables to link it to . the session is going to deploy all of the nodes in your graph everything in the graph is already on it so when you call minimize on this particular node it's already there inside your session to like compute if that answers it . okay so the last thing we need to do now that we have the gradients we have the gradient update . it's just to create an iterative learning schedule . so we're going to iterate over say 1000 iterations the 1000 is arbitrary . we're going to call on our favorite data sets we're gonna take our next batch data is just any abstract data in this arbitrary program . so we're gonna get a batch for our inputs a batch for our labels . we're then going to call sess.run on our training step variable . 
so remember when we call run on that it already applies the gradients onto all the variables in our graph . and it's gonna take a feed dictionary for the two place holders that we've defined so far . the x and the label where x and label are graph nodes . the keys in our dictionary are graph nodes and the items are going to be numpy data . and this is actually a good place to talk about just how well tensorflow interfaces with numpy because tensorflow will automatically convert numpy arrays when we feed it in to our graph in to tensors . so we can insert in to our feed dictionary numpy arrays which are batch_x and batch_label and we are also going to get is an output from sess.run . if i defined some variable like output equals sess.run that would also be a numpy array of what the nodes of what the nodes evaluate to . though train_step would return you the gradient i believe . are there any other questions up to that point before we take a little bit of a turn . some ways to create queues for that might be the answer to your question . i can testify to why this but it certainly is a simple one where you can just work with numpy data which is what python programmers are used to . and that is the insert point into our placeholder . your question was how does a cross so the cross entropy is going to take an i haven't defined what prediction it is fully i just wanted to abstract that part . the prediction is going to be something at the end of your no network where all of those are symbols inside your graph . something before that's going to be all these notes in your graph . i think this might be a better answer to your question when you evaluate some node in the graph like if i were to call session.runonprediction it automatically computes all of the nodes before it in the graph that need to be computed to actually know what the value of prediction is . behind the season tensorflow it's going to transverse backwards in your graph and compute all of those operations and that happens behind you . so the last important concept that i wanna talk about before we move over to the live demo is the concept of variable sharing . so when you wanna build the large model you often need to share large sets of variables and you might want to initialize them all in one place . for example i might want to instantiate my graph multiple times or even more interestingly i want to train over like a cluster of gpus . 
we might not have the benefit to do that in the class because of the research limitations you wanna talk about but especially moving on from this class it's often the case that you wanna train your model on many different devices at one go . if it's instantiating or model on each of these devices but we wanna share the variables . so one naive way you might think of doing this is creating this variable's dictionary at the top of your code that a dictionary of some strings into the variables that they represent . and in this way if i wanna build locks below it that depends on this parameters . i would call variables_dic and i would take the key as these values . but there are many reasons this is not a good idea . and it's mostly because it breaks the encapsulation . so what the code that builds your graph's intensive flow should always have all of the relevant information about the nodes and operations that you are using . you want to be able to in your code document the names of your neurons . the types of your operations and and you kind of lose this information if you just have this massive variables dictionary at the top of your code . so tensorflow inspired solution for this is something called variable scope . a variable scope provides a simple name spacing scheme to avoid clashes and the other relevant function to go along with that is something called get_variable . so get_variable will create a variable for you if a variable with or it will access that variable if it finds it to exist . so let us see some examples about how this works . let me open a new variable scope called foo . and i'm gonna called so this is the first time i'm calling get_variable on v so it's going to create a new variable and you'll find that the name of that variable is foo/v . so kind of calling this variable scope on foo . it's kind of accessing a directory that we're calling foo . let me close that variable score an reopen it with another argument called reuse to be true . now if i call get_variable with the same name v i'm actually going to find the same variable i'm gonna access the variable i'm gonna access the same variable that i created before . 
so you will see that v1 and v are pointing to the same object . if i close this variable scope again and reopen it but i set reuse to be false your program will absolutely crash if i try to run that line again . because you've set it to not reuse any variables so it tries to create this new variable but it has the same name as the variable we defined earlier . the uses of variable scope will become apparent in the next assignment and over the class but it is something useful to keep in the back of your mind . so in summary what have we looked at . we learned how to build a graph in tensorflow that has some sort of feedforward or prediction stage where you are using your model to predict some values . i then showed you how to optimize those values in your neural network how tensorflow computes the gradients and how to build this train_step operation that applies gradient updates to your parameters . i then showed you what it means to initialize a session which deploys your graph onto some hardware environment to run your program . i then showed you how to build some sort of simple iterating schedule to continuously run and train our model . are there any questions up to this stage before we move on in this lecture . it doesn't because in feed_dict you can see that in feed_dict it takes in some node . we're not really understanding feed_dict with what the names of those variables are . so whenever you create a variable or a placeholder there's always an argument that allows you to give the name of that node . so when you create the name of that node not name in my python variable but name as a tensorflow symbol . that's a great question the naming scope changes the name of the actual symbol of that operation . so if i were to scroll back in the slides and look at my list of operations . the names of all those operations will be appended with foo as we created earlier . maybe one more question before we move on if there's anything . yes if you load a graph using the get variable it will call the same variable across devices . this is why it's extremely important to introduce this idea of variable scope to shared devices . 
i believe the answer to that question is correct . but i'm not entirely sure as of this time . okay so we just have a couple of acknowledgements . when we created this presentation we consulted with a few other people who have done tensorflow tutorials . most of these slides are inspired by jon gauthier in a similar presentation he gave . we also talked with bharath and chip . chip is teaching a class cs20si tensorflow for deep learning research . so we are very grateful to all the people we talked with to create these slides . and now we will move on to the research highlights before we move on to the live demo . and let's take a break from tensorflow and talk about something also very interesting . i'm gonna present a paper called visual dialog . basically in recent years we are witnessing rapid development improvement in ai . especially in natural language processing and computer vision . and many people believe that the next generation of intelligent systems . will be able to hold meaningful conversations with humans in natural language based on the visual content . so for example it should be able to help blind people to understand their surroundings by answering their questions . or you can integrate them together with ai assistants such as alexa to understand people's questions better . and before i move on to the paper let's talk about some related work . there have been a lot of efforts trying to combine natural language parsing and computer vision . and the first category is image captioning . 
the first one is a paper called show attend and tell . which is an extension of tell with some attention mechanisms . and the second one is an open-source code written by andrej karpathy . in both models the models are able to give you a description of the image . and for the second case that's a typo right there . basically the model's able to summarize the content of the video . so imagine if you are watching a movie and you don't wanna watch the whole movie . and in this category is visual-semantic alignment . so instead of giving a description for each image this model actually gives a description for each individual component in the image . and as we can see on the right the data collection process it's very tedious . because you actually need to draw a lot of boundary boxes and give a description to every single one . and the next one is more related to our paper which is called visual question and answering . basically given an image and a question the model answers the question based on the visual content . and in this case as you can see the answers are either binary yes or no or very short . and this paper visual dialog actually tries to solve the issue i just mentioned . and it proposes a new ai task called visual dialog . hold meaningful conversation with humans based on the visual content . and also develop a novel data collection protocol . and in my opinion this is the best invention ever . because you make contributions to science make money and socialize with people all at the same time . 
and it also introduces the family of deep learning models for visual dialog . and i'm not gonna go into too many details today because we are gonna cover deep neural networks later in this class . this model encodes the image using a convolutional neural network . and encodes the question and the chat history using two recurrent neural network . and then concatenates three representations together as a vector . it is then followed by a fully connected layer and a decoder which generate the answer based on the representation . and here's some analysis of the dataset . as you can see the dataset is much better than the previous work because there are more unique answers . and also the question and answers tend to be longer and here are some results . they actually show the model in the form of a visual chat bot . basically you can chat with and if you guys are interested please try it [laugh] and that's it . so we're gonna start with linear regression . i'm sure all of you if you have taken cs 221 or cs 229 then you have heard and coded up linear regression before . this is just gonna be a start to get us familiarized with tensorflow even better . so we're gonna start at what does linear regression do again . it takes all your data and tries to find the best linear fit to it . so imagine house prices with time for example or location it's probably a linear fit . and so we generate our data set artificially using y equals 2 x plus epsilon where epsilon is sampled from a normal distribution . i won't really go much into how we obtain the data . because that we assume is normal python processing and not really tensorflow so we will move on . 
and actually start implementing linear regression and the function run . so in this first function linear regression we will be actually implementing the graph itself . as [inaudible] said we will be implementing and defining the flow graph . so first we're gonna create our placeholders because we're gonna see how we can feed in our data . so we have two placeholders here x and y . so let's just start with creating x first and this is gonna be of type float so we are gonna make float32 and it's gonna be of shape . so we're gonna make this likely more general and have it of shape none . and what this means that you can dynamically change the number of batches that you can send to your network . or in this case your linear model . and it's just a row vector here . all right and we're gonna name it x . we're gonna create y which is the label and which will also be of the same type and shape as well . all right and we're gonna name it y . all right so now that we have defined our placeholders we're gonna start creating other variables . so we start with first by defining our scope . so let's say tf.variable_scope and we're gonna name it just a lreg because linear regression . and we're gonna call it scope all right . so now that we are here we're gonna create our matrix which is w . so we're gonna call it tf.variable and since it's just a linear regression it'll just be a single integer or not an integer my bad but just one number . and we're gonna randomly initialize it with np.random.random . 
we're gonna start with the normal distribution rather . now we're gonna actually build the graph now that we have defined our variables and placeholders . we're gonna define y_pred which is just prediction and it's gonna be given by tf.mul(w x) . yeah so as i mentioned earlier none in this case is so that you can dynamically change the number of batches that so imagine like if i'm doing hyper parameter tuning i don't want to go and change shape to be 10 or 32 or 256 later on . almost you can imagine that you're i'm gonna change the number of batches that i'm gonna send to my network . so as we mentioned it'll just go into the variable scope and then define the name as it pleases so yeah . so now that we have our prediction the next logical thing is to actually compute the loss . so this we are gonna do by just there are two norm . so let's just first get the norm itself . and that's gonna be given by square and we just do (y_pred- y) . and since we wanted to be of a particular shape its gonna be over reduce some . okay so now with this we have finished building our graph . and so now we'll return x y y_pred and from our linear regression model . now we're gonna actually start computing what's in the graph . and we first start by generating our dataset . and i'm gonna fill in code here which we'll define the training procedure for it . all right so let's get started on that part . so first we get what we call the model . we make an instance of it and that's just gonna be given by this . all right so once we have that we are gonna create our optimizer . 
and this is gonna be given by as barack mentioned earlier in his slides descentoptimizer . and we are gonna define the learning rate to be 0.1 . and we are gonna minimize over the loss that we just got from our model . okay now we are gonna start a session . and we are first gonna initialize yeah that's one thing i forgot . we are gonna first initialize our variables as someone earlier asked . so this is actually a new function . so it's likely different from what barrack mentioned . and this sort of explains our tens of our base really quickly and since the time barrack made the slide then i need the code . it's already been updated so we're going to change that . and now we are gonna run the init function which is just initialization of variables . now we are gonna create our feed_dict . and so what we are gonna feed in is essentially just x_batch and y_batch which you got from our regenerate dataset function . all right now we're gonna actually just loop over our data set multiple times because it's a pretty small dataset . 30s is just our arbitrary chosen here . we are gonna get our loss value and optimize it . i'll explain the step in a second . so now we're gonna call run and what we want to fetch is the loss and the optimizer and we are gonna feed in our feed dict . does anyone have any questions on this line . all right and we are just gonna print for the loss here . 
and then since this is an array we are just going to want the mean because we have almost 101 examples . all right so now that we're done with that we can actually go and train our model but we'd also like to see how it actually ends up performing . so what we are gonna do is we are gonna actually see what it predicts and how we get that is again calling the session.run on y_pred . and our feed dictionary here will be just this . so the optimizer was defined as a gradientdescentoptimizer here . so you can see we are not returning anything for that which is why i just ended up with a blank there . so over here you see i'm yeah all right so we can actually go and start running our code and see how it performs okay . so let's actually go and run our [inaudible] . so you see the loss decrease and we can actually go ahead and see how it turns out . okay i guess it didn't like my tmux . anyways so you see we fed a linear line over the data . all right so now we are actually gonna go and implement word2vec using skip-gram which is slightly gonna be more complex . this was just to see how we create very simple models in tensorflow . this is refine our understanding of word2vec again . the first cs224 homework was a lot of fun . and if you were suppose to make a dataset out of this sentence here . and we are going have the cs221 for a 224 end [inaudible] first . and so we are just basically decomposing other sentence into a data set . remember that skipgram tries to predict each context word given this target word . and since the number of context words here is just two because our window size is one . 
and so the task now becomes to predict d from cs224n . and fun from off and so on . and so this is our data set . so just clarifying what word2vec actually was . so let's go ahead and start implementing that . i've already made the data processing functions here . so we won't have to deal with that . and this function load data already loads the pre-process training and validation set . training data is a list of batch and we have about 30000 of those . and we are going to see a train as well here . the valuation data is just a list of all validation inputs . and the reverse dictionary is a python dictionary from word index to word . so let's start and go ahead and implement skipgram first . so we are again going to start and so this is going to be batch inputs . and we are going to define a placeholder here . but in this case we can define with int32 . and the shape is going to be batch_size and nothing . because we are not going to call multiple variable scopes . then we go and create our batch labels . this will also be of the same shape as previous . 
and finally we will go and create a constant for our validation set . because that is not going to change anytime soon . and that is going to be defined by a val_data which we previously loaded . and we have to define what type it is . and the type for that is int32 again . so since i'll be applying transposes later . i just wanted to make sure that it's one . it doesn't really make that big of a difference . so in this case i'll be calling transpose on labels . which is why i just wanted to make you wouldn't . it's just i wanna make it absolutely not a column vector . so now we can go and all right . so this is where we'll define our model . and first we are going to go and create an embeddings as we all did in our assignment . and that's going to be a huge variable . and it's going to be initialized randomly with uniform distribution . and this is going to kick vocabulary size which you have previously defined in the top . and it's going to take embedding size . so this is going to be a number of words in your dictionary times the size of your embedding size . we just going to also give the parameters for that . 
all right so we just created our embeddings . now since we want to index with our batch . we are going to create batch embeddings . and you are going to use this function . which is actually going to be pretty handy for our current assignment . and so we do an embedding lookup with the embeddings . and we are going to put in the batch inputs here . finally we go and create our weights . and we are going to call it here .variable here . so we are going to use truncated normal distribution here . which is just normal distribution where it's cut off at two standard deviations instead of going up the internet . this is also going to be of the same size as previously . but this is going to be vocabulary size and embedding size . and this is because i turn tracks with our input directly . since this is truncated normal we need to define what the standard deviation is . and this is going be given by one over the square root of the embedding size itself . finally we go and create our biases which are also going to be variables . and this is going to be initialized with zeros of size vocabularies . now we define our loss function now that we have all our variables . so in this case we used a soft max cross entropy in our assignment are the negative log likelihood . 
in this case you'd be using something similar . and this is where tensorflow really shines . it has a lot of loss functions built in . and say we are going to use this called negative constraint negative concentrate . but it is very similar in the sense that the words that need to come up with a higher probability are emphasized . and the words which should not appear with lower probability are not emphasized . and so we are going to call tf.nn . nn is the neural network library in tensorflow our module . and this is going to take a couple of parameters . which is what you're trying to learn . no w is the weight matrix that is a parameter that you're trying to also learn . effectively you can think of these embeddings as sort of semantic representations of those words right . our embeddings is defined as the vocabulary size . so let's say we have 10000 words in our dictionary . and each row is now the word vector that goes with that word . and since our batch is only a subset of the vocabulary we need to index into that eh matrix . with our batch which is why we used the embedding lookup function okay . all right so we're gonna go and just use this api obviously everyone would need to look up on the tensorflow website itself . but what this would do is now take the weights and the biases and the labels as well . and they also take an input which is batch_inputs . 
and so here's where tensorflow really shines again the num_sampled . so in our data set we only have positive samples or in the sense that we had the context words and the target word . we also need context words and noisy words . we have defined num_samples to be 64 earlier . and what it would essentially do is look up 64 words which are not in our training set and which are noise words . and this would serve as sort of negative examples so that our network learns which words are actually context words and which are not . and finally our num_classes is defined by our vocabulary size again . with that we have defined our loss function . and now we have to take the mean of that because loss needs to be and we get that by reduced mean . so we get the loss is given for that particular batch . and since we have multiple samples in a batch we want to take the average of those . and now we have completely defined our loss function . now we can go ahead and actually if you remember from the assignment we take the norm of these word vectors . so let's go ahead and do that first . so that will be reduce_mean this is just api calling which is very valuable and detailed on the tensorflow website itself . so this is where in this i have added an argument called keep dimensions . and this is where if you sum over a dimension you don't it to disappear but just leave it as 1 . and now we divide the embeddings with the norm to get the normalized_embeddings . and now we return from we get batch inputs we return batch labels because this will be our feed . with this done we can come back to this function later . 
so now we go and define our run function . we actually make a object of our model . and loss from our function which was just called word2 or skipgram rather . okay and now we initialize the session . and over here again i forgot to initialize our variables . we just initialized all of our variables for the default values as barak mentioned again . now we are gonna go and actually loop over our data to see if we can actually go ahead and train our model . and so let's actually do that first step . so for each iteration in this for loop we are gonna obtain a batch which has its input data as well as the labels . okay so we have inputs and labels from our batch . and we can now define our feed_dictionary accordingly where the batch_inputs . and our batch_labels would just be labels . we go ahead and and we do this by calling session.run where we fetch the optimizer again and the loss . and we pass in our feed dictionary which we already defined above . and since we are trying to get the average we're gonna add it first and then divide by the number of examples that we just saw . so we're just gonna put a couple of print statements now just to make sure to see if our model actually goes and trains . since the loss will be zero in the first step we can just [inaudible] . all right so and we reset our average loss again just so that we don't for every iteration loop . we have almost finished our implementation here . so we can define that as the beginning of a run step . 
and we'll take a learning rate of zero and we're gonna minimize the loss . one thing that we're missing here is we haven't really dealt with our value addition set . so although we are training in our training set we would wanna make sure that it actually generalizes to the value addition set . and that's the last part that's missing . and we just gonna do that now . but before we do that there's only one step missing . where we once we have the validation set we still need to see how similar our word vectors are with that . and we do that in our flow graph itself . so let's go back to our skip gram function . anyway here we can implement that okay . so we have our val_embeddings against index into the embeddings matrix to get the embeddings that correspond to the validation words . and we use the embedding look up function here embedding_lookup embedding and we call in train data set or val data set . we'll actually use the normalized_embedding because we are very concerned about the cosine similarity and not necessarily about the magnitude of the similarity . okay and the similarity is essentially just a cosine similarity . so how this works is we matrix multiply the val_embeddings which we just obtained and the normalized_embeddings . and since they won't work you can just may just multiply both of them because of dimensional incompatibility we'll have to transpose_b . all right since we also returned this from our function again this is just a part of the graph . and we need to actually execute the graph in order to obtain values from it . okay and let's do since this is a pretty expensive process computationally expensive let's do this only every 5000 iterations . so the way you're gonna do this is by calling eval on the similarity matrix what this does is since we had this noted it actually goes and evaluates . 
this is equal on to calling session.run on similarity and fetching that okay . so we go on call and we get similarity and for every word in our validation set you gonna find the top_k and we can define k to be 8 here . and we will now get the nearest words . and we'll sort it according to their magnitude . and since the first word that will be closest will be the word itself we'll want the other eight words and this will be given by top_k+1 any questions so far . right so your embedding is on number of words you have in your vocabulary times the size of your word embedding for each word . so it's a huge matrix and since your batch that you're currently working with is only subset of that vocabulary this function embedding to lookup actually indexes into that matrix for you and obtains the word . this is the equivalent to some complicated python splicing that you do with matrices but it's just good syntax should go over it . so we have our nearest key words we'll just go and i have this function in my utils you can check this on the github that we'll post after the class is over . and you can play with it as you wish and in the past a nearest and a reverse_dictrionary gesture actually see the words and not just numbers all right . finally be open our final_embeddings which will be a normalized_embedding at the end of the train and on that again which is equal to calling session.run and passing and fetching this . all right we are done with the coding and we can actually see and visualize how this performs . and python word2vec oops i missed a bracket again . so we'll first load up our data set and then it will iterate over it and we will use our scripting model . you know why please tell me why have to be there okay . so as you can see here we have 30000 batches each with a bad set of 128 . all right so as we see the loss started off as 259 all right ends up at 145 and then decreases i think it goes somewhere to around 6 . here we can also see as a printing the nearest word for this is e leaders orbit this gets better with time and with more training data . we only use around 30000 examples to actually get better . and in the interest of time i'm only limited to around 30 epochs yes . 
so tensorflow comes with tensorboard which i didn't show in the interest of time . essentially you can go up to your local host and then see the entire graph and how it's organized . and so that'll actually be a huge debugging help and you can use that for your final project . all right well thank you for your time . welcome to lecture seven or maybe it's eight . definitely today is the beginning of where we talk about models that really matter in practice . we'll talk today about the simplest recurrent neural network model one can think of . but in general this model family is what most people now use in real production settings . we only have a little bit of math in between and a lot of it is quite applied and should be quite fun . just one organizational item before we get started . i'll have an extra office i'll be again on queuestatus 68 or so . last week we had to end at 8:30 . and there's still a lot of i'll be here after class for probably another two hours or so . all right then let's take a look at the overview for today . so to really appreciate the power of recurrent neural networks it makes sense to get a little bit of background on traditional language models . which will have huge ram requirements and won't be quite feasible in their best kinds of settings where they obtain the highest accuracies . and then we'll motivate recurrent neural networks with language modeling . it's a very important kind of fundamental task in nlp that tries to predict the next word . something that sounds quite simple but is really powerful . and then we'll dive a little bit into the problems that you can actually quite easily understand once you have figured out how to take gradients and you actually understand what backpropagation does . 
and then we can go and see how to extend these models and apply them to real sequence tasks that people really run in practice . all right so let's dive right in . so basically we want to just compute the probability of an entire sequence of words . and you might say well why is that useful . why should we be able to compute how likely a sequence is . and actually comes up for a lot of different kinds of problems . so one for instance in machine translation you might have a bunch of potential translations that a system gives you . and then you might wanna understand which order of words is the best . so "the cat is small" should get a higher probability than "small the is cat" . that you translate from it might not be as obvious . and the other language might have a reversed word order and whatnot . another one is when you do speech recognition for instance . it also comes up in the machine translation a little bit where you might have well this particular example is clearly more a machine translation example . but comes up also in speech recognition where you might wanna understand which word might be the better choice given the rest of the sequence . so "walking home after school" sounds a lot more natural than "walking house after school" . but home and house have the same translation or same word in german which is haus h a u s . and you want to know which one is the better one for that translation . so comes up in a lot of different kinds of areas . now basically it's hard to compute all potential sequences 'cause there are a lot of them . and so what we usually end up doing is we basically condition on just a window we try to predict the next word based on the just the previous n words before the one that we're trying to predict . 
so this is of course an incorrect assumption . the next word that i will utter will depend on many words in the past . but it's something that had to be done to use traditional count based machine learning models . so basically we'll approximate this overall sequence probability here with just a simpler version . in the perfect sense this would basically be the product here of each word given all preceding words from the first one all the way to the one just before the i_th one . but in practice this probability with traditional machine learning models we actually approximate that with some number of n words just before each word . so this is a simple markov assumption just assuming the next action or next word that is uttered just depends on n previous words . and now if we wanted to use traditional methods that are just basically based on the counts of words and not using our fancy word vectors and so on . then the way we would compute and estimate these probabilities is essentially just by counting how often does if you want to get the probability for the second word given the first word . we would just basically count up how often do these two words co-occur in this order divided by how often the first word appears in the whole corpus . let's say we have a very large corpus and we just collect all these counts . and now if we wanted to condition not just on the first and the previous word but on the two previous words then we'd have to compute all these counts . and now you can kind of sense that well if we want to ideally condition on as many n-grams as possible before but we have a large vocabulary of say 100000 words then we'll have a lot of counts . essentially 100000 cubed many numbers we would have to store to estimate all these probabilities . are there any questions for these traditional methods . all right now the problem with that is that the performance usually improves as we have more and more of these counts . but also you now increase your ram requirements . and so one of the best models of this traditional type actually required 140 gigs of ram for just computing all these counts when they wanted to compute them for 126 billion token corpus . so it's very very inefficient in terms of ram . and you would never be able stores all these different n-gram counts . 
you could never store it in a phone or any small machine . and now of course once computer scientists struggle with a problem like that they'll find ways to deal with it and so there are a lot of different ways you can back off . you say well if i don't find the 4-gram or i didn't store it because it was not frequent enough then maybe i'll try the 3-gram . and if i can't find that or i don't have many counts for that then i can back off and estimate my probabilities with fewer and fewer words in the context size . but in general you want to have at least tri or 4-grams that you store and the ram requirements for those are very large . that you'll observe in a lot of comparisons between deep learning models and traditional nlp models that are based on just counting words for specific classes . the more powerful your models are sometimes the ram requirements can get very large very quickly and there are a lot of different ways people tried to combat these issues . now our way will be to use recurrent neural networks . where basically they're similar to the normal neural networks that we've seen already but they will actually tie the weights between different time steps . and as you go over it you keep using linear plus non-linearity layer . and that will at least in theory allow us to actually condition what we're trying to predict on all the previous words . and now here the ram requirements will only scale with the number of words not with the length of the sequence that we might want to condition on . again they're you'll see different kinds of visualizations and i'm introducing you to a couple . i like sort of this unfolded one where we have here a abstract hidden time step t and we basically it's conditioned on h_t-1 and then here you compute h_t+1 . but in general the equations here are quite intuitive . we assume we have a list of word vectors . for now let's assume the word vectors are fixed . later on we can actually loosen that assumption and get rid of it . and now at each time step to compute the hidden state . at that time step will essentially these two linear layers matrix vector products and we sum them up . 
and that's essentially similar to saying we concatenate h_t-1 and the word vector at time step t and we also concatenate these two matrices . and then we apply an element-wise non-linearity . so this is essentially just a standard single layer neural network . and then on top of that we can use this as a feature vector or as our input to our standard to get an output probability for instance over all the words . this out in this formulation is basically the probability that the next word is of this specific at this specific index j conditioned on all the previous words is essentially the j_th element of this large output vector . so here you can have different some people just use u v but here we basically use the superscript just identify which matrix we have . and these are all different matrices so w_(hh) the reason we call it hh is it's the w that computes the hidden layer h given the input h t- 1 . and then you have an h_x here which essentially maps x into the same vector space that we have . our hidden states in and the weights of the softmax classifier . and so let's look at the dimensions here . so why do we concatenate and not add is the question . same notation plus w_(hx) times x then this is actually the same thing . and so this will now basically be a vector and we are feed in linearity but it doesn't really change things so let's just look at this inside part here . now if we concatenated h and x together we're now have and let's say x here has a certain dimensionality which we'll call d . so x is in r_d and our h will define to be in for having the dimensionality r_(dh) . now what would the dimensionality be if we concatenated these two matrices . so we have here the output has to be again a dh matrix . and now this vector here is a what dimensionality does this factor have when we concatenate the two . so this is a d plus dh times one and here we have dh times our matrix . it has to be the same dimensionality so d plus dh and that's why we could essentially concatenate here w_h in this way and w_hx here . 
and now we could basically multiply these . and if you again if this is confusing you can write out all the indices . and you realize that these two are exactly the same . right so as you sum up all the values here it'll essentially just get summed up also it doesn't matter if you do it in one go or not . just a single layer and that worked where you compact in two inputs but it's in many cases for recurrent neutral networks is written this way . so now here are two other ways you'll often see these visualized . this is kind of a not unrolled version of a hidden of a recurrent neural network . i actually find these kinds of unrolled versions the most intuitive . it's essentially the word vector for the word that appears at the t_th time step . as opposed to x_t and intuitively here x_t you could define it in any way . it's really just like as you go through the lectures you'll actually observe different versions but intuitively here x_t is just a vector at xt but here xt is already an input and what it means in practice is you actually have to now go at that t time step find the word identity and pull that word vector from your glove or word to vec vectors and get that in there . so x_t we used in previous lectures as the t_th element for instance in the whole embedding matrix all our word vectors . so this is just to make it very explicit that we look up the identity of the word at the tth time step and then get the word vector for that identity like the vector in all our word vectors . so i'm showing here a single layer neural network at each time step and then the question is whether that is standard or just for simplicity . it is actually the simplest and still somewhat useful . variant of a recurrent neural network though we'll see a lot of extensions even in this class and then in the lecture next week we'll go to even better versions of these kinds of recurrent neural networks . but this is actually a somewhat practical neural network though we can improve it in many ways . now you might be curious when you just start your sequence and this is age 0 here and there isn't any previous words . what you would do and the simplest thing is you just initialize the vector for the first hidden layer at the first or the 0 time step as just a vector of all 0s . right and this is the x[t] definition you had just describe through the column vector of l which is our embedding matrix at index [t] which the time step t . 
all right so it's very important to keep track properly of all our dimensionality . here w(s) to softmax actually goes over the size of our vocabulary v times the hidden state . so the output here is the same as the vector of the length of the number of words that we might wanna to be able to predict . all right any questions for the feed forward definition of a recurrent neural network . all right so how do we train this . well fortunately we can use all the same machinery we've already introduced and carefully derived . so basically here we have probability distribution over the vocabulary and we're going to use the same exact cross entropy loss function that we had before but now the classes are essentially just the next word . so this actually sometimes 'cause now technically this is unsupervised in the sense that you just give it raw text . but this is the same kind of objective function we use when we have supervised training where we have a specific class that we're trying to predict . so the class at each time step is just a word index of the next word . and you're already familiar with that here we're just summing over the entire vocabulary for each of the elements of y . and now in theory you could just . to evaluate how well you can predict the next word over many different words in longer sequences you could in theory just take this negative of the average log probability is over this entire dataset . but for maybe historical reasons and also reasons like information theory and so on that we don't need to get into what's more common is actually to use perplexity . so that's just 2 to the power of this value and hence we want to basically be less perplexed . so the lower our perplexity is the less the model is perplexed or confused about what the next word is . and we essentially ideally we'll assign a higher probability to the word that actually appears in the longer sequence at each time step . any reason why 2 to the j . yes but it's sort of a rat hole we can go down maybe after class . information theory bits and so on it's not necessary . 
now you would think well this is pretty simple we have a single set of w be relatively straightforward . sadly and this is really the main drawback of this and a reason of why we introduce all these other more powerful recurrent neural network models training these kinds of models is actually incredibly hard . and we can now analyze using the tools of back propagation and chain rule and all of that . now we can analyze and understand why that is . so basically we're multiplying here the same matrix at each time step right . so you can kind of think of this matrix multiplication as amplifying certain patterns over and over again at every single time step . and so in a perfect world we would want the inputs from many time steps ago to actually be able to still modify what we're trying to predict at a later much later time step . and so one thing i would like to encourage you to do is to try to take the derivatives with respect to these ws if you just had a two or three word sequence . it's a great exercise great preparation for the midterm . and it'll give you some interesting insights . now as we multiply the same matrix at each time step during foreprop we have to do the same thing during back propagation we have remember our deltas our air signals and sort of the global elements of the gradients . they will essentially at each time step flow through this network backwards . so when we take our cross-entropy loss here we take derivatives we back propagate we compute our deltas . now the first time step here that just happened close to that output would make a very good update and will probably also make a good update to the word vector here if we wanted to update those . but then as you go backwards in time what actually will happen is your signal might get either too weak or too strong . and that is essentially called the vanishing gradient problem . as you go backwards through time and you try to send the air signal at time step t many time steps into the past you'll have the vanishing gradient problem . so what does that mean and how does it happen . let's define here a simpler but similar recurrent neural network that will allow us to give you an intuition and so here we essentially just say all right instead of our original definition where we had some kind of f some kind of non-linearity here we use the sigma function you could use other one . first introduce the rectified linear units and so on instead of applying it here we'll apply it in the definition just right in here . 
and then let's assume for now we don't have the softmax . we just have here a standard a bunch of un-normalized scores . which really doesn't matter for the math but it'll simplify the math . now if you want to compute the total error with respect to an entire sequence with respect to your w then you basically have to sum up all the errors at all the time steps . at each time step we have an error of how incorrect we were about predicting the next word . and that's basically the sum here and now we're going to look at the element at the t timestamp of that sum . so let's just look at a single time step a single error at a single time step . and now even computing that will require us to have a very large chain rule application because essentially this error at time step t will depend on all the previous time steps too . de_t over dy_t so the t the hidden state . but then you have to multiply that with the partial derivative of yt with respect to the hidden state . so that's just that's just this guy right here or this guy for ht . this one here but it also depends on that one and that one and the one before that and so on . and so that's why you have to sum over all the time step from the first one all the way to the current one where you're trying to predict the next word . and now each of these was also computed with a w so you have to multiply partial of that as well . now let's dig into this a little bit more . and you don't have to worry too much if this is a little fast . you won't have to really go through all of this but the math that we've done before . so you can kind of feel comfortable for the most part going over it at this speed . so now remember here our definition of h_t . we basically have all these partials of all the h_t's with respect to the previous time steps the h's of the previous time steps . 
now to compute each of these we'll have to use the chain rule again . and now what this means is essentially a partial derivative of a vector with respect to another vector . something that if we're clever with our backprop definitions before we never actually have to do in practice right . 'cause this is a very large matrix and we're combining the computation with the flow graph and our delta messages before such that we don't actually have to compute explicitly these jacobians . but for the analysis of the math here we'll basically look at all the derivatives . so just because we haven't defined it what's the partial for each of these is essentially called the jacobian where you have all the partial derivatives with respect to each element of the top here ht with respect to the bottom . and so in general if you have a vector valued function output and a vector valued input and you take the partials here you get this large matrix of all the partial derivatives with respect to all outputs . and now we got this beast which is essentially a matrix . and we multiply for each partial here we actually have to multiply all of these right . so this is a large product of a lot of these jacobians . now we can try to simplify this and just say all right . let's say there is an upper bound . and we also the derivative of h with respect to h_j . actually with this simple definition of each h actually can be computed this way . and now we can essentially upper bound the norm of this matrix with the multiplication of basically these equation right here where we have w_t . and if you remember our backprop equations you'll see some common terms here but we'll actually write this out as not just an element wise product . but we can write the same thing as a diagonal where we have instead of the element wise . elements we basically just put them into the diagonal of a larger matrix and with zero path everything that is off diagonal . now we multiply these two norms here . and now we just define beta w and beta h as essentially the upper bounds . 
some number single scalar for each as like how large they could maximally be right . we have w we could compute easily any kind of norm for our w right . it's just a matrix computed matrix norm we get a single number out . and now basically when we write this all we put all this together then we see that an upper bound for this jacobians is essentially for each one of these elements as this product . and if we define each of the elements here in terms of their upper bounds beta then we basically have this product beta here taken to the t- k power . and so as the sequence gets longer and larger it really depends on the value of beta to have this either blow up or get very very small right . if now the norms of this matrix for instance that norm and then you have control over that norm right . you initialize your wait matrix w with some small random values initially before you start training . if you initialize this to a matrix that has a norm that is larger than one then at each back propagation step and the longer the time sequence goes . you basically will get a gradient that is going to explode cuz you take some value that's larger than one to a large power here . say you have 100 or something and your norm is just two then you have two to the 100th as an upper bound for that gradient and vice-versa . if you initialize your matrix w in the beginning to a bunch of small random values such that the norm of your w is actually smaller than one then the final gradient that will be sent from ht to hk could become a very very small number right half to the power of 100th . basically none of the errors will arrive . none of the error signal we got small and further backwards in time . so if the gradient here is exploding does that mean a word that is further away has a bigger impact on a word that's closer . and the answer is when it's exploding like that you'll get to not a number in no time . and that doesn't even become a practical issue because the numbers will literally become not a number cuz it's too large a value to compute . and we'll have to think of ways to come back . it turns out the exploding gradient problem has some really great hacks that make them easier to deal with than the vanishing gradient problem . and we'll get to those in a second . 
all right so now you might say this could be a problem . now why is the vanishing gradient problem an actual common practice . and again it basically prevents us from allowing a word that we're trying to break in terms of the next word . and so here a couple of examples from just language modeling where that is a real problem . so let's say for instance you have jane walked into the room . now you can put an almost probability mass of one that the next word in this blank is john right . but if now each of these words have the word vector you type it in to the hidden state you compute this . and now you want the model to pick up the pattern that if somebody met somebody else and your all this complex stuff . and then they said hi too and the next thing is the name . you wanna put a very high probability on it but you can't get your model to actually send that error signal way back over here to now modify the hidden state in a way that would allow you to give john a high probability . and really this is a large problem in any kind of time sequence that you have . and many people might intuitively say well language is mostly a sequence problem right . you have words that appear from left to right or in some temporal order as we speak . and so this is a huge problem . and now we'll have a little bit of code that we can look into . but before that we'll have the awesome shayne give us a little bit of an intercession intermission . from recurrent neural networks to talk about transition-based dependency parsing which is exactly what you guys saw this time last week in lecture . so just as a recap a transition-based dependency parser is a method of taking a sentence and turning it into dependence parse tree . and you do this over and over again in a greedy fashion until you have a full transition sequence which itself encodes the dependency so i wanna show you how to get from the model that you'll be implementing in your assignment two question two which you're hopefully working on right now to syntaxnet . syntaxnet is a model that google came out with and they claim it's the world's most accurate parser . 
and it's new fast performant tensorflow framework for syntactic parsing is available for over 40 languages . the one in english is called the parse mcparseface . a little bit here but hopefully you can read through it . so basically the baseline we're gonna begin with is the chen and manning model which came out in 2014 . and chen and manning are respectively your head ta and instructor . and the models that produce syntaxnet in just two stages of improvements those directly modified chen and manning's model which is exactly what you guys will be doing in assignment two . and so we're going to focus today on the main bulk of these changes modifications which were introduced in 2015 by weiss et al . so without further ado i'm gonna look at their three main contributions . so the first one is they leverage unlabeled data using something called tri-training . their neural network and made some slight modifications . and the last and probably most important is that they added a final layer on top of the model involving a structured perceptron with beam search . so each of these seeks to solve a problem . so as you know in most supervised models they perform better the more data that they have . and this is especially the case for dependency parsing where as you can imagine there are an infinite number of possible sentences with a ton of complexity and you're never gonna see all of them and you're gonna see even some of them very very rarely . so the more data you have the better . a ton of unlabeled data and two highly performing dependency parsers that were very different from each other . and when they agreed independently agreed on a dependency parse tree for a given sentence then that would be added to the labeled data set . and so now you have ten million new tokens of data that you can use in addition to what you already have . and this by itself improved performance by 1% using the unlabeled attachment score . so the problem here was not having enough data for the task and they improved it using this . 
the second augmentation they made was by taking the existing model which is the one you guys are implementing which has an input layer consisting of the word vectors . the vectors for the part of speech tags and the arc labels with one hidden layer and one soft max layer predicting which transition and they changed it to this . now this is actually pretty much the same thing except for three small changes . there are two hidden layers instead of one hidden layer . the second is that they used a relu nonlinearity function instead of the cube nonlinearity function . and the third and most important is that they added a perceptron layer and notice that the arrows that it takes in the previous layers in the network . so this perceptron layer wants to solve one particular problem and this problem is that greedy algorithms aren't able to really look ahead . they make short term decisions and recover from one incorrect decision . so what they said is let's allow the network then to look ahead and which we can search over and this tree is the tree of all the possible so each edge is a possible transition form the state that you're at . as you can imagine even with three transitions your tree is gonna blossom very very quickly and you can't look that far ahead and explore all of the possible branches . so what you have to do is prune some branches . and for that they use beam search . now beam search is only gonna keep track of the top k partial transition now how do you decide which k . you're going to use a score computed you guys probably have a decent idea at this point of how perceptron works . the exact function they used is shown here and i'm gonna leave up the annotations so you can take a look at it later if you're interested . but basically those are the three things that they did solve the problems with the previous chen & manning model . so in summary chen & manning had an unlabeled attachment score of 92% already phenomenal performance . they boosted it to 94% and then there's only 0.6% which is google's 2016 state of the art model . and if you're curious what the did to get that 0.6% take a look at andrew all's paper which uses global normalization instead of local normalization . so the main takeaway and it's pretty straight forward but i can't stress it enough is when you're trying to improve upon an existing model you need to identify the specific in this case the greedy algorithm and solved those problems specifically . 
in this case they did that using semi-supervised method using unlabeled data . they tune the model better and they use the structured perception with beam search . you can now look at these kinds of pictures and you totally know what's going on . and in like state of the art stuff the world publishes . all right so we'll gonna through a little bit of like a practical python notebook sort of implementation that shows you a simple version of the vanishing gradient problem . where we don't even have a full recurrent real network we just have a simple two layer neural network and even in those kinds of networks you will see that the error that you start at the top and the norm of the gradients as you go down through your network the norm is already getting smaller . and if you remember these were the two equations where i said if you get to the end of those two equations you know all the things that you need to know and you'll actually see these three equations in the code as well . let me get out of the presentation now zoom in . so here we're going to define a super simple problem . this is a code that we started and 231n (with andrej) and we just modified it to make it even simpler . so let's say our data set to keep it also very simple is just this kind of classification data set . where we have basically three classes the blue yellow and red . and they're basically in the spiral clusterform . we're going to define our simple nonlinearities . you can kind of see it as a solution almost to parts of the problem set which is why we're only showing it now . and we'll put this on the website too so no worries . but basically you could define here f element-wise and the gradients for them . so this is f and f prime if f is a sigmoid function . we'll also look at the relu the other nonlinearity that's very popular . and here we just have the maximum between 0 and x and very simple function . 
now this is a relatively straight forward definition and three layer neural network . has this input here our nonlinearity our data x just these points in two it's one of those three classes . we'll have this model here we have our step size for sdg and our regularization value . now these are all our parameters w1 w2 and w3 for all the outputs and variables of the hidden states . is the relu then we have here relu and we just input x multiply it . and in this case your x can be the entirety of the dataset cuz the dataset's so small each mini-batch we can essentially do a batch . again if you have realistic datasets you wouldn't wanna do full batch training but we can get away with it here . we multiply w1 times x plus our bias terms and rectified linear units or relu . then we've computed in layer two same idea . but now it's input instead of x is the previous hidden layer . and then we compute our scores this way . and then here we'll normalize our scores with the softmax . so very similar to the equations that we walk through . and now it's just basically an if statement . either we have used relu as our activations or we use a sigmoid but the math inside is the same . all right now we're going to compute our loss . our good friend the simple average cross entropy loss plus the regularization . so here we have negative log of the probabilities we summed them up overall the elements . and then here we have our regularization as the l2 standard l2 regularization . and we just basically sum up the squares of all the elements in all our parameters and i guess it does cut off a little bit . 
all three have the same of we add that to our final loss . and now every 1000 iterations see what's happening . and this is something you always want to do too . you always wanna visualize see what's going on . and hopefully a lot of this now looks very familiar . maybe if implemented it not quite as efficiently as efficiently in problem set one but maybe you have and then it's very very straightforward . now that was the forward propagation we can compute our error . now we're going to go backwards and we're computing our delta messages first from the scores . and now we have the hidden layer activations transposed times delta messages to compute w . again remember we have always for each w here we have this outer product . and that's the outer product we see right here . and now the softmax was the same regardless of whether we used a value or a sigmoid . we now basically have our delta scores and have here the product . so this is exactly computing delta for the next layer . and that's exactly this equation here and just python code . and then again we'll have our updates dw which is again this outer product right there . so it's a very nice sort of equations code almost a nice one to one mapping between the two . all right now we're going to go through the network from the top down to the first layer . and now we add the derivatives for our regularization . in this case it's very simple just matrices themselves times the regularization . 
and we combine all our gradients in this data structure . and then we update all our parameters with our step_size and sgd . all right then we can evaluate how well we do on the training set so that we can basically print out the training accuracy as we train us . all right now we're going to initialize all the dimensionality . we compute our hidden sizes of the hidden vectors . let's say they're 50 it's pretty large . all right we'll train it with both sigmoids and rectify linear units . and now once we wanna analyze what's going on we can essentially now plot some of the magnitudes of the gradients . so those are essentially the updates as we do back propagation through the snap work . and what we'll see here is the first and the second layer when we use sigmoid non-linearities . and basically here the main takeaway messages that blue is the first layer and green is the second layer . so the second layer is closer to the softmax closer to what we're trying to predict . and hence it's gradient is usually had larger in magnitude than the one that arrives at the first layer . and now imagine you do this 100 times . and you have intuitively your vanishing gradient problem in recurrent neural networks . they're already almost half in size over the iterations when you just had two layers . and the problem is a little less strong when you use rectified linear units . but even there you're going to have some decrease as you continue to train . all right any questions around this code snippet and vanishing creating problems . the question is why are the gradings flatlining . 
and it's essentially because the dataset is so perfectly fitted your training data . and then there's not much else to do you're basically in a local optimum and then not much else is happening . so yeah so these are the outputs where if you visualize the decision boundaries here at the relue and the relue you see a little bit more sort of edges because you have sort of linear parts of your decision boundary and the sigmoid is a little smoother little rounder . all right so now you can implement a very quick versions to get an intuition for the vanishing gradient problem . now the exploding gradient problem is in theory just as bad . but in practice it turns out we can actually have a hack that was first introduced by unmathematical in some ways 'cause say all you have is a large gradient of 100 . that's it you just define the threshold and you say whenever the value is larger than a certain value just cut it . totally not the right mathematical direction anymore . but turns out to work very well in practice yep . so vanishing creating problems how would you cap it . but then it's like it might overshoot . and you don't want to have the hundredth word unless it really matters . you can't just make all the hundred words or thousand words of the past all matter the same amount . that doesn't make that much sense either . so this gradient clipping solution is actually really powerful . and then a couple years after it was introduced yoshua bengio and one of his students actually gained it's something i encourage you always to do too . not just in the equations where you can write out recurrent neural network where everything's one dimensional and the math comes out easy and you gain intuition about it . but you can also and this is what they did here implement a very simple recurrent neural network which just had a single hidden unit . not very useful for anything in practice but now with the single unit w . and you know at still the bias term they can actually visualize exactly what the air surface looks like . 
so and oftentimes we call the air surface or the energy landscape or so that the landscape of our objective function . you can see here the size of the z axis here is the error that you have when you trained us on a very simple problem . i forgot what the problem here was but it's something very simple like keep around this unit and remember the value and then just return that value 50 times later . and what they essentially observe is that in this air surface or air landscape you have these high curvature walls . and so as you do an update each little line here you can interpret as what happens at an sg update step . and you say in order to minimize my objective function right now of my one hidden unit and my bias term just like by this amount to go over here go over here . and all of a sudden you hit these large curvature walls . and then your gradient basically blows up and it moves you somewhere way different . and so intuitively what happens here is if you rescale to the thick size with the special method then essentially you're not going to jump to some crazy faraway place but you're just going to stay in this general area that seemed useful before you hit that curvature wall . so the question is intuitively why wouldn't such a trick work for the vanishing grading problem but it does work for the exploding grading problem . why does the reason for the vanishing does not apply to the exploding grading problem . so intuitively this is exactly the issue here . so the exploding you basically jump out of the area where you in this case here for instance we're getting closer and closer to a local optimum but the local optimum was very and without the gradient problem without the clipping trick you would go way far away . right now on the vanishing grading problem it get's smaller and smaller . so in general clipping doesn't make sense let's say so that's the obvious answer . you can't something gets smaller and smaller it doesn't help to have a maximum and then make it you know cut it to that maximum 'cause that's not the problem . that's kind of most obvious intuitive answers . why couldn't you if it gets below a certain threshold blow it up . and now you're 50 time steps away . and really the 51st doesn't actually impact the word you're trying to predict at time step t right . 
so you're 50 times to 54 and it doesn't really modify that word . and now you're artificially going to blow up and make it more important . so that's less intuitive than saying i don't wanna jump into some completely different part of my error surface . the wall just comes from this is what the error surface looks like for a very very simple recurrent node network with a very simple kind of problem that it tries to solve . of the networks that you have you can try to make them have just two parameters and then you can visualize something like this too . in fact it's very intuitive sometimes to do that . when you try different optimizers we'll get to those in a later lecture like adam or sgd or achieve momentum we'll talk about those soon . you can actually always try to visualise that in some simple kind of landscape . this just happens to be the landscape that this particular recurrent neural network problem has with one-hidden unit and just a bias term . so the question is how could we know for sure that this happens with non-linear actions and multiple weight . so you also have some non-linearity here in this . so that intuitively wouldn't prevent us from transferring that knowledge . we can't really visualize a very high dimensional spaces . there is actually now an interesting i think by ian goodfellow where you can actually try to let's say you have your parameter space inside your parameter space you have some kind of cross function . so you say my w matrices are at this value and so on and i have some error when all my values are here and then i start to optimize and i end up somewhere here . now the problem is we can't visualize it because it's usually in realistic settings you have the 100 million . at least a million or so parameters sometimes 100 million . and so something crazy might be going on as you optimize between this . and so because we can't visualize it and we can't even sub-sample it because it's such a high-dimensional space . from where they started with their random initialization before optimization . 
and end the line all the way to the point where you actually finished the optimization . and then you can evaluate along this line at a certain intervals you can evaluate how big your area is . and if that area changes between two such intervals a lot then that means we have very high curvature in that area . so that's one trick of how you might use this idea and gain some intuition of the curvature of the space . but yeah only in two dimensions can we get such nice intuitive visualizations . so the question is why don't and the question of course it's a legit question but ideally we'll let the model figure this out . ideally we're better at optimizing the model and long range dependencies . in fact when you implement these and you can start playing around with this and this is something i encourage you all to do too . as you implement your models you can try to make it a little bit more interactive . have some ipython notebook give it a sequence and look at the probability of the next word . and then give it a different sequence where you change words like ten time steps away and look again at the probabilities . and what you'll often observe is that after seven words or so the words before actually don't matter especially not for these simple recurrent neural networks . but because this is a big problem there are actually a lot of and so the biggest and best one is one we'll introduce next week . but a simpler one is to use rectified linear units and to also initialize both of your w's to ones from hidden to hidden and the ones from the input to the hidden state with the identity matrix . and this is a trick that i introduced a couple years ago and then it was sort of combined with rectified linear units . and applied to recurrent neural networks by quoc le . and so the main idea here is if you move around in your space . let's say you have your h and usually we have here our whh times h plus whx plus x . and let's assume for now that h and x have the same dimensionality . so then all these are essentially square matrices . 
and we have here our different vectors . now in the standard initialization what you would do is you'd just have a bunch of small random values and all the different elements of w . and what that means is as you start optimizing whatever x is you have some random projection into the hidden space . instead the idea here is we actually maybe you can scale it so instead you have a half times the identity and what does that do . intuitively when you combine the hidden state and the word vector . if this is an identity initialized matrix . so it's just 1 1 1 1 1 1 on the diagonal . and you multiply all of these by one half . same as just having a half a half a half and so on . and you multiply this with this vector and you do the same thing here . what essentially that means is that you have a half times that vector plus half times that other vector . the beginning if you don't know anything . let's not do a crazy random projection into the middle of nowhere in our parameter space but just average . and say well as i move through the space my hidden state is just a moving average of the word vectors . and then i start making some updates . and it turns out when you look here and you apply this to the very tight problem of mnist . which we don't really have to go into but its a bunch of small digits . and they're trying to basically predict what digit it is by going over instead of using other kinds of neural networks like convolutional neural networks . and basically we look at the test accuracy . and the test accuracy for these is much much higher . 
when you use this identity initialization instead of random initialization and also using rectified linear units . now more importantly for real language modeling we can compare recurrent neural so we had the question before like do these actually matter or did i just kind of describe single layer recurrent neural networks for the class to describe the concept . and here we actually have these simple recurrent neural networks and we basically compare . this one is called kneser-ney with 5 grams so a lot of counts and some clever back off and smoothing techniques which we won't need to get into for the class . and we compare these on two different corpora and we basically look at the perplexity . so these are all perplexity numbers and we look at the neural network or the neural network that's combined with kneser-ney assuming probability estimates . and of course when you combine the two then you don't really get the advantage of having less ram . so ideally this by itself would do best in general combining the two used to still work better . these are results from five years ago and they failed most very quickly . i think the best results now are pure neural network language models . but basically we can see that compared to kneser-ney even back then the neural network actually works very well . and has much lower perplexity than just the kneser-ney or just account based . now one problem that you'll observe in a lot of cases is that the softmax is really really large . so your word vectors are one set of parameters but your softmax is another set of parameters . and if your hidden state is 1000 and let's say you have 100000 different words . then that's 100000 times 1000 dimensional matrix that you'd have to multiply with the hidden state at every single time step . so that's not very efficient and so one way to improve this is with a class-based word prediction . where we first try to predict some class that we can come up and there are different kinds of things we can do . in many cases you can sort just the words by how frequent they are . and say the thousand most frequent the next thousand most frequent words in the second class and so on . 
and so you first basically classify try to predict the class based on the history . and then you predict the word inside that class based on that class . and so this one is only a thousand dimensional and so you can basically do this . and now the more classes the better the perplexity but also the slower the speed the less you gain from this . and especially at training time which is what we see here so if you have just very few classes you can actually reduce the number here of seconds that each eproc takes . by almost 10x compared to having more classes or even more than 10x if you have the full softmax . and even the test time is faster cuz now you only essentially evaluate the word probabilities for the classes that have a very high probability here . this is maybe obvious to some but it wasn't obvious to others even in the past when people published on this . to do a single backward's pass through the sequence . once you accumulate all the deltas from each error at each time set . so looking at this figure really quick again . here essentially you have one forward pass where you compute all the hidden states and all your errors and then you only have a single backwards in time you keep accumulating all the deltas of each time step . and so originally people said for this time step i'm gonna go all the way back and then i go to the next time step and then i go all the way back and then the next step and all the way back which is really inefficient . and is essentially same as combining all the deltas in one clean back propagation step . and again it's kind of is intuitive . an intuitive sort of implementation trick but people gave that the term back propagation through time . all right now that we have these simple recurrent neural networks we can use them for a lot of fun applications . in fact the name entity recognition that we're gonna use in example with the window . in the window model you could only condition the probability of this being a location a person or an organization based on the words in that window . the recurrent neural network you can in theory take and on a lot larger context sizes . 
and so you can do named entity recognition (ner) you can do entity level sentiment in context so for instance you can say . i liked the acting but the plot was a little thin . and you can say i want to now for acting say positive and predict the positive class for that word . predict the null class and all sentiment for all the other words and then plot should get negative class label . or you can classify opinionated expressions and this is what researchers at cornell where they essentially used rnns for opinion mining and essentially wanted to classify whether each word in a relatively smaller purpose here is either the direct subjective expression or the expressive subjective expression so either direct or expressive . so basically this is direct subjective expressions explicitly mention some private state or speech event whereas the eses just indicate the sentiment or emotion without explicitly stating or conveying them . so here's an example like the committee as usual has refused to make any statements . and so you want to classify as usual as an ese and basically give each of these words here a certain label . and this is something you'll actually observe a lot in sequence tagging paths . again all the same models you have the soft max at every time step . has a set of classes that indicate whether a certain expression begins or ends . and so here you would basically have this bio notation scheme where you have the beginning or the end or a null token . it's not any of the expressions that i care about . so here you would say for instance as usual is an overall ese expression so it begins here and it's in the middle right here . and then these are neither eses or dses . all right now they started with the standard recurrent neural network and i want you to at some point be able to glance over these equations and just say i've seen this before . it doesn't have to be w superscript hh and so on . but whenever you see the summation order of course doesn't matter either . but here they use w v and u but then they defined instead of writing out softmax they write g here . but once you look at these equations i hope that eventually you're just like it's just a recurrent neural network right . 
you have here are your hidden to hidden matrix . you have your input to hidden matrix and here you have your softmax waits you . so same idea but these are the actual equations from this real paper that you can now kind of read and immediately sort of have the intuition of what happens . all right you need directional recurrent neural network where we if we try to make the prediction here of whether this is an ese or whatever name entity recognition any kind of sequence labelling task what's the problem with this kind of model . what do you think as we go from left to right only . what do you think could be a problem for making the most accurate predictions . words that come after the current word can't be helping us to make accurate predictions at that time step right . cuz we only went from left to right . extensions of recurrent neural networks is actually to do bidirectional recurrent neural networks where instead of just going from left to right we also go from right to left . and it's essentially the exact same model . in fact you could implement it by changing your input and just reversing all the words of your input and then it's exactly the same thing . and now here's the reason why they don't have superscripts with whh cuz now they have these arrows that indicate whether you're going from left to right or from right to left . and now they basically have this concatenation here and in order to make a prediction at a certain time step t they essentially concatenate the hidden states from both the left direction and the right direction . and those are now the feature vectors . and this vector ht coming from the left has all the context ordinal again seven plus words depending on how well you train your rnn . from all the words on the left ht from the right has all the contacts from the words on the right and that is now your feature vector to make an accurate prediction at a certain time set . any questions around bidirectional recurrent neural networks . the recent papers you'll be learning in various modifications . they have and we have a special lecture also we will talk a little bit about convolutional neural networks . so you don't necessarily have a cycle right . 
you just go basically as you implement this you go once all the way for your the left and you don't have any interactions with the step that goes from the right . you can compute your feet forward hts here for that direction are only coming from the left . and the ht from the other direction you can compete in fact you could paralyze this if you want to be super efficient and . have one core implement the left direction and one core implement the right direction . so in that sense it doesn't make the vanishing create any problem worse . but of course just like any recurring neural network it does have the vanishing creating problem and the exploding creating problems and it has to be clever about flipping it and so yeah we call them standard feedforward neural networks or window based feedforward neural networks . and now we have recurrent neural networks . and this is really one of the most powerful family and in fact if there's no other question we can go even deeper . and so now you'll observe [laugh] we definitely had to skip that superscript . and we have different characters here for each of our matrices because instead of just going from left to right you can also have a deep neural network at each time step . and so now to compute the ith layer at a given time step you essentially again have only the things coming from the left that modify it but you just don't take in the vector from the left you also take the vector from below . so in the simplest definition that is just your x your input vector right . but as you go deeper you now also have the previous hidden layers input . instead of why are the so the question is why do we feed the hidden layer into another hidden layer instead of the y . in fact you can actually have so called short circuit connections too where each of these h's can and so here in this figure you see that only the top ones go into the y . but you can actually have short circuit connections where y here has as input not just ht from the top layer noted here as capital l but the concatenation of all the h's . and in fact there a lot of modifications in fact shayne has a paper an arxiv right now on a search based odyssey type thing where you have so many different kinds of knobs that you can tune for even more sophisticated recurrent neural networks of the type that we'll introduce next week that it gets a little unwieldy and it turns out a lot of the things don't matter that much but each can kind of give you a little so if you have three layers you have four layers what's the dimensionality of all the layers and the various different kinds of connections and short circuit connections . we'll introduce some of these but in general this like a pretty decent model and will eventually extract away from how we compute that hidden state and that will be a more complex kind of cell type that we'll introduce next tuesday . so now how do we evaluate this . it's very important to evaluate your problems correctly and we actually talked about this before . 
when you have a very imbalanced data set where some of the classes appear very frequently and others are not very frequent you don't wanna use accuracy . in fact in these kinds of sentences you often observe this is an extreme one where you have a lot of eses and dses but in many cases just content . standard sort of non-sentiment context and words and so a lot of these are actually o have no label . and so it's very important to use f1 and we basically had this question also after class but it's important for all of you to know because the f1 metric is really one of the most commonly used metrics . and it's essentially just the harmonic precision is just the true positives divided by true positives plus false positives and recall is just true positives divided by true positives plus false negatives . and then you have here the harmonic mean of these two numbers . so intuitively you can be very accurate by always saying something or have a very high recall for a certain class but if you always miss another class that would hurt you a lot . and now here's an evaluation that you should also be familiar with where basically this is something i would like to see in a lot of your project reports too as you analyze the various hyper parameters that you have . and so one thing they found here is they have two different data set sizes that they train on in many cases if you train with more data you basically do better but then also it's not always the case that more layers . so this is the depth that we had here the number l for all these different layers . it's not always the case that more layers are better . in fact here the highest performance they get is with three layers instead of four or five . recurring neural networks best deep learning model family that you'll learn about in this class . you can gain an intuition of why that might be the case . we'll in the next lecture extend them some much more powerful models the gated recurring units or lstms and those are the models you'll see all over the place in all the state all right . today we'll do a brief recap some organizational stuff and then we'll talk about i love when we call them fancy recurrent neural networks . but those are the most important deep learning models of the day lstms and gru type models . they're very exciting and really form the base model for pretty much every deep learning paper or almost all the deep learning papers you see out there . so after today you'll really have in your hands the kind of tool that is the default tool for a lot of different deep learning final p applications so super exciting . and the best part is you kind of know most of the important math already of it so we can just define the model . 
and everything else will kind of follow through with these basic and sometimes painful building blocks that we went through before . we have a new office hour schedule and places . today we're continuously trying to optimize the whole process based on your feedback . so i'll have office hours every day multiple times . i hope that will allow us to kind of distribute the load a little bit cuz i know sometimes lots of people come to one office hour and then there's a long wait there . also it's important that you register for your gpu teams by the end of today or ideally before . so that we can make sure you all get a gpu . ideally we also encourage people to have pairs for problem set 3 and 4 the project at least pairs cuz we only have 300 or so gpus and almost 700 students in the class . so try to form teams but do make sure that you don't just have your partner or work on and implement all the gpu stuff and you do all the other parts of the problem set . cuz then you really miss out on a very important valuable skill for both research and applied deep learning if you don't know how to use a gpu . and sadly i have to get back to some work event . so i'll have a pretty short office hour today . but then i know we have the deadline for project proposals on thursdays . so on thursday i'm gonna have an unlimited office hour . i'm gonna start after class and will end when queue status is empty . so if you come half an hour late prepare to talk to me three hours later . so [laugh] the project is the coolest part so i don't wanna discourage so it's gonna be great . you should bring food too if by any chance even after midnight people we still have the queue status is still full which i doubt at that point i think i hope then we can push the proposals out for those or you can submit the proposal . and then we'll figure out the mentor situation very soon thereafter . so all right any questions around any class organization . 
all right then lets dive right in . so basically today cutting edge blast from the past because while pedagogically it'll make sense for us to first talk about a model from 2014 from just three years ago . the main model we'll end up with the long-short-term-memories is actually a very old model from 97 and has kind of been dormant for a while . as very powerful model you need a lot of training data for it you need fast machines for it . but now that we have those two things this is a very powerful model for nlp . and if you ask one of the inventors the second model is really just a special case of the lstm . but i think pedagogically it makes sense to sort of first talk about the so-called gated recurrent unit which is slightly simpler version . is one of the sort of most useful tasks you might argue of nlp sort of a real life task . something that actual people outside academia outside research outside linguistics really care about . and by the end you'll actually have the skills to build one of the best machine translation models out there modulo a lot of time and some extra effort . but the biggest parts of 90% of you'll be able to understand at least and probably build also if you have the gpu skills after this class . all right so i'm not gonna go through too many of the details but in just in preparation to mentally make you also think about the midterm that's coming up . but ideally these kinds of equations that i'm throwing up here you're pretty familiar with . at this point you're like yeah i just do some negative sampling here from my word2vec . and i have my inside and my outside vectors in the window . and similarly for glove i have two sets of vectors you optimize this . you have a function here dead limits how important very frequent pairs are in your concurrence matrix . you have scores of good windows from the large training corpus and corrupted windows . so all of these should be familiar . and if not then you really should also start thinking about sort of studying again for the midterm . 
the most basic definition of neural net at the end or some soft max . and really being comfortable with these two final equations that if you understand those all the rest of the models will basically be in many cases sort of fancy versions or adapted versions of these two equations . and then we'll have our standard recurrent neural network that we already went through and we kind of assume you should know for the midterm as well . and our grade of cross entropy error as one of the main loss or objective functions that we optimize . and when we optimize we usually use the mini-batched sgd . we don't go through the entire batch of our trained data but we take 100 or so of examples each time we train . so all those concepts you should feel reasonably comfortable now . and if not then definitely come start sort of studying for the midterm . all right and we'll go over more midterm details in the next lecture . all right now onto the main topic of today machine translation . so you might think for some nlp tasks that you can get away with thinking of all the rules that for a sentence might come up positive or negative right . you say i have a list of all the positive words most of all the negative words . and i can think of the ways you can negate positive words and things like that . and you could maybe conceive of creating a sentiment analysis system of just all your intuitions about linguistics and sentiment . that kind of approach is completely ridiculous for machine translation . there's no way you would ever nobody will ever be able to think of all the different rules and exceptions for translating all possible sentences of one language to another . so basically the baseline that's pretty well established is that all machine translation systems are somewhat statistical in nature . we will always try to take a very large corpus . in fact we'll have so called parallel copra where we have a lot of the sentences or paragraphs in one language . and we know that this paragraph in this language translates to that paragraph in another language . 
one of the popular parallel copra of all of for a long time for the last couple thousand years is the bible for instance . and each paragraph is translated in different languages . that would be one of the first parallel corpora . the very first is actually the rosetta stone . which allowed people to have at least some understanding of ancient egyptian hieroglyphs . and it's pretty exciting if you're into historical linguistics . and it allows basically to translate those to the demotic script and the ancient greek also which we still know . and so we can gain some intuition about what's going on in the other two . now in the next couple of slides i will basically try to bring across to you that traditional statistical machine translation systems are very very complex beasts . and it wouldn't have been impossible for me to say at the end of the lecture all right now you could implement this whole thing yourself after just one lecture going over mt cuz there are a lot of different moving parts . you won't have to actually implement traditional statistical mt system in this class . but i want you to appreciate a little bit the history . and why deep learning is so impactful and amazing for machine translation . cuz it's replacing a lot of different submodules in these very complex models . and sometimes it uses still ideas from this but not very . most of them we don't need any more for neural machine translation systems . all right so let's set the stage . let's call that f such as french . and we have a target language e in our case let's say it's english . so we wanna translate from the source french to the target language of english . 
and we'll usually describe rule where we basically try to find the target sentence usually e here we assume is the whole sentence in the target language . that gives us the largest conditional so this is an abstract formulation . we'll try to fill in how to actually compute these probabilities in traditional and then later in neural machine translation systems . posterior equals its prior times likelihood divided by marginal evidence . marginal evidence here would just be for the source language . so we can drop that argmax would not change from that . so basically we'll try to compute these two factors here . sentence given or the source language given the target times the probability of just the target . and the other one is our language model . remember language modeling where we tried to get the probability of a longer sequence . this is a great use case for it . basically you can think of this as you get some french sentence . your translation model will try to find . maybe this phrase i can translate into that . and this phrase i can translate into this . and then you have a bunch of pieces of english . and then your language model will essentially in the decoder be combined to try to get a single smooth sentence in the target language . so it'll help us to take all these pieces that we have from the translation model . and make it into one sentence that actually sounds reasonable and flows and is grammatical and all that . so the language model helps us to weight grammatical sentences better . 
so for instance i go home will sound better than i go house right . because i go home will have a more more likely english sentence to be uttered . and how would you go about doing this . well if you wanted to translate do this translation model here . then the first thing you'd have to do is you'd find so called alignments . which is basically the goal of the alignment step is to know which word or phrase in the source language would translate to the other word or phrase in the target language . and now again we have these three different systems . and now we're zooming in to the step one of that system . now that one is already hard because alignment is non-trivial . these are actually some cool examples from previous incarnation from chris's class alignment is already hard . and this is for a language pair that is actually quite similar . english and french share a lot and they're more similar . but even if we have these two sentences here like japan shaken by two new quakes . or le japon secoue par deux nouveaux seismes . then we'll basically have here a spurious word . so le was actually not translated to anything . and we would skip it in our alignment . so you see here this alignment matrix . and you'll notice that le just wasn't translated . we don't say the japan or a japan or something like that . 
cuz there are also so called zero fertility words that are not translated at all . so we start in a source and we just drop them . and for some reason the translators or for grammatical reasons and so on they don't actually have any equivalent in the target language . and to make it even more complex we can also have one-to-many alignments . so implemented in english is actually so made into an application the verb implemented here . so then we'll have to try to find . and now as you try to think through alignment for you . you'll have to think so this word could go to either this one word or no word . and you can see how that would create if you tried to go through all the statistics and collect all of these probabilities of which phrase would go to what phrase . it'll get pretty hard to actually combine them all . and language is just incredible and very complex . so you'd have two words in german . and so you have many-to-one alignments making the combinatorial explosion even harder if you try to find good alignments . and lastly you'll also have many-to-many alignments . you have certain phrases like don't have any money . this just goes to sont demunis in french . and so it's a very very complex problem that has combinatorial explosion of all potential all right so now really if you were to take a traditional class you could have several lectures or at least an entire lecture just on the various ways you could implement cleverly an alignment model . and sometimes and other times they actually use parses like the one you're now familiar syntactic parses . and try to find which no not just words but phrases from a parse would map to the other language . and then of course it's not just that . 
and not usually are sentences and you can also have complete reorderings . so german sometimes for sub clauses actually has the verb at the end so you flip a lot of the words and you can't just have this vocality assumption that words rough in this area will translate to roughly a similar area in terms of the sequence of words in the other language . so yeah ja nicht here ja is technically just yes in german also not translated at all . and then actually going over there and going moving also . all right now let's say we have all these potential alignments and now as we start from the source language we say all right . let's say the source here is this german sentence geht ja nicht nach hause . now could be translated into many different words . so german it's technically just the he of he she it as the es in german . but sometimes english as you do your alignment when not unreasonable one is just it or comma he or he will be cuz those were dropped before in the alignment and so on . so you now have lots of candidates for each possible word and for might want to combine now in some principled way to so you have again here a combinatorial explosion of lots of potential ways you could translate each of the words or phrases of various lengths . and so basically what that means is you'll have a very hard search problem that also includes having to have a good language model . so that as you put all these pieces together you essentially try to keep saying or combining phrases that are grammatically plausible or sound reasonable to native speakers . and this often ends up being so-called beam search where you try to keep around a couple of candidates as you go from left to right and you try to put all of these different pieces together . now again this is totally not right we just went in five minutes over what could have been an entire lecture on statistical machine translation or maybe even many multiple lectures . so there are lots of important details we skipped over . but the main gist here is that there's a lot of human feature engineering that's required and involved in all of these different pieces that used to require building and it also meant that there were whole companies that you could form just for machine translation because nobody could go through all that work and really build out a good system . whereas now you have companies that have worked for decades in this and they start using an open-source machine translation system that anybody can download . and now a normal student a phd student can spend a couple months and then he has like one of the best mt systems . which just completely would have been completely impossible in their large groups that all work together in very large systems before in academia . so one of the main problems of this kind of approach is actually that not only is it's also a system of independently trained machine learning models . 
and if there's one thing that i think that i like most when property of deep learning models not just for mt but in all of nlp and maybe in all of ai . is that we're usually in deep learning try to have end to end trainable models where you have your final objective function that you care about and everything is learned jointly in one model . and this mt system is kind of the opposite of that . you have an alignment model you optimize for that and then you have a reordering model maybe and then you have the language model . and they're all separate systems and you couldn't jointly train all of it together . so that's kind of the very quick summary for traditional machine transaction . any high level questions around traditional mt . all right so now deep learning to the rescue maybe probably . so let's go through a sequence of models and see if they would suffice . so the simplest one that we could possibly do is kind of an encoder and decoder model that looks like this . where we literally just have a single recurrent neural network where we have our word vectors so let's say here we translate from german to english echt kiste is awesome sauce in english . and we now have our word vectors here we learned them in german and we have our soft max classifier here . and we just have a single recurrent neural network and once it sees the end of german sentence and there's no input left we'll just try to output the translation . not totally unreasonable it's an end-to-end trainable model . we'll have our standard cross entry pair here that tries to just predict the next word . but the next word actually has to be in a different language . now basically this last vector here if this was our main model this last vector would have to capture the entirety of the phrase . and sadly i've already told you that usually five or six words or so can be captured and after that we don't really we can't memorize the entire context of the sentence before . so this might work for like very short sentenced but maybe not . but let's define what this model would be in its most basic form cuz we'll work on top of this afterwards . 
so we have here our standard recurrent neural network from the last lecture . where we have our next hidden state it's just basically a linear network here followed by and we sum here the matrix vector product with the vector the previous hidden state in our current word vector xt . and that's our encoder and then in our decoder in the simplest form again not the final model in the simplest form we could just drop this cuz the decoder doesn't have an input at that time . this matrix vector product and we just go each time step . it's just basically moving along based on the previous hidden time step . and we'll have our final softmax output here at each time step of the decoder . now i also introduced this phi notation here and basically whenever you have we'll see this only in the next couple of slides . but whenever i write phi of two vectors that means we'll have two separate w matrices for each of these vectors . this is the little shorter notation and then the default here would be well just like i said minimize the cross entropy error for all the target words conditioned on all the source words that we hoped would be captured in that hidden state . all right any questions concerns thoughts about how this model would do . so the comment or question is that neither are the traditional model or this model account for grammar . and in some ways that's not true . so there are actually a lot of traditional models that work on top of syntactic grammatical tree structures . and they do this alignment based prefer potentially the alignment step . but also for the generation and the encoding step and all these different steps . so there are several ways you can infuse grammar and chromatical sort of priors into neuro machine translation systems or so syntactic machine translation systems . it turns out it's questionable if that actually helps . in many cases for machine translation you have such a broad range of sentences . you actually might have un-grammatical sentences sometimes and you still want them to be translated . you have very short complex ambiguous kinds of sentences like headlines and so on . 
so it's tricky the jury was sort of out . and some tactic models were battling it out with non-tactic models until neural machine translation came . and now it's not as important of a question anymore . now for neural systems we would assume and hope that our hidden state actually captures some grammatical structures and some grammatical intuitions that we have . but we don't explicitly give that to the algorithm anymore . which some people who are very good at giving those kinds of features your algorithms might think is sad . but at the same time it's good if we don't have to right . it's less work for us putting more artificial back into artificial intelligence less human intelligence on designing grammars . good question so sometimes the number of input words is different to the numbers of output words and that's very true . so one modification we would have to make to this kind of model for sure is actually say have the last output word here ba stop out putting up words work . like a special token that says i'm done . and one you add that to your softmax classifier sort of the last row . and then you hope that when it predicts that token it just stops . and that is good enough and not uncommon actually for all these neural machine translations . the superscript s is just again to distinguish the different w matrices that we have for hidden connections visible or all right now sadly while neural mt is pretty cool and it is simpler than traditional systems it's not quite that simple . so we'll have to be a little more clever . and so let's go through a series of extensions to this model where in the end we'll have a very big powerful lstm type model . so step one is we'll actually have different recurrent neural network weights for encoding and decoding . so instead of having the same w here we actually should have a different set of parameters a different w for the decoding step . all right so again remember this notation here of fi where every input has its own matrix w associated with it . 
the second modification is that the previous hidden state is kind of the standard that you have as input for during decoding . but instead of just having the previous hidden state we'll actually also add the last hidden vector of the encoding . so we call this c here but it's essentially ht . so at this input here we don't just have the previous hidden state but we always take the last hidden and we have again a separate matrix for that . and then on top of that we will also add and that's actually if you think about it it's a lot of parameters we'll add the previous predicted output word . so as we translate we have three inputs for each hidden state during the decoding step . we'll have the previous hidden state as a standard recurrent neural network . we have the last hidden state of the encoder . and we have the actual output word we predicted just before that . and this will essentially help the model to know that it just output a word and it'll prevent it from outputting that word again . cuz it'll learn to transform the hidden state based on having just upload a specific word before . so whenever you have fi of xyz here it'll just f of w times x + u of y + v of z . so you just i don't wanna define all the matrices . so why do we need to make y t minus one a parameter if we actually had computed yt minus one from ht minus one right . so two answers one it will allow us to have the softmax weights also modify a little bit how that hidden state behaves at test time . and two we actually can choose usually yt and there are different ways you can do this . you could take the actual probability the multinomial distribution from the softmax . but here we'll actually make a hard choice and we chose exactly this one . so instead of having the distribution we'll make a hard choice . and we say this is the one word the highest probability that had the highest probability we predicted that one and that's the one we give us input . 
so it turns out in practice that helps to prevent the model from repeating words many times . and again it incorporates the softmax weights in that computation indirectly . that is not how we define the model . in theory again so i didn't define it but you can also you can do the same thing with the softmax and this is what the picture actually shows . so instead of having a softmax of just w you can also concatenate here your c and that's what the picture said . but i wanted to skip over the details so you caught it well done . so this model usually so the question is do we have kind of a look ahead type thing . and the model basically has to output the words in the right order . and it doesn't not have the ability to do this whole reordering step or look ahead kind of thing . or there's no sort of post processing of reordering at the end so this model isn't able to output the verb at the right time stamp . now of course once it works well everybody will try to see if they can kind of improve it and eventually you can do beam searches too for these kinds of models . but surprisingly in many cases you don't have to get a reasonable mt system . become more and more familiar to be able to read the literature . so the same picture that we had here and the same equations we defined here's another way off looking at this . so with the exception that this one doesn't have the c connection that you caught . it's the same exact model just a different way to look at it and it's kind of good to see . sometimes people explicitly write that you start out with a discreet one of k and coding of the words . it's just like you want one-hot vectors that we defined and then you embed it into you give those as input you compute your recurrent neural network ht steps . and now you give those as input to the decoder . and that each time stamp of decoder you get the one word sample that you actually took as input the previous hidden state and to see vector we defined before . 
so all these three already are the inputs for each node in this recurrent neural network . so just a different picture for the same model we just defined so you learn picture in variances first model semantics . it needs to get more powerful cuz even with those two assumptions here we have a very simple recurrent neural network with just one layer that's not going to cut it . so we'll use some of the extensions we discussed in the last lecture we'll actually have stacked deep recurrent neural networks where we have multiple layers . and then we'll also have in some cases this is not as common but sometimes it's used we have a bidirectional encoder . where you go from left to right and then we give both of last hidden states of both directions as input to every step of the decoder . and then this is kind of almost an xor here . if you don't do this than another way to improve your system slightly is by training the input sequence in reverse order because then you have a simpler optimization problem . so especially for languages that align reasonably well like english and french . you might instead of saying a b c the other word's a the word b or c goes to in the different language the words x and y . you'll say c b a goes to x y because as they align a is more likely to translate to x and b is more like to y . and as you have longer sequences you basically bring the words that are actually being translated closer together . and hence you have less of a vanishing gradient problems and so on because where you want the work to be predicted it's closer to where it came in to the encoder . that's right but yeah it's still an average force . so how does reversing not mess it up . cuz this sentence doesn't make grammatical sense . so we never gave this model an explicit grammar for the source language or the target language right . it's essentially trying in some really deep clever continuous function general function approximation kind of way just correlation basically right . and it doesn't have to know the grammar but as long as you're consistent and you just reverse every sequence the same way . and the model reads it from potentially both sides and so on . 
so it doesn't really matter to these learning models as long as your transformation of the input is consistent across training and testing times and so on . so the question is he understands the argument but it could still change the meaning . and it doesn't change the meaning if you assume the model will always go from one direction to the other . if you start to sometimes do it and sometimes not then it will totally mess up the system . but as long as it's a consistent transformation it is still the same order and so you're good . imagine you had a very long sequence here . and again this is only the case if the languages align well . as in usually the first capital words in one of the source language translated to first capital words in the target language . now if you have a long sequence and you try to translate it to another long sequence and say there are a lot of them here . now what that would mean is that this word here is very far away from that word cuz it has to go through this entire transformation . and likewise these words are also very far away . so everything is far away from everything in terms of the number of non-linear function applications before you get to the actual output . now if you just reverse this one then this word so let's call this a b c d e f . now this is now f e d c b a . so now these two are very very close to one another . and so as you do back propagation and we learn about the vanishing creating problem in the last lecture you have much less of a vanishing creating problem . it'll be much better at translating those . so how does this check work for languages with different morphology . it doesn't actually matter but the sad truth is also that very few mt researchers work on languages with super complex morphology . so like finnish doesn't have very large parallel corpora of tons of other languages . 
and so you don't sadly see as many people work on that . and for german actually a lot of other tricks that we'll get to . and really these tricks are not as important as the one as trick number six . but before that we'll have a research highlight . so i'm gonna talk about building towards a better language modeling . so as we've learned last week language modeling is one of the most canonical task in nlp . and there are three different ways we can make it a little bit better . we can have better regularization or preprocessing . and eventually we can have a better model . have all played with glove and that's a word level representation . from you guys who are down there . so in fact you can code the word at a subword level . you can eventually do character level embedding . what it does is that it drastically reduce the size of your vocabulary make the model prediction much easier . so as you can see tomas mikolov in 2012 and yoon kim in 2015 explored this route and got better results compared to just plain word-based models . so another way to improve your model is that one of the bigger problems for language modelling is over-fitting . and we know that we need to apply regularization techniques when the model is over-fitting . so there are a bunch of them but today i'm gonna focus on preprocessing because it's a little bit newer . what preprocessing does is that we know that we're never gonna have unlimited training data . so in order to have our corpus look more like the true distribution of the english language what we can do is quite similar to computer vision we can do this type of data augmentation technique where we try to replace some words in our corpus with some other words . 
so for example your model during the first pass you can see a word called new york the next pass you can see new zealand the next pass you can see new england . so by doing that you're basically generating this data by yourself and eventually you achieve a smoothed out distribution . the reason this happens is that more frequent word by replacing by dropping them . they appear less often and rarer words by making them appear . so a smooth distribution allow us to learn a better language model and the result is on the i think is on the right hand side of you guys . and the left hand side is what happen when we apply better regularization techniques . so at last we can wait that's it okay awesome thank you guys . in these tables is that the default for all these models is an lstm and that's exactly what we'll end up very soon with . which is basically a better type of recurrent unit . and so we'll start with gated recurrent units that were introduced by cho just three years ago . and the main idea is that we wanna basically keep around memories that capture long distance dependencies and you wanna have the model learn when and how to do that . and with that you also allow your error messages to flow differently at different strengths depending on the input . what is a gru as our step to the lsdm . and sometimes you don't need to go all the way to the lsdm . the gru is a really good model by itself . in many cases already in its simpler . so let's start with our standard recurrent neural network which basically computes our hidden layer at the next time step directly . so we just have again previous hidden state recurring to our vector that's it . gated recurring units or grus is we'll compute to gates first . these gates are also just like ht continuous vectors of the same length as the hidden state and they are computed exactly the same way . 
and here it's important to note that the superscripts that's just basically are lined with the kind of gate that you're computing . so we'll compute a so called update gate and a reset gate . now the inside here is the exact same thing but is important to note that we so we'll have elements of this vector are exactly between zero and one . and we could interpret them as probabilities if we want to . and it's also important to note that the super scripts here are different . uses a different set of weights to the reset gate . now why are they called update and reset gates and how do we use them . we just introduced one new function here just the element wise product . we also call it the hadamard product sometimes . where we just element wise multiply this vector here from the reset gate with this which would be our new memory content . we call it ht this is our intermediate memory content we also know as a [inaudible] . this part here is exactly the same we just have to input our word vector and then transformed with a w . so intuitively right this is just a long vector of numbers between zero and one . now intuitively if this reset gate at a certain unit is around zero then we essentially ignore all the past . we ignore that entire computation of the past and we're just going to define that element where our zero now why would we want to do that . let's take the task of sentiment analysis cuz it's very simple and intuitive . if you were to say you're talking about a plot of a movie review . and you talk about the plot and you know some girl falls in love for some guy who falls in love with her but then they can't meet blah blah blah . that's a long plot and in the end you say but the movie was really boring . then really doesn't matter that you keep around that whole plot . 
you wanna say boring as a really negative strong word for sentiments and you wanna basically be able to allow the model to ignore the previous plot summary . cuz for the task of sentiments analysis it's irrelevant . now this is essentially what the reset gate will let you do but of course not in this global fashion where you update the entire hidden state but in a more subtle way where you learn which of the units you actually will reset and which ones you will keep around . so this will allow some of the units to say well maybe i want to be a plot unit and i will keep around the plot . but other units learn well if i see one of the sentiment words i will definitely set that reset gate to zero and i will now make sure that i don't wash out the content with previous stuff by summing these two right . you're sort of like not quite averaging but you're summing the two . so you wash out the content from this word and instead it will set that to zero and take only the content from that current word . now the final memory it will compute we'll combine this with the update gate . and the update gate now basically allows us to keep around only the past and not the future . so intuitively here when you look at z then what we would do is essentially do ht = ht-1 + 1-1 is 0 so this term just falls away . basically if zt was all ones we could just copy over our previous time step . super powerful if you copied over the previous time step you have no vanishing gradient problem right . your vector just gets a bunch of ones . so that's very powerful and intuitively you can use that same sentiment example . but you say in the beginning man i love this movie so much here's this beautiful love story . and now you go through the love story and really what's important for sentiment is not about the love story but it's about the person saying i love this movie a lot . and you wanna make sure you don't lose that information . and with the standard recurring neural network we update our hidden state every time every word . no matter how unimportant a word is we're gonna sum up those two vectors move further and further along . here we can decide and what's even more amazing you don't have to decide . 
you can say this word is positive so i'm gonna set my reset gate manually . no the model will learn when to reset and when to update . so this is a very simple kind of modification but extremely powerful . now we're gonna go through it and explain it a couple more times . have an attempt here at a clean illustration . honestly personally i feel the equations here are still straight forward and very intuitive that i don't know if these illustrations always help but some people like them more than others . so intuitively here you basically see that only the final memory that you computed is the one that's actually used as input to the next step . so all of these are only modifying through the final state . and now this one gets as input to our reset gate or update gate the intermediate state and the final state of the memory . and so does our x vector the word vector here also gets its input through the reset gate the update gate and our intermediate memory state . and then i tried to use this so the dotted line here as basically gates that modify how these two interact . all right so i've said i think most of these things already but again reset gate here is close to 0 . and that again allows the model in general to drop information that is irrelevant for the future predictions that it wants to make . and if we update the gate z controls how much of the past state should matter at the current time stamp . and again this is a huge improvement for the vanishing gradient problem which allows us to actually train these models on nontrivial long sequences . does it matter if you reset first or update first . well so you can't compute h until you have h tilled . so the order of these two doesn't matter . you can compute that in peril but you first have to compute h tilled with the reset gate before you can compute that one . so the question is does it matter to switch and use an equation like this first and then an equation like that . 
i guess it's just a different model . it's not one that i know of people having tried . it's not super unreasonable i don't see a sort of reason why it would be illogical to ever to that but yeah just not the gru model . you will actually see in [inaudible] she has a paper on a search space odyssey type paper where there are a thousand modifications you can make to the next model the lstm . and people have tried a lot of them and it's not trivial . and a lot of times they seem kind of intuitive but don't actually change performance that much across a bunch of different tasks . but sometimes one modification improves things a tiny bit on one of the tasks . it turns out the final model of gru here and the lstm are actually incredibly stable they give good performance across a lot of different tasks . but it can't ever hurt to if you have some intuition of why you want to have make something different it can't hurt to try . so the question is is it important of how they're computed . i think there are some people who have tried once to have a two layer neural network to compute . these a z and update z and r . in general it matters of course a lot of how they're computed but not in the sense that you have to modify them manually or something . it just the model learns when to update and when not to update . so what do i mean when i say unit . so in general what you'll observe in a slide that's coming up very soon is that we will kind of abstract away from the details of what these equations are . and we're going to write that just ht equals gru of xt and ht minus 1 . and then we'll just say that gru abbreviation means all these other things all these equations and we're going to abstract away from that . and that's something that you'll see even more in subsequent lectures where you just say a whole recurrent network with a five layer gru and combine lots of different ways is just one block . we often see this in computer vision too where cnns are now just like the cnn block and you assume you've got a feature vector out at the end . 
and people will start abstracting but yeah you'll always have to remember that yes there's a lot of complexity inside that unit . here's another attempt at an illustration which i'm even less of a fan of then the one i tried to come up with . basically how you have your z gate that kind of can jump back and forth . but of course it's usually a continuous type thing . it's not a zero one type thing so i'm not a big fan of this kind of illustration . and so in terms of derivatives we couldn't theory asks you to derive all the details of the gru . and the only change here is that we now have the derivative of these element wise multiplications both of which i have parameters or inside . and we all should know what derivative of this is and the rest is again the same kind of chain rule . but again now you're sort of realizing why we wanna modularized this more and more and abstract a way from actually manually taking these instead having error messages and deltas sent around . explain why we have both update and reset . so basically it helps the model to have different mechanisms for when to memorize something and keep it around versus when to update it . you're right in theory you could try to put both of those into one thing right . in theory you'd say well if this was just my previous ht here then this could say well i wanna keep it around or i wanna update it here . but now this update here if you just had an equation like this it would be still be a sum of two things . so that means that xt here does not have complete control over modifying the current it would still be summed up with something else and that happens at every single time stamp . so its only once you have this reset gates are here . these reset gates are here that you would allow h to be completely dominated by the current word vector if the model so chooses . if the reset gates are all okay so if these are all ones then you have here basically a standard recurrent neural network type equation . and then if you just have zs all 0s then you take that exact equation and you're right . then you just have a standard rnn . 
it's also beautiful it's always nice to say my model is a more general form of your model or- you're model's a special case of my model . it was actually a couple years ago that you could by and say that . and likewise the inventor of this model made exactly that statement about the gru . not knowing why anybody had to publish a new paper about this instead of just referring to this and the special cases of the lstm . so if we have one more why tanh and sigmoid . so in theory you could say the tan h here could be a rectified linear unit or other kind of unit . in practice you do want sigmoids here because you have this plus 1 minus that . and so if they're all over the place then everything will kind of be modified and it's less intuitive that you kind of have a hard reset in sort of a hard sort of yeah hard reset or a hard update . and if this wasn't 10h and was rectified linear unit then these two might be all over the place too and it might be kind of easy to potentially have the sum also the not very synthecal . but at the same time it's not unreasonable to try having a rectified learning unit here . and maybe if you combine it with proper regularization and so on you could get away with other kinds of other kinds of linearities . that's unlike probabilistic graphical models for certain things just make no sense . and you can't do them deep learning you can often try some things and sometimes even nonsensical things surprisingly work . and then other people try to analyse why that was the case in the first place . but yeah there's no mathematical reasons why you couldn't at all have a rectified linear unit here . all right now on to a even more complex sort of overall recurrent unit . so now this is the hippest model of the day and it's pretty important to know it well . fortunately it's again very similar to the kinds of basic building blocks . but now we allow each of the different steps to have again we separate them out even more . so how do we separate them out . 
basically this is what's going on at each time step . we will have an input gate forget gate output gate memory cell final memory and a final hidden state . now let's gain a little bit of intuition and there is good intuition of why we want any of them . so the input gate will basically determine how much we will care about the current vector at all . so how much does the current cell or the current input word vector matter . the forget gate is a separate mechanism that just says maybe i should forget maybe i don't . in this case here just kind of counterintuitive sometimes and they're actually different models in the literatures . some have the one minus there and others don't . but in general here if it's 0 then we're forgetting the past . then we have an output gate basically when you have this output gate you will separate out what matters to a certain prediction versus what matters to being kept around over the current recurrent time steps . so you might say at this current time step this particular cell is not important but it will become important later . and so i'm not going to output it to my final softmax for instance but i'm still gonna keep it around . so it's yet another separate mechanism to learn when to do that . and then we have our new memory cell here which is similar to what we had before . so in fact all these four here have the same equation inside and just three sigmoid non linearity and one tan h non linearity . so these are all just four single layer neural nets . now we'll put all of these gates together when we compute the memory cell and the final hidden state . so the final memory cell now basically separated out the input and the forget gate . instead of just c and 1 minus c we have two separate mechanisms that can be trained and learn slightly different things . and actually become also in some ways counter intuitive like you say i don't wanna forget but you do wanna forget but you also input something right now . 
but the model turns out to work very well . so basically here we have final hidden state is just to forget gate how to mark product with the previous hidden states final memory cell ct-1 . how much do you wanna keep this around or how much do we wanna forget from the past . and then the new memory cell here this has a standard recurrent neural net . if i is all 1s then we really keep the input around . and if the input gate says no this one doesn't matter then you just basically ignore the current word back there . so in that sense this equation is quite intuitive right . forget the past or not take the input or not that's basically it yeah . so the secret question once you forget the past does it mean you forget grammar or something else . and the truth is we can think of these forget gates as sort of absolutes . they're all vectors and they will all forget only certain elements of a long hidden unit . and so really i can eventually show you what these hidden states look like . and sometimes they're actually more intuitive than others . but it's rare that you would find this particular unit when it was turned off or on actually had like this perfect interpretation that we as humans find intuitive and think of as grammar . and so it's hard to say any single unit would capture any particular like entirety of a grammar it might only capture certain things . so it's not implausible to think suggest that the next noun should be a plural noun or something like that . but that's the most we could hope for in many cases . all right and then here we can keep these cs around right . and cs will compute our computer from other cs . but we might not want to expose the content of this memory cell hidden state ht minus 1 . 
all right now yeah it's a really powerful model are there any questions around the equations . we're gonna attempt at some illustrations again i think the equations does the lstm and gru completely liviate or just help with an engine came problem . and the truth is they helped with it a lot but they don't completely obviate it . you do multiply here a bunch of numbers that are often smaller than 1 . and over time even if it would have to be a perfect one but that would mean that that unit is really really strongly active . and then it's hard to sort of dies it's like the gradient when you have unit that's really really active and looks something like this . to that unit and it's here then grade in around here it's pretty much 0 . and then the model can't do anything with it anymore . you'll observe some units just sort of die after training after awhile . and you'll just sort of keep around stuff or delete stuff at each time step . but in general most of the units are somewhat small than 1 and so you still have a bit of a vanishing creating problem but much less so . and intuitively you can come up with final p for a lot of good ways to think about this right . maybe you want to predict different things at different time steps . but you wanna keep around knowledge through the memory cells but not expose it at a given prediction . what is the point of the exposure gate when it already had the forget gate . so basically you want to sort of forget gate will tell you whether you keep something around or not . but exposure gate will mean does it matter to this current time step or not . so you might not wanna forget something . but you also might not wanna show it to the current output because it's irrelevant for that output . and it would just confuse the softmax classifier at that output . 
does the exposure gate help you or do you mean the output gate here right . so does the output gate does it help you to what exactly . to not have to forget everything forever . you can basically while it doesn't wanna give as output something for a long time . and hence it's basically it will only be forgotten at that time set but actually be kept around in . i don't wanna use like anthropomorphize the models but like the subconsciousness of this model or whatever right . keeps it around but doesn't expose it . the initialization to all these models matters it matters quite significantly . so if you initialize all your weights for instance such that whatever you do in the beginning all of the weights are super large . then your gradients are zero and you're stuck in the optimization . so you always have to initialize them properly . in most cases as long as they're relatively small you can't go too wrong . eventually it might slow down your eventual convergence but as long as all your parameters w here and your word vectors and so on are initialized to very small numbers . it will usually eventually do it pretty well . yes you could use lots of different strategies for initialization . i like this one from chris olah on his blog from not too long ago . i feel like the equations speak mostly for themselves . i have four different neural network layers and then you combine them in various ways with pointwise operations such as multiplication or addition . and sometimes you know multiplication and then addition and concatenation and copies and so on . but in the end you often observe this kind of thing where we'll just write lstm in this block . 
and has an x and an h and we don't really look into too many details of what's going on there . and here's some i think even less helpful [laugh] illustrations that yeah i think are mostly output gates input gates and so on . but and your memory cells as they try to modify each other . you know you have some inputs your gates you have your forget gates on top of your memory cell and so on . but in general i think the equations are actually quite intuitive right . if you think of your extremes if this is zero one then this input matters more to the output . all right now as i said lstms currently super hip . the en vogue model are for pretty much all sequence labeling tasks and sequence to sequence tasks like machine translation . super powerful in many cases you will actually observe that we'll stack them . so just like the other rnn architectures we'll have a whole lstm block and we put another lstm block with different sets of parameters on top of it . and then the parameters are shared over time but are different as you have a very deep model . and of course with all these parameters here we have essentially many more parameters then the standard recurrent neural network . where we only have two such parameters and we update every time . you wanna have more data especially if you stack you now have 10x the parameters of standard rnn we wanna train this on a lot of data . and in terms of amount of training data available machine translation is actually one of the best tasks for that . and is also the one where these and so in 2015 i think the first time i gave the deep learning for nlp lecture the jury was still a little bit out . the neural network models came up fairly quickly . but some different more traditional machine translation systems were still slightly better like by half a bleu point . you can essentially think of it as an engram overlap . the more your translation overlaps in terms of unigrams and bigrams and trigrams the better it likely is period . 
so you have this reference translation sometimes multiple reference translations . you have your translation you look at engram overlap between the two . and basically the neural network models were often also just use it for rescoring traditional mt model . now just one year later last year really a couple months ago the story was completely different . and you have different universities and different companies and so on submit their systems . and the top three systems were all neural machine translation systems . the jury is now basically not out anymore . it's clear neural machine translation is the most accurate machine translation model in the world . yeah that number two was us yeah . james bradbury was actually a linguistics undergrad while he was doing that but now he's full-time . so yeah basically we haven't talked that much about ensembling and ensembles of different models . but you can also train five of these monsters and then average all the probabilities and you'll usually get a little better . we just as general thing you'll observe for every competition machine learning competition out there . if you go on kaggle other machine learning competitions usually train even the same kind of model five times . you end up in slightly different local optimum average what's cool also though is that while we might not be able to exactly recover grammar or have specific units be explicitly sort of capturing very intuitive things . as we project this down similar to the word vectors we actually do observe some pretty interesting regularities . so this is a paper from sutskever in 2014 they projected different sentences . they were trained basically with a machine translation task and basically observe quite interesting regularities . to john is in love with mary and to john respects mary . now of course we have to be a little carefull here to not over interpret the amazingness . 
it's amazing but we also have a selection vice here right . maybe if we just had john did admire mary or something it might also be close to it right . and it might be closer too but if you just project these six particular sentences into lower dimensional space . then you do see very nicely that whenever john has some positive feelings for mary all those sentences are in here . and all the ones that are on this area of the first two item vectors mary admires john mary admires john mary is in love with john and mary respects john . they're all closer together which is kind of amazing cuz well it's a sequence model so how could it ever capture that the word order changes . so here we have she was given a card by me in the garden versus in the garden i gave her a card . and i gave her a card in the garden and despite the word order being actually flipped right . in the garden is in the beginning here and in the end here . these are still closer together than the different ones where in the garden basically she gave me a card verses i gave her a card . so that shows that the semantics here turn out to be more important than the word order . despite the model just going from left to right or this one was still the trick where we reversed the order of the input sentence . but it choses that its incredibly invariant and variance is a pretty important concept right . we want this model to be changes when the semantics are actually kept the same . it's pretty incredible that it does that . so this is also the power i think of some of these . this is a very deep lstm model where you have five different lstm stacked in the encoder and several in the decoder . and they're all connected in multiple places too . all right any questions around those visualizations and lstms . all right you now have knowledge under you belt that is super powerful and very interesting . 
i expected to maybe have five minutes more of time . so i'm going to talk to you about a recent improvement two recurrent neural networks that i think is also very applicable to machine translation . but nobody has actually yet applied it to machine translation . with all softmax classification that we do in all the models i've so far described to you . and really up until two or three months ago that everybody in nlp had as a major problem . and that is you can only ever predict answers if you saw that exact word at training time . and you have your cross entropy error saying i wanna predict this word . and if you've never predicted that word no matter how obvious it is for the translation system it will not be able to do it right . so we have some kind of translation and let's us say we have a new word like a new name or something that we've never seen at training time . and it is very obvious that this word here should go at this location . this is like mrs. and then maybe the new word is like yelling or something like that it could be any other word . and now let's say at training time we've never seen the word yelling . but now it's like vowel german misses miss in yeah german translation for this . and now it's very obvious to everybody that after this word it should be the next one the name of the of the miss . and so these models would never be able to do that right . and so one way to fix that is to think about character meant translation models where the model's actually surprisingly similar to what we described here . well many times it have to go but instead of having words we just have characters . so that's one way but now we have very long sequences . and at every character you have a lot of matrix multiplications . and these matrix multiplications that we have in here are not 50 dimensional for really powerful mt models they're a 1000 dimensional . 
and now you have several thousand by a thousand matrices here multiplying with thousand dimensional vectors . and you stack them so doing that for every single character actually gets really really expensive . so at the same time it's very intuitive that after we see a new word at test time we wanna be able to predict it . and also in general when we have the softmax even for words that we do see once or twice it's hard for the model to then still predict them . it's this skewed data set distribution problem . but you have very rare very infrequent classes our words are hard to predict for the models . so this is one attempt at fixing that which is essentially a mixture model of using standard softmax and what we call a pointer . it's essentially a mechanism to say well maybe my next word is one of the previous words in the context . you say 100 words in the past and every time step you say maybe i just wanna copy a word and if not then i will use my standard softmax for the rest . so this is kind of this sentinel idea here . this is a paper by stephen merity and some other folks . and basically we now have a mixture model where we combine the probabilities from the standard vocabulary and from this pointer . and now how do we compute this pointer . it's very straightforward we basically have a query . this query is just a modification of the last hidden layer that we have here . and we pipe that through a standard single layer neural network to compute another hidden layer which we'll call q . and then we'll do an inter product between this q and all the previous hidden states of the last 100 timed steps . and that will give us basically the single number for each of these interproducts . and then we'll apply a softmax on top of that . and this gives us essentially a probability for how likely do we wanna point to each of these words . 
or the very last one is we don't point to anything we just take the standard softmax . so we keep one unit around where we do this . and now of course in the context the same word might appear multiple times . and so you just sum up all the probabilities for specific words . if they appear multiple times you just sum them up . with this simple modification we now have the ability to predict unseen words . we can predict based on the pattern of how rare words appear much more similar things . for instance fed chair janet yellen raised rates and so on ms . is very obvious that this is the same ms . and you can base or you combine this in this mixture model . and now over many many years for language modeling . the perplexity that we defined before was sort of stock actually around 80 . and then in 2015 we have a bunch of modifications to lstms that were very powerful . and lower this and now were down to the lowest 70s . and was some modifications will cover another class were actually down on the 60s now . so it really had to told for several years and now perplexity numbers are really dropping in . and this models are getting better and bettered capturing more and more the semantics and the syntax of language . this is a pretty advanced lecture i hope you gained some of the intuition . again most of the math falls out from the same basic building blocks we had before . and next week or no next thursday we'll do midterm review . 
stanford university welcome to the midterm of this session for cs224n . i'm just gonna start with some announcements . so the first one as i hope you're aware of homework 2 is due today . and we had an issue with the submissions script that made submissions unavailable for a couple hours . so we apologize for that but that is now fixed . there's a slight change to the submission instructions to the initial version . and that is we don't want you to include the trained weights for your models because it turns out and we run out of afs and we can't take any more submissions . the other thing due today is the project proposal so these are both due at midnight . and we are saying that if you have not yet found a mentor but you would still like to do a final project other than the default final project you can still submit a project proposal . and say that you've not found a mentor yet and we would do our best to find you a mentor anyway . now for a couple of notes on the midterm . same time as the regular lecture but i want to point out that it?셲 not being held here . there is an alternate exam only if you cannot at all make the february 14th is normal midterm time . and if you want to take the alternative exam or you have to take the alternative exam i'll post about that on piazza . we are allowing a single cheat sheet . and the material on this midterm the most recent one . the midterm is gonna be a mix of multiple choice and true/false questions short answer questions and some longer more involved questions that may involve radiant computation . and for scpd students you have to turn up in person or have an exam monitor pre-registered . the rest of this lecture is going to be just review material to help prepare you for the midterm . and in particular we're going to cover word vector representations neural network basics back propagation and gradient computation . 
and we're gonna do some example problems on the white board for that section rnns and dependency parsing . and with that i will leave it to our first section word vector representations . all right we can start that again . for every word we could come up with a vector that encapsulates semantic information about the word . there could be various ways of interpreting what meaning or semantic representation could be . and which is why it so severely depends on how you actually go about training your word vectors . in this class we first covered word2vec and glove and we'll be reviewing that now . so just to recap word2vec the task for it is learn word vectors to encode the probability of a word given its context . now consider this example our window size is 2 here . if you consider the center word to be word2vec understand the and model now become context words . and you'll see how to probabilistically model this now . so for each work we have two vectors . the input vector and the output vector then for vectors represented by v and the output vector by u . and we'll see how v and u are used in the model now . so we'll be covering two algorithms here skipgram which predicts the probability of the context words given the center word . and the continuous bag-of-words or cbow which predicts the center word from surrounding context . all right so to recap on skipgram consider the sentence again where i understand the word2vec model now . and in this case since the center word is word2vec that'll be the only word that's left and the context words will be omitted . and our job is to actually go about and assign probably the words there . so how do we go about doing this . 
in this case this vector will have the same dimensions as the number of words in a vocabulary with one of the index of the word word2vec and 0 everywhere else . now we need to use this one-hot vector to index into over embedding matrix which is capital v the input vector in embedding matrix . and we looked in a word vector corresponding to that word . we then go and do a doc product with the output word vectors hue and to generate a score for it . so once we get a score we get the probabilities by using softmax . and once we have those we have softmax probabilities for every context word . and we can then find out which word actually would go into these context windows . how do we actually go about training this . we assign over a cost which is either given by softmax cross-entropy or the negative sampling loss . and as you can see the formula takes abstracts or so we take the input vector and we take the word vector for the context words and we apply these cost functions and sum them over okay . and our job is to minimize this loss . i'm using cost and loss interchangeably as we will do that in the midterm in the class as well . now we move onto continuous bag of words now let's take the previous sentence again . in this case our job is to actually predict the context word or the center word my bad given the context words . so in this case our job would be to actually guess the word2vec model here . so in this case as in the previous case as well we generate one-hot vectors for each of the context words . and this would again be the size of your vocabulary with one at the index of the word itself . and then we look into our embedding matrix again the input vector embedding matrix and obtain the word vectors for those context words . we take the average of those word vectors and then compute the score again by multiplying with the output vector matrix . we can get the probabilities by computing the softmax function . 
and now we have for each of the words in the vocabulary . we have the probability assigned that what how likely it is to be the center word . this should be pretty clear since we also had a live coding session on this . the cost function here is similar to the one in skipgram except in this case it takes in the average of the word vectors as one of the arguments and the center word itself . and again our job is to minimize the loss here . we didn't do this as a part of the assignment but glove is a pretty famous set of word vectors as well . like word2vec it also captures semantic information about the words . but unlike word2vec however it also takes into account the co-occurence statistics . and we'll see what we refer to just in a second . from one of the lines in our review notes glove consists of a weighted least squared model that trains on global word co-occurrence accounts . so what do we mean by co-occurence . consider our corpus to be only a set of three sentences . i like nlp and i enjoy flying . now you can see that we can come up with a n by n where n represents the number of words in our corpus . a matrix where each index corresponds to whether word ga belongs in the context window of word i this is a symmetric matrix as well . so you can as you can assume . so let us denote this matrix by x . and so if you index into this using -ing that would be the number of times word j occurs in the context of word i . and let us also denote xi to be the number of times any word appears in the context window of a word i . just like in word2vec we also have two vectors for each word here the input vector and the output vector . 
and the cost function is given by the equation here . there are a couple of interesting things about this cost function which we'll cover right in a second . but a few things to note here we're applying the least squares as we mentioned earlier . we are iterating over every pair of words in the co-occurrence matrix . what's interesting is that since you can imagine words like d or a the articles as the stock words would have a large xij value we need to sort of clip on it . and the other option is to just take the logarithm of it . and so which is why you can see there's a log over the xij over there okay . so any questions about the cost function here . so now after minimizing this loss for the glove word vectors they will be having v and u where v represents the output word vectors and u represents the input . and since both of these capture similar co-occurrence information we can obtain the word vector for a single word just by summing them up . so just u plus v will equal the word vector for that particular word . and we can use that for all sorts of nlp tasks . it's essentially an embedding for that word . so before we move on from word vector presentations any questions on any of the terminology . we got a lot of questions about how does an embedding defer from a word vector . we want to make sure that that definitions ambiguity is cleared up . it is a vector i mean it is a scalar . if xi is a vector shouldn't the laws actually be referring to a scalar . xi is actually a scalar because it's solely the number of times that word appears in the context window of any other word . so xi is defined as the number of times any word key appears in the context so it's not a vector . 
most research papers tend to use embeddings and word vectors pretty interchangeably . basically you can imagine it's a a vector that encapsulates semantic information . so in terms of analogies you can imagine that if you had word vectors for king man queen and woman then if you subtract king minus queen it should be roughly equal into sweet any other questions . so those input vectors and output vectors are solely just differing in the sense that if you have a context word we are going to predict the output vector for it . so in any case you're considering a word solely you're considering only the output vectors . okay all right so let's cover neural networks . this will be roughly quick because we also have rnns and lstms to cover . you have a data set x and y where x is the input y are the labels . and you want to train a 1-hidden you do a forward pass given by xw + b and then you apply non-linearity or a activation function on it . you compute the loss you compute the gradients from that loss using back propagation . we'll go into back propagation with barack . then we update the weight using an optimization algorithm like saccustic gradient descent or sgd . we do hyperparameter tuning of dev set not on the test set . and then evaluate the neural network on the test set itself all right . so this was a very high level of review of how neural networks have changed . let's go where the activation functions are the nonlinearities quite quickly . what it does it takes and input and squashes at between 0 and 1 . if the activation function was very large if the activation was very large is still always end up being 1 . or if the activation is really small it's still end up always being 0 . so you can imagine that a lot of neurons get saturated by this . 
and the output is not centered as 0 . this is particularly bad because if the output for our sigmoid is always a positive number then the gradients are always negative or positive . and we do not want that we want the gradients to be more adaptive okay . another issue this is also that takes the exponent and it's computationally expensive but nothing in here can run with the map there . okay then the tan h function at between negative 1 and 1 . here in this case the output is centered at 0 so which is a nice thing and it sort of resolves however similar to a sigmoid this also kills the gradients at saturation . and which is why tan h is not as good as well . right so if your input if the output of your node . the question was why is it bad that the output of our activation function is is not centered at 0 . so if the output of your activation function is always positive then the gradients are always positive or always negative because they can only go in one of the two directions always . they do break the so we are able to train networks with this function . so it's not like it's the end of the world in terms of training neutral networks . so in this case what would actually be better is that you take your input and you center it at zero either way works . moving on to relu the rectify linear unit which essentially takes some acts on checks if it's greater than 0 . if it's greater than 0 it returns the same value . it does not have the problem of saturation because it's linear . it's confidentially cheap there are no explain on calculations . and it's also empirically known to converge faster right . so which is why i have the second point in the problem says referring to that . yeah so the problem with this is that it's again not centered at 0 . 
the larger problem with this and it's slightly annoying as well is that if the input is less than 0 then the relu gradient is always 0 . and this is a problem because once a neuron dies it always is dead . you can never revive a neuron after the input becomes less than 0 . anyways moving on to stochastic gradient descent or the optimization algorithm . in this case theta represents our weights or the parameters . alpha is our learning rate and j is our loss or the cost function . and going very technically sgd update happens after every training example . however researchers tend to abuse the notation and call minibatch sgd also normal sgd . gradient descent does is it takes a small batch of averages their loss over before performing the gradient update . okay moving on to regularization this is a form dropout is a form of regularization . while this might seem very counterintuitive that it should work it does because it prevents overfitting . it forces the network to learn dependencies . neurons to learn the same task it forces each neuron to capture more important information about your data and learn more important abstractions . so think about dropout as training an ensemble of networks . every time you run your network during forward pass a couple of neurons are dropped . and as a result you are getting a new network almost every time and then you're training an ensemble of networks . during test time make sure that you turn your dropout also that you again take this ensemble of networks and then use the shared power between all of these networks . so the question was do you have to scale your output whenever you run dropout at tester . and just because if during test time you make sure that your activations are already scaled correctly then you don't have to do it during test time . so a few training tips and tricks here . 
these are a couple of loss plots . so the one in the green represents a phenomenon when your loss is very noisy . and what happens you can see the jagged line . and in this case what must be happening is that since your gradient updates are very large you are not converging properly . you are almost iterating over and around the local minima which is why it's advisable to lower your learning rate in that case . if it's blue you see that there is high potential for your network to actually train faster . and hence you should make your learning rate higher . the red line is sort of an ideal one . again this is something i just drew up on paint . so it's not something that's very technical . so this is something you must have faced issues with in the last assignment . where if the gap between your training curve and your dev accuracy is very large that means you're overfitting . and in this case there are a couple of ways you can actually counter that . one is by increasing your regularization constant . this could be by increasing your dropout rate or by increasing the l2 norm rate okay . and again this is very important to stress that do not test your model in the test set before you're done resolving overfitting issue . it almost sort of breaks the scientific pipeline if you test your network on the test set and then tweak your parameters because you're not supposed to look at the test set . so the question was if you're not supposed to test them on the test set then what kind of data are you suppose to test it on . and the answer to that is you can divide your data into three parts . you evaluate regularly on the dev set . 
and then finally at the very end test on the test set okay . with that i'll leave you to barak for backpropagation and gradients . it's another bright and sunny day to do some backpropagation . i think it's also pretty ironic that last time i spoke to you from here i told you about all of the wonderful with automatic differentiation and here we are again computing gradients by hand . but anyway so the main ideas of this part of the review is to go over some of the intuition and the math behind backpropogation as well as how to use it in practice . before i begin i highly urge you to check out kevin's notes on gradient computation as well as some of the principles behind like matrix calculus . i think it's very helpful to understand you the most with the midterm . okay so our itinerary is to first review what backpropagation is then to have a quick chat about matrix calculus . then we're gonna talk about how to compute products of gradients correctly . in other words when do i transpose my darn symbols . and then we're going to solve two midterm problems . okay so the problem statement that we are looking at is given some function f with respect to inputs x and some labels y including some parameters theta we wanna compute the gradient of your loss with respect to all your parameters theta . and what backpropagation is it is an algorithm that allows you to compute the gradient for some compound function as a series of local intermediate gradients . so if you have some compound function backpropogation is the tool that you have that allows you to compute the total gradient through an application of the chain rule to all of the local intermediate gradients in you function . so there are three kind of parts to backpropagation . the first is that you want to identify what your intermediate functions are i.e the intermediate variables . and this is basically done for you in the forward propagation stage . you then want to know what the local gradients are of all of the variables inside your compound function . and then you want to somehow combine those through some application of the chain rule to get your full gradients . so the first thing that we need to talk about is what modularity is and let us look at is at an extremely simple example . 
we have some function of three scalar variables x y z . we're going to add x and y and then multiply that by z . our intermediate variable is q which is equal to x + y . and our final compound function is q x z . so the idea of modularity is to kind of separate out our smaller operations such that each level of our forward propagation we know how to complete the gradient of the output with respect to our inputs of interest . so the key idea of modularity is that it allows you separate out the operations that you're using . so that at each point you're able to calculate the gradient in some sort of palatable way . okay so modularity for a neural network is basically what we've seen in our assignments so far in terms of splitting up our compound function as a series of all of our propagation steps . so in this example of a two-layer neural network we have the loss of some cross-entropy of a sigmoid activation of some linear function again applying to linear function over that and taking the cross-entropy with y . so intermediate variables are going to be all the things you're familiar with . it's going to be our hidden layer it's going to be the activaion it's going to be our scoring which is z2 in this example followed by our loss . so the idea is that at each step of this forward propagation stage we know how to compute the gradient of our output with respect to our inputs . so let us look at how the forward propagation and the backward propagation relate to each other . on the left we have the forward propagation which is where the values from our inputs are propagated down through our network . propagation is basically the reverse side of the mirror . where at each point we take the gradient at that level with respect to the variable above it . so the last thing that we need to do to finish off our backpropagation is to find out how we're going to merge all of these local gradients together to get our total gradient . and this is as good a time as ever so the key intuition behind the chain rule is that slopes multiply . so if i'm trying to take the derivative of some sort of compound function f and g of some input x over that x . it's going to equal that derivative of our total function with respect to the intermediate value . 
times the derivative of the intermediate value over our inputs . and this is a bit of a mathematical wonder you might think . it's so beautiful that if you have two functions . you can just multiply the slope of the intermediate by the slope of the function that happens after that . so this is kind of the key tool that allows back propagation to work . another useful analogy for understanding back propagation is looking at circuit diagrams . so this is going to be the circuit diagram of our initial simple example . we're going to add x and y and that is our q node over there . we're going to multiply that by our z variable . so the green values in this diagram represent the forward propagation values . and the red values represent the backward propagation . we start off with an error the derivative of f with respect to itself is just 1 . what is interesting though is if we look at the error signal that is happening at the q node . it's currently -4 because the derivative of f with be the value of z minus 4 . we know that the derivative of q with respect to x is just 1 . but what we're going to do to get the total gradient at position x . is we're going to multiply that gradient times our error signal that is flowing into q . and this is i guess the key point of what we're showing here . and we'll see how this kind of figure can help us do back propagation in midterm questions soon . okay are there any questions so far about this large overview of back propagation before we start talking about matrix calculus . 
great okay moving to some matrix calculus . so let us first talk about derivatives over vectors . so the derivative over a vector is going to be a matrix of partial derivatives . where the derivative of each row is going to be the derivative of that index of the output with respect to all of the indices of your input x . so for the scalar by vector example you're going to get a single vector . where each column is going to be the partial of y by the partial of x at that particular column index . in the vector by vector case we're going to have the matrix of partial derivatives . of yi on the ith row by all of the indices at position x . the case for derivatives over matrices question . the question was does the presentation of the derivatives change with respect to whether x and y are column vectors . so a potentially complicated answer to your question is that each of these derivatives represents the jacobean of that derivative . that is a matrix that takes as input whatever shape your x is and outputs the shape of y . so if x was a row vector yes . so the question was are we going to be using the convention that x is a row or a column . that depends on the question and it'll be stated very clearly when you're taking derivatives in that question okay . so the case for derivatives over matrices is slightly more complicated . you can interpret y as a function . if i'm trying to compute my partial of y over my partial of a where y is a scalar . y is actually going to be a function of every single element inside your matrix a . okay so the proper derivative of partial y over partial a is actually going to be an element of thanks . 
it is actually going to be an element of rmn . so the true jacobean derivative of partial y over partial a is actually going to be some long form vector ofsize mn . but this is not such a good presentation for us when we want to do derivatives of matrices . so we actually rearrange that sort of derivative to be some matrix of derivatives . where the partial of y at each index is going to correspond to the partial of the case for derivatives of vectors over matrices is even more complicated . because the true form derivative of a derivative over a matrix is actually some sort of three-dimensional array . or it's going to be a tensor that is sort of beyond the scope of what we're trying to do here . so since we're only interested in computing with respect to matrices . we can actually hide out that sort of strange three-dimensional array derivative thing . cuz we're going to be computing it by some error signal from above . and let me show you what that means over here . so if we're interested in computing the derivative of z with respect to a . let us look at what the derivative would be of that z with respect to a single element inside our a matrix . so you can think of aij as representing the sensitivity of the ith index of the output with respect to the jth index of the input . and this is what we're going to be looking at . so the derivative at the ith position of the dz gradient is going to be exactly the value of xj in our input x . since that is what is modifying the a . and this leads to the identity that the partial of j over the partial of aij is going to equal the dot of our delta . since that is the only value that is not 0 in our derivative of z over aij times the value at xj . and this leads to the identity of dj by da . 
so you can somewhat ignore the slide and just focus on the identity for the purposes of the midterm . but this might be a bit of an explanation of what it means to take sort of derivatives over matrices . okay so on this slide i have for your reference . a list of perhaps the most useful identities that you'll need for computing all of the gradients that we use in our midterms . so i highly urge you to check these out or to check them in the notes . so are there any questions before we move onwards . so one thing that you might notice from the previous slides . is that the nice thing of taking derivatives of scalars over vectors or matrices . is that the shape of our output has the same dimensions as the shape of our inputs . both in the scalar-by-vector case and the scalar-by-matrix case . so what we enforce when we do back propagation is this shape rule . where when you take the gradients against a scalar which is what you're essentially always doing . the gradients at each intermediate step have the shape of your denominator . so if x whether it's a row vector column vector or matrix has shape m by n . the error signal of d scalar [inaudible] loss by dx is equal to the same size as our denominator and what this allows us to do is dimension balancing . so this is going to be the general gradients for any sort of matrix multiplication you do on any gradient question where x and w can be either both matrices or x can be a row vector and w a matrix or x can be a matrix and w a column vector so this is a general form . so if z is going to be some m times w matrix and we represent the error at that point as partial loss over partial z as being delta that because of our shape rule we know has shape m times w . how do we find out what the dimension of our d loss by d x . well if we are looking for a gradient of shape m times n multiplying our gradient signal by w using our identities from the previous slide . then we realize that we need to take the transpose of w so that the w is on the left side and the delta is and the w is on the right side of the delta . 
so you can kind of solve these identities by looking at the dimensions of all of your terms making sure they balance out to give you the correct dimensions for the gradients . by matching the gradients of your error and the gradient the dimensions of your gradient and the dimensions of your term . are there any questions about this dimension and balancing concept . delta is going to be the error signal at the point of z . so since we're trying to compute the gradient of the loss with respect to x and the level above that the intermediate level above that is z . to apply the chain rule we're going to have delta represent the derivative up to the point z . and we're going to multiply that as an application of our chain rule with the derivative of z with respect to x . so the idea whenever we're if i'm starting with some z as a matrix multiplication of x and derivative of loss with respect to w is . using the chain rule i know that that's the derivative of the loss with respect to z times the derivative of z with respect to w . so this is going to be our intermediate variable that we're taking our local gradient of . and we know what the derivative of this with respect to this is . but this is going to give us the error signal up to that point . the value of the error is up to the point of this intermediate computation okay . the error signal is kind of what the gradient is up to the point of your equation of the loss down to the last variable that you're looking at which is the output of that expression all right so it kind of feels like dimension balancing is this kinda cheap and dirty approach to doing gradient calculations . but they are indeed kind of the most efficient tool that you have to computing gradients quickly in most practical settings and especially the midterm . but i do encourage you to read the gradient computation notes the gradients work from first principle . the last thing that i wanna talk about is how gradients over activation functions work . so one of the questions that we fielded frequently in office hours is why is it the case that when you're taking gradients over an activation function you're doing this element-wise multiplication or hadamard product . and the answer to that is if you're looking at so the answer to that is the activation function is a function that maps a scalar to another scalar . though we represent z as a vector as a sigmoid of an entire vector h the sigmoid is actually being applied to each individual element in that vector which maps the same index in your output . 
so the way we represent the true gradient the true derivative is that it's going to be a diagonal matrix . where each index on your leading diagonal is going to be the gradient of the ith index of your input . and multiplying some matrix or vector by a diagonal matrix is equivalent to doing the element-wise multiplication of a vector itself . which we represent and the hadamard product . far before we get to some midterm problems . this is the true jacobian form of the partial of some y vector with respect to x you can see that it's a matrix that takes as input a column vector x and maps it to an output y . so if the shape of your jacobian takes as an input something with shape x and maps to an output something with a shape y which is exactly what is happening in the top left box over here . can we talk about that question maybe after the lecture so we can move on . cuz we have a few more sections . okay so let's get through and maybe when we actually do some midterm problems it'll become more apparent how it works . okay so the first question that we're going to look at is to do with siamese networks . and a siamese network basically allows you to compute a similarity metric between two inputs x between an x1 and some x2 inputs . and this allows you to sort of compare the similarity of two word vectors or two sentences . so the first thing that i like to do when we're starting some gradient problem variables are related to each other . so we can see is we start off with some j at the top we have our cost function j . it takes as input an h1 and an h2 as two hidden vectors . these are activations of z1 and z2 . and i am defining these variables myself so i need to specify what they are . and this is going to be in this problem we're treating our inputs as column vectors . and for z2 it's going to equal to x2 plus b . 
so we have two inputs x1 and x2 . and one interesting thing to notice about siamese networks is that the variables sorry the parameters w and b are shared across both of these activations . so if we're interested in taking the derivative of j with respect to w we need to add the derivatives coming from both branches down . so the first thing that i like to do when i'm computing my gradients is basically to express this graph and we'll see how it's useful to us in a second . okay can the camera see me awesome . so if we want to start our gradient computations the first thing we want to do is to take the derivative with respect to j . so let me write out what my j is going to be . it's going to be half of the hadamard of h1 minus h2 squared f . and we're going to have a regularization term . so this is i'm first writing out what my cost function is going to be . and i use this graph to kind of inform me about where i'm taking my derivatives and what my error signals are going to be . so the first thing i wanna find out if i wanna compute what is the derivative of j with respect to w . so the first thing we're going to do is flow down this branch . so we're going to compute the partial of j with respect to the partial of h1 and i'll leave this for what the derivative over the frobenius norm is . but it's basically just going to be h1- h2 cuz the 2 cancels out and you're left with an h1- h2 term . equivalently the gradient with respect to h2 is of that expression okay . so one thing that i like to do is to also record what the dimensions are of all of my gradients . so over here since we're dealing with column vectors this is going to remain an m by 1 vector . and this is going to be an m by 1 vector as well . and the other thing i'm going to do is give a name to these error signals . 
this one is going to be delta 1 and this one is delta 2 . using this graph is what my error signals are flowing down it . so over here i've defined delta 1 to be the partial of j with respect to partial of h1 . and over here we have delta 2 being the partial of j over the partial of h2 okay . so the next thing we are interested in computing is what is the derivative of j with respect to z1 . so we're going to apply our chain rule . we know what the derivative of j with respect to h1 is so we only sorry we want to compute the derivate of j with respect to z1 . we know what the derivative of j with respect to h1 is so we only need to compute the derivative of h1 with respect to z1 . so over here we have a sigmoid activation function so we just need to compute the gradient over that . so my partial of j over equal partial of j by partial of h1 times the partial of h1 by the partial of z1 . and that is equal to my first error signal with the hadamard product of this is getting a bit tight here it's going to equal my error signal hadamard product with h1 times 1- h1 . which as we know from assignment one is the derivative of the sigmoid . similarly the partial of j with respect to z2 is equal to the partial of j by the partial of h2 times the partial of h2 by partial of z2 . and that's also going to equal our current error signal flowing down to h2 so it's delta 2 hadamard product with h2 times 1- h2 . and we're gonna give this a name . this is now going to be delta 4 and this one is going to be delta 3 . okay so over here we have delta 3 and here delta 4 okay . so now the last thing that we wanna do if we wanna compute the gradient of j with respect to w this is going to equal my error signal up to z1 so partial of j by partial of z1 times the partial of z1 by the partial of w . it's also going to and we're going to add to that what's happening in the error signal at the other branch . that's partial of j by partial of z2 by the partial of z2 over the partial of w . 
plus we don't want to forget about our regularization experiment . frobenius norm of w by w okay . and this is going to now we need to do some matrix balancing using what we discussed earlier . since we're taking the derivative over this matrix product over here . so what we need to do is we want this final product of each of these things to have the same shape as w . this is just given to you in the problem . we know that delta 3 and delta 4 are m by 1 . because when we computed our delta 3 and delta 4 we just took the elementwise product of whatever was up here so it's still going to remain m times 1 . finally we know that x1 and x2 are both n by 1 since they are column vectors . so if we want to we know that this is delta 3 and we're going to have to multiply it by the xs in some way . so we see that if we want our final to be m times n we're gonna have to do this expression times the transpose of this . plus delta 2 sorry delta 4 times sorry delta 3 times x1 transpose + delta 4 times x2 transpose . and finally we want to add this expression . and again i will leave this for you to show on your own time that the derivative of this frobenius norm the 2 cancels out with this . and you're just left with lambda w okay . what happens if you run into some sort of ambiguity over here . so the nice thing about when we're since at least for computing the neural network models that we are looking at for the midterm and . the only tractable problems that we're going to be computing gradients over are things like linear transformations and activations over those which are already the key operations that you use in neural networks . apart from things like cross entropies and other losses at the top . so in most of those situations you won't really run into ambiguities . 
and when you do you just go back to deriving things using like traditional matrix calculus so you just dig deeper . like if you dug deeper into how these dimensions work out it should still work out . so you just won't find ambiguities in these types of problems okay . i will quickly run through my second example . okay so the next example that we are looking at . suppose you wanted to build a word vector representation that you and couldn't decide whether you wanted to have a hidden layer that is activated by a sigmoid . or you cannot decide whether you wanted a hidden layer that is activated by a relu . so the only thing better than making a choice is to do both of them . so what we're going to do is we're going to have a network where at the top we're going to have a cross entropy over some prediction y that takes n from z3 . so over here we're going to have some input that flows through both of these variables . so here we have xw1 + b1 . so we're going to apply a linear transformation over our input x and then sigmoid activate it to produce this hidden layer . over here we're going to have a different linear transformation with two other parameters two other matrix and bios term . but here we're going to relu activate it . then we're going to add our h1 and h2 . at z3 we're going to guess our score vector which is z3 and then we apply the cross entropy to that that's your model . representation we did in assignment one except now we have two branches that take the input twice . so what we want to compute at least for this question is what is the partial so the nice thing about this problem is that we already know how to compute the derivative of our j with respect to z3 . since z3 is the input into our cross entropy and we did this in our homework . so the partial of j with respect to z3 is just going to equal to y hat minus y as we already know . 
and this is going to be our first r signal . so we already know what the errors from j flowing down to z3 . and we're calling that delta 1 okay . the partial of z3 with respect to h1 and h2 are actually equivalent . cuz if we know what the partial of z3 is with respect to this sum since the gradients of h1 + h2 and similarly with h2 we can simply write down the partial of j with respect to h1 is equal to the partial of j with respect to sorry partial of h2 . which is equal to the partial of j up to z3 times the partial of z3 over partial of h1 + h2 . and what we need to do here is again some matrix balancing . since we know that h is a [1 x m] row vector . and i forgot to specify but this is actually pretty crucial and this is the next slide . i did convert this problem into the exact same model except using row vectors . just to show you that the way we do dimension balancing in a row vector versus a column vector situation is basically exactly the same . and our w3 is m times k is m by k . our delta 1 is also 1 value k . so what we can see is that if we wanted to multiply our error signal from above by what we know is going to be the derivative with respect to h1 + h2 we're going to need to transpose our w3 and multiply our delta 1 by that transpose . so going up here i'll just write an equal sign . this is i'm continuing this line over here . that is just equal to delta 1 by w3 transpose . and i'm going to call this delta 2 . and then i'm going to fill in my error gradient flowing down here okay . moving over to this side if we wanna move down the branches we now wanna compute the derivative of j with respect to z1 and z2 . 
so the partial of j with respect to z1 is equal to the partial of j up to h1 . there's a partial of h1 up to z1 and that's just going to equal since this is an activation this is a gradient over an activation function we again use our elements y as multiplication so that's just equal to delta 2 . by since on the left branch we're doing a sigmoid we're going to have just h1 times 1 minus h1 . and on the other side our partial of j with respect to partial of z2 . and i'll just skip writing out i won't write out the chain rule for the sake of time . that's going to equal our delta 2 by the gradient of the relu activation function which is 1 . if our z2 is greater than 0 based on the value as we it saw earlier . so the last thing we're gonna do and i'm again running out of space is to compute the gradients with respect to x which is what what we wanna do . so the partial of j with respect to x is going to equal the partial of j up to z1 times the partial of z1 respect to x . plus since we're working with our derivatives x appears in both branches it's going to equal the partial of j with the respect to z2 partial of z2 with respect to x okay . and what we're gonna do is matrix bouncing one more since we're taking derivatives so we wanna make sure our dimensions balance out . so we know from the problem statement that x is a row vector . all right w1 w2 are by n by m and that's just given from the problem formulation . so since x is [1 x n] and we're gonna need to compute we're gonna need to multiply our error signals by this being transposed . and although i didn't write over here you can see you can prove to yourself that delta 3 and delta 4 are 1 by m . so this final expression and going down here . since we need to transpose this it's going to be the error gradient up to this point . so delta 3 times our w1 transpose since w1 appears on the left side plus the error signal delta 4 times w2 transpose . bit long winded but the general approach that we're using is to first of all build the graph of our variables starting from j we compute the derivative going down and at each point we mark what the error signal is up to that point . after that we apply our chain rule where we compute the derivative of our current variable times our new thing going down and signal coming from above . 
and we use matrix balancing to make sure our products work out . i would say yes since it helps you keep track it helps to keep track of derivatives up to a certain point . and it also helps you if you can just take my current error signal specify what the dimensions of that are . it's very easy to make sure that your dimensions balance out when you compute my error signal by what follows afterwards . i mean you don't have to but it certainly is a way . of keeping your calculations clean and organized . for the sake of time i'm going to move on to the next section about back log after the lecture . so my last slide and you can look this up later . your menu for success i recommend doing all of them is to write down your graph compute the derivatives from the top down keep track of your error signals and force that the dimensions balance out . and yes that is it thank you . so our men are probably some of the coolest architectures that you've learned in this class and and i may have the pleasure unrolling them a little bit further for you . we'll also share some material questions from past exams . so here you now have a distributed representation of each patient note . you assume that a patient's past medical history is informative of their current illness . as such you apply a recurrent neural network to predict the current illness based on a patient's current and previous note-vectors . you also explain why a recurrent neural network would be better than a feed-forward network in which your input is summation or average of past and current note-vectors . so you can talk to your neighbors just like take a minute to discuss this problem and figure out why an rnn would actually be better than a feed-forward one . are there any suggestions to why our rnn might be better than feed-forward ones . i'll try to repeat what you just said you can correct me if i'm wrong . so you said that the rnns they can do something specific for a particular patient and while if you sum them up they wouldn't . 
are there any other suggestions as well . mm-hm and so what you just said and again you can correct me if i'm wrong is that the more recent information about this patient might be more relevant . and that is something that an rnn could capture which a feed network might not be able to capture . yeah so what i just said right now is that and there is some time dependency in this data since we have a sequence of a patient notes . and if we were to sum them up together we lose that temporal information . and so these are all very good answers . and i do want to comment just on the first answer a slight misunderstanding . so we are not summing up across different patients . i'm summing across the nodes for the same patient across time stubs . and i think that was probably just like maybe it wasn't phase super clearly . and what the other people said was and also correct . so one thing is that rnns can take in more recent information into account . and in a more general sense they can take in temporal relationships into account which we would ignore by summing up or arching over them . cool so in order for us to see this more yes there's a question . the question was on what advantage would an rnn give us over taking a weighted average . often note-vectors well if you do weighted average we actually had to come up with the weights for on the particular notes which we actually don't really know beforehand . and the idea of like chaining an rnn here is that like the rnn might potentially be able to learn to what extent it should take in . really old notes into account versus perhaps like more recent ones so we don't want to predefine those weights . and we can also talk about that after class as well . so let us take a really quick look at the rnn structure . 
so this is a very simple vanilla rnn . they key points are the weights are shared across timesteps . and the core of an rnn is the hidden state and the hidden state depends both on that previous hidden state so kind of like capture some of that memory from earlier as well as the new input and you should have seen that on the assignment two . and rnns because of their architecture are very suitable for learning representations for sequential data that has sometimes temporal relationships . and the predictions can be made at every timesteps so you can have a y as can be seen in this figure over here . you can make a prediction every timestep or you can make can at the end of the sequence depending on whatever your application might be . so in our key rnn's come to use a special when we do language modelling . so language modelling is the tasks of computing probability distributions over a sequence of words so that can be for example seconds or a document . and language modeling is very important when we are doing things like speech recognition text summariztion and so on . use it as a language model would be passing at every time subsidy inputs xt are our word embeddings . and you could use word embeddings for example that you produce with word and so you pass it at every timestep . and our prediction is just the next word in the sequence . so we can use that as our task in order to train this rnn . and the idea is that the hidden representation at the end . so if you take the hidden representation at the last time stamp on our trained rnn capture some of the semantic meaning of that sentence or sequence of words . so that can be for example used for translation which is something you saw or you saw on tuesday i think which richard presented . so here this is actually a [inaudible] rnn which have an encoding part and a decoding part . so for the encoding part that's your first rnn . you feed in the word embeddings for the german words so [foreign] . and then you pass in that hidden representation which captures the meaning of the sentence into the second rnn . 
so the weights are shared across the first rnn and they're shared within the second rnn . and a second rnn is a decoder which produces the english sentence here . so this is one example of using this type of rnn as a language model for a translation . okay so the problem though with the vanilla rnns is that we can see the problem vanishing gradients . in back propagation when we do back propagation rnns call on the hidden layer . and so that's basically since we define a hidden layer in terms of its p precedent layer . and the magnitude of the gradients off the typical activation functions that we'd use like ton h or sigmoid are between 0 and 1 which causes and if you multiply a number that's smaller than 1 multiple times what happens is that the number will shrink very quickly . and if it shrinks very quickly essentially what happens is that you final gradient will be close to 0 . and if your gradients are close to 0 then essentially means that you're so in order to address this issue this is like why people have worked on many of the different architectures and grus lstms were some of the more popular ones . okay i should also repeat the question . so the question was can we adjust the vanishing gradient problem by using a different activation function something like a volume . yeah we can also discuss this after class too . okay so grus and lstms to the rescue . so i'll talk briefly about grus gated recurrent units . so the addition here is that we are introducing gate . so we have a reset gate and an update gate . and you can intuitively understand these gates as they're controlling the long term and short term dependencies . to what extent we want to memorize things from the past . and to what extent to we just want to take something from the current input . and that might be also something that you might see or like . 
it's related to what we talked about with patient notes . it's kind of this network might be able to learn whether i should only look at the last patient note or whether i should take like the entire history into account . so and this is a more visual representation of a gru games . so you can see that the input x team over here is fed into both of these gates which are sigmoid functions . and it's also fed into this other one which calculates h tilde . so this is kind of a first initial hidden state but in order to decide whether we really want to use it we are using the z factor over here to decide whether we just want to use the previous hidden state or if you want to update it to the h tilde this might also be a picture if you wanted to look at it again . it's on our blog so you can just try to figure out how this information is flowing and there . but in order to just get some more intuitive understanding of how gru works we are going to do some exercises together . so first try to figure out what the dimensions are off the ws and the u matrices in here . and you can write it in terms of the x dimension of x and the h which is the dimension of h . again you can just take a minute and try to figure that out . and you can also discuss again with your neighbor . since we are running a little bit out of time i'm just gonna go ahead and explain how i would approach this problem . so first you know that ht the hidden vector h has dimensioin h just by definition . so i'll write a little bit bigger okay . so this is just by definition and we also know that ht minus 1 was also has mentioned dh which means 1 minus zt so zt itself needs to have to mention the dh as well right . since we are doing a aliments bite operation in the fourth equation over here . so it also shows us that h tilde suggest by looking at the last equation h tilde means half dimension dh as well . and now that we know that h tilde has dimension dh equation over here . so we know that the product of w and xc has to give us something with dh . 
so that tells us that the matrix w needs to have dimension dh and dx . cool i see a couple nods okay . so similarly if you look at the matrix u in there we know that like the result of u ht minus 1 needs to be dh . like [inaudible] so percent should be dh . and since we're multiplying it with a hidden vector the second dimension should be dh as well . and you can apply a similar approach to figure out the dimensionality for the other matrices as well . if the update gate is close to 0 the net does not update its state significantly . yeah this one should be pretty obvious if you look at the fourth equation because in that case you're essentially just setting ht to ht minus 1 . and this also shows that if the network learns that zt is pretty close to 0 that just means that we are essentially using our input from the first time . so essentially this means that your input from a long time ago still matters a lot in the future . okay another related question if the update gate is close to one and a reset gate is close to zero the net remembers the passing very well true or false . yeah this is actually very similar to the first question so again you can just set them on z t to 1 r t to 0 in these equations . and you'll find out that in this case h t would depend very strongly on the input since we're essentially saying that this term over here becomes zero . and we're only using the x t part of it . okay so joee will now talk about lstms . lstms are kind of similar to gius except they are a more common model . so instead of just having two gates now we have multiple gates . one is the input gate which decides how much weight should we give to the current input or the current word that we are looking at . the ft gate or the forget gate will decide how much we want to forget our past how much you want to remember the past or just forget it . the new gate ot which is a little bit different is how much we wanna expose our current cell to future . 
so combine the ot from the current combined with the ft from the future will decide how much all of memory will be use in any other future node . so you can see how adjusting this can decide like whether you just wanna remember the b minus 3 times 3 but not t minus 20 minus 1 . instead of just going linearly backwards i'm having to remember everything from the past . so all these gates are computered using a sigmoid because that's between 0 and 1 . and that makes sure that when you are doing the dot products of sorry the micro dot products in the end you'll have kind of like a probability distribution of not just completely forgetting which would be zero . or completely remembering which would one a fuzzy possibility of remembering a little bit or forgetting a little bit . another difference between lstms and grus is the ct and ht node so now you don't just have one memory you have two memories . like the ct would define exactly what your memory is while your ht defines in your it ft and ot how much or how you want to remember the ht minus one memory . or how much your memory should be remembered by the future . so instead of just having one now we have two . these have a certain number of disadvantages for example there are a lot more parameters now which means you need more space . which means you need more learning much more training data so that your model doesn't start warpening . and empirically grus and lstms have been are very close in results . so in the end it's a trade off on how much accuracy you want compared to how much you are willing to use your resources on training time and training data . the lower row corresponds to your ht flow while the upper row corresponds to your ct flow . and both of them are memories and they interact with each other which means they are not completely independent . but they represent different ways in which your future or your next words will be using your current word . so the question was can you give a little bit of intuition on what is the difference between c and h and so if you see from the formulae the edges are present or the edges decide on what your input gates your fts or ots will be . so they decide how much you want to remember or forget or expose yourself . while your cts are the ones that are actually being used in the future . 
so they are the ones that and so basically it's like saying your hts will decide the fuzzy probability of how much you want to remember and how you want to remember and your ct will kind of decide what you want to remember from the previous states . this is just like illustration it's not exactly like that . but that's like a rough way of how you can think about it . great so the question was will we have the equations . and yes all the equations for rins or lstms or grts will be there so that you don't have to remember them . also you are allowed a cheat sheet so to be honest if you wanted to remember it just write it on your cheat sheet and you can bring it . so we'll just go to the midterm questions really quickly because we are running out of time . if xt is the 0 vector then ht = ht -1 . it is true that ht will depend primarily on ht -1 but it won't be exactly equal because of the non-linearities and the multiplication with the parameters . the second question is if fd is very small or 0 then error will not be back-propagated to earlier time steps . again intuitively it feels like because the fd is 0 they are trying to forget . but that is not true because it and ct still depend on ht minus 1 and hence the error will still back propagate . so one easy way to do it is just look at the mat see what depends on what and see how the rows are actually back-propagating . the next question is if either entries ft it and ot are negative and we see that the sigmoid activation is used which is the range is between zero and one . and as i said that just represents the fuzzy probability . which brings me to the next question can they be viewed as probability distributions . and the problem with viewing them as probability distributions is that they do not sum to one like any probability distribution should . so these are applied independently of the element and of having to sum to one . so this is just like an intuition of probability but does that make sense to everyone . so very quickly we'll go dependency parsing . 
so these are two views of linguistic structures we haven't seen which was constituency structure we'll be seeing it for later . but basically it uses a kind of c f g model where you decide which phrase is broken down into what words . what we are seeing is dependency structure where we decide what word depends on what other word in the sentence and how they depend on each other so what modifiers are used . it's binary which means two words are related to each other no more and no less . it's asymmetric which means one word depends on the other but not the other way around . in this case the dependent depends on the head . which are the terms that we'll generally be using . and the arrows when you're drawing the dependency tree will go from the head to the dependent . they usually form a connected acyclic single-head tree . and to make sure that this happens we also add a fake root node so that one of the words will always and this also makes sure that every word has a head . and none of the words are just standing . and in the rare cases where the root known make sure that all of them are connected into a single tree . so we saw two different types of dependency parsing . one was the greedy deterministic transition based parsing . we've just seen this in the assignment right now so i am sure it's all fresh in your memories . you have a stack a buffer and a dependency list . initially the buffer will be full of the words in your sentence in order . so that will act like a queue structure . your stack will act like a stack structure so that you pop from the top of this chart . and whenever you do this shift transition you take something from the top of the queue and put it onto the stack . 
the left toll will take the top two elements from the stack and make an arrow from the first element to the second element and just remove the second element . and this can be seen from the example here which you can just scroll through the slides and go like decide . now how do we decide which transition to use . this is generally done by some kind of a classifier you could use multiclass as svms or any other kind of machine learning classifiers that you know of . learning involves need so here we'll use the features and you can always add more features . but the typical features that are generally used is the top of the word on the stack the first word in the buffer and maybe a lookahead on what words are gonna come next . and also the dependence of the current word in the stack . and now that we have all the words we also use the parts of speech of all these words . so generally we know that adjectives and nouns already likely to be dependents or connector so that there's something that you would want to use as a feature . yeah those are kind of what dependencies that we have already figured out until now . and using all of these we try to get what kind of transition we want to do next . and define the valuation metric that we use is either uas if you're not typing the dependency from one word to another . or las which is the liberal attachment school which types their attachment from one word to another and these are basically kind of like accuracies . you can think of them as accuracies . so one thing that we saw was projectivity and the non-projectivity comes in . and what projectivity means is that there are no arrows that cross each other when you put them in a horizontal line . and why this would be a problem is if you go back to this kind of a parsing mechanism here the from and who will never be next to each other on a stack . and thus left arc or right arc will never be able to get the dependency between from and who . and so how do we handle something like this . one simple thing would be you just declare defeat . 
you just say that okay these are really really rare cases . and if you don't care about your accuracy as much and you don't want to complicate your model a lot then you can just leave them as it is and let them decrease your accuracy a little bit . if you want to handle them one of them would be to use a post resistor . so you go through the entire parsing mechanism and in the end you use some kind of a post-processor to identify which ones have been parsed strongly and try to dissolve them . this can be done using multiple methods like classifiers or other things . the last one is you use a completely different parsing mechanism or to the ones that already exist . so in the greedy transition based parsing you could add a transition let's say swap where you just swap the elements that are already in the stacks so that you bring the element that is at the bottom to the top and then you can either lift right down to it . there are other more complicated ways of doing it and those can also be done . finally we have the neural dependency parsing . so one problem with really the domestic parsing that we saw was the features were all one-hot vectors where it was either a word or a part of speech . and we have seen a lot of problems with one-hot vectors before . one is that you don't have the semantic representation of the word . so if two words are actually similar in either a grammatical sense or meaning wise you don't understand digest because of the one-hot vector notation . and an easy way to do some to solve something like that and here we'll not only be using embedded matrix for the words but also the parts of speech and the dependency labels . and all of these are then added into one feature stack or a feature drawer and they are put into a black box which would be any unit that you want to use . for example here you are using a hidden layer and a soft max layer . neural network and then finally you use classification to decide what transition you want to do . and i know this was a little quick but we are running out of time so . okay so there are just a couple of acknowledgements and all the best for your exam . 